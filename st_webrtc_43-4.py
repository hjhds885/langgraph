# streamlit_watcher_ignore_module: torch, torchaudio, torchvision
import streamlit as st
from streamlit_webrtc import WebRtcMode, webrtc_streamer, VideoTransformerBase
##########################################################
from langchain_community.tools.tavily_search import TavilySearchResults
from newsapi import NewsApiClient
from exa_py import Exa
from langchain.agents import tool,Tool
from langchain.tools import Tool
#from langchain_community.agent_toolkits.load_tools import load_tools
from pydantic import BaseModel, Field
#from metaphor_python import Metaphor
#from langchain_community.utilities import SerpAPIWrapper
#from langchain_community.utilities.google_search import GoogleSearchAPIWrapper
##from langchain_community.tools import WikipediaQueryRun
#from langchain_community.utilities import WikipediaAPIWrapper
##########################################################
from langchain_core.messages import HumanMessage,AIMessage
from langchain_core.utils.function_calling import convert_to_openai_tool
from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langchain.chat_models import init_chat_model
#from langchain_core.prompts import ChatPromptTemplate
#from langchain_core.output_parsers import StrOutputParser
##########################################################
#from langchain_openai import ChatOpenAI
#from langchain_anthropic import ChatAnthropic
#from langchain_google_genai import ChatGoogleGenerativeAI
#from langchain_cohere.chat_models import ChatCohere
#from langchain_nvidia_ai_endpoints import ChatNVIDIA
#from langchain_ollama import ChatOllama
#from langchain_ibm import ChatWatsonx
#from databricks_langchain import ChatDatabricks
#from langchain_groq import ChatGroq
#from langchain_mistralai import ChatMistralAI
#from mistralai import Mistral
############################################################
import requests
import numpy as np
import threading
import time
from datetime import datetime, timedelta
from typing import Any, Dict, List,Tuple
import asyncio
from collections import deque
import cv2
import av
import base64
from gtts import gTTS
import whisper
import pydub
from pydub import AudioSegment
from pydub.effects import low_pass_filter  #,high_pass_filter
#from scipy.signal import resample
from io import BytesIO
import psutil
import gc
import re
import librosa
import subprocess
import vertexai
import os
#import getpass
#import json
import tiktoken
import uuid
import torch
torch.classes.__path__ = []
import torchaudio
import torchvision
#####################################################
#ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™
#Community Cloud ã®ã™ã¹ã¦ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯åŒã˜ãƒªã‚½ãƒ¼ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã€åŒã˜åˆ¶é™ãŒé©ç”¨ã•ã‚Œã¾ã™ã€‚
# ã‚¢ãƒ—ãƒªãŒã“ã‚Œã‚‰ã®åˆ¶é™ã«é”ã—ãŸå ´åˆã€ã¾ãŸã¯åˆ¶é™ã‚’è¶…ãˆãŸå ´åˆã€ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°ã«ã‚ˆã‚Šé€Ÿåº¦ãŒä½ä¸‹ã—ãŸã‚Šã€
# æ©Ÿèƒ½ã—ãªããªã£ãŸã‚Šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
# 2024å¹´2æœˆæ™‚ç‚¹ã§ã®åˆ¶é™ã¯ã€ãŠãŠã‚ˆãä»¥ä¸‹ã®ã¨ãŠã‚Šã§ã™ã€‚# ã“ã‚Œã‚‰ã®åˆ¶é™ã¯äºˆå‘Šãªãå¤‰æ›´ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
# #CPU: æœ€å° 0.078 ã‚³ã‚¢ã€æœ€å¤§ 2 ã‚³ã‚¢
#ãƒ¡ãƒ¢ãƒª: æœ€å°690MBã€æœ€å¤§2.7GB
#ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸: æœ€å°å®¹é‡ãªã—ã€æœ€å¤§50GB
#åŸå› 
#Streamlitã®ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–æ©Ÿèƒ½ãŒtorchã®__path__._pathã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã‚ˆã†ã¨ã—ã¦å¤±æ•—ã—ã€RuntimeErrorãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚
#ã•ã‚‰ã«ã€asyncio.get_running_loop()ã®å‘¼ã³å‡ºã—æ™‚ã«ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€
# RuntimeError: no running event loopãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚
#å¯¾å‡¦æ–¹æ³•
# 1. ã‚¢ãƒ—ãƒªè‡ªä½“ã¯å‹•ä½œã™ã‚‹å ´åˆãŒå¤šã„
# ã“ã®ã‚¨ãƒ©ãƒ¼ã¯å¤šãã®å ´åˆã€ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è­¦å‘Šã‚„ã‚¨ãƒ©ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã ã‘ã§ã€Streamlitã‚¢ãƒ—ãƒªè‡ªä½“ã¯å‹•ä½œã—ç¶šã‘ã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã€‚
# ã‚‚ã—ã‚¢ãƒ—ãƒªãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã‚‹å ´åˆã¯ã€ç„¡è¦–ã—ã¦ã‚‚å¤§ããªå•é¡Œã«ã¯ãªã‚Šã¾ã›ã‚“<sup>å‚è€ƒ</sup>ã€‚
# 2. ãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ©ã‚¦ãƒ³ãƒ‰
#torchã®importç›´å¾Œã«ã€ä»¥ä¸‹ã®ã‚ˆã†ã«torch.classes.__path__ = []ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€
# ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–æ©Ÿèƒ½ã«ã‚ˆã‚‹ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã§ãã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚


# ç’°å¢ƒå¤‰æ•°ã®è¨­å®šã‚’ã¾ã¨ã‚ã¦è¡Œã†é–¢æ•°
def setup_environment():
    try:
        # streamlit cloudã®ç’°å¢ƒå¤‰æ•°ã«APIã‚­ãƒ¼ãªã©ã‚’è¨­å®š (å¿…è¦ã§ã‚ã‚Œã°)
        os.environ["LANGCHAIN_API_KEY"] = st.secrets.key["LANGCHAIN_API_KEY"]
        os.environ["LANGSMITH_API_KEY"] = st.secrets.key["LANGSMITH_API_KEY"]
        os.environ["USER_AGENT"] = st.secrets.key["USER_AGENT"]
        os.environ["OPENAI_API_KEY"] = st.secrets.key["OPENAI_API_KEY"]
        os.environ["GOOGLE_API_KEY"] = st.secrets.key["GOOGLE_API_KEY"]
        os.environ["ANTHROPIC_API_KEY"] = st.secrets.key["ANTHROPIC_API_KEY"]
        os.environ["COHERE_API_KEY"] = st.secrets.key["COHERE_API_KEY"]
        os.environ["NVIDIA_API_KEY"] = st.secrets.key["NVIDIA_API_KEY"]
        os.environ["HUGGINGFACEHUB_API_TOKEN"] = st.secrets.key["HUGGINGFACEHUB_API_TOKEN"]
        os.environ["GROQ_API_KEY"] = st.secrets.key["GROQ_API_KEY"]
        os.environ["FIREWORKS_API_KEY"] = st.secrets.key["FIREWORKS_API_KEY"]
        os.environ["MISTRAL_API_KEY"] = st.secrets.key["MISTRAL_API_KEY"]
        os.environ["TOGETHER_API_KEY"] = st.secrets.key["TOGETHER_API_KEY"]
        os.environ["OPENWEATHERMAP_API_KEY"] = st.secrets.key["OPENWEATHERMAP_API_KEY"]
        os.environ["SEARCHAPI_API_KEY"] = st.secrets.key["SEARCHAPI_API_KEY"]
        os.environ["SERPAPI_API_KEY"] = st.secrets.key["SERPAPI_API_KEY"]
        os.environ["EXA_API_KEY"] = st.secrets.key["EXA_API_KEY"]
        os.environ["TAVILY_API_KEY"] = st.secrets.key["TAVILY_API_KEY"]
        os.environ["NEWSAPI_API_KEY"] = st.secrets.key["NEWSAPI_API_KEY"]
        os.environ["METAPHOR_API_KEY"] = st.secrets.key["METAPHOR_API_KEY"]
        os.environ["WOLFRAM_ALPHA_APPID"] = st.secrets.key["WOLFRAM_ALPHA_APPID"]
        os.environ["GOOGLE_CLOUD_PROJECT"] = st.secrets.key["GOOGLE_CLOUD_PROJECT"]
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ ID ã¨ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¨­å®š
        PROJECT_ID = st.secrets.key["GOOGLE_CLOUD_PROJECT"]
        LOCATION = "us-central1"
        vertexai.init(project=PROJECT_ID, location=LOCATION)
    except:
        print("streamlit cloudã§ã¯ãªããƒ­ãƒ¼ã‚«ãƒ«PCã§ã®èµ·å‹•")
        pass

# æ—¥ä»˜ã¨æ™‚åˆ»ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_current_datetime():
    now = datetime.now()
    return now.strftime("%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†")

# é–¢æ•°ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
def get_memory_usage():
    process = psutil.Process()
    mem_info = process.memory_info()
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’MBå˜ä½ã§è¿”ã™
    return mem_info.rss / (1024 * 1024)

def current_memory_use(i,memory_use,memory_alt,memory_ok):
    # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
    current_memory_usage = get_memory_usage()
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º

    #memory_use.metric("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)", f"{current_memory_usage:.2f}")
    #memory_use.write(f"ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:\n\n    ãƒ«ãƒ¼ãƒ—{i}å›ç›®:{current_memory_usage:.0f}MB")
    memory_use.write(f"ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:\n {current_memory_usage:.0f}MB")
    #print("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)", f"{current_memory_usage:.2f}")
    # ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã‚’å®šç¾©
    MEMORY_LIMIT_MB = 2700  # 1GB
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ã‚’è¶…ãˆãŸå ´åˆã®è­¦å‘Š
    if current_memory_usage > MEMORY_LIMIT_MB:
        memory_alt.error(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ ({MEMORY_LIMIT_MB} MB) ã‚’è¶…ãˆã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ãã ã•ã„ã€‚")
        #memory_ok.empty()
        #st.stop()
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ ({MEMORY_LIMIT_MB} MB) ã‚’è¶…ãˆã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ãã ã•ã„ã€‚")
    else:
        #memory_ok.success("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚")
        memory_alt.empty()
        #print("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚")

############################################################################
# ãƒ¢ãƒ‡ãƒ«å®šç¾©: ãƒ¢ãƒ‡ãƒ«åã¨åˆæœŸåŒ–ã«å¿…è¦ãªæƒ…å ±ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
# initialize_models é–¢æ•°ã®å¤–ã«ç§»å‹•ã—ã¦ main é–¢æ•°ã‹ã‚‰ã‚‚å‚ç…§å¯èƒ½ã«ã™ã‚‹
model_definitions = {
    "mistral-small-latest": {"provider": "mistralai", "temperature": 0, "cost_input": 0, "cost_cached_input": 0, "cost_output": 0, "input_max": 128000},
    "gemini-2.5-pro-exp-03-25": {"provider": "google_vertexai", "temperature": 0, "cost_input": 0, "cost_cached_input": 0, "cost_output": 0, "input_max": 1048576},
    "gemini_2.5_flash": {"provider": "google_vertexai", "model_id": "gemini-2.5-flash-preview-04-17", "temperature": 0, "cost_input": 0.15, "cost_cached_input": 0, "cost_output": 3.5, "input_max": 1048576},
    "gpt-4.1-mini": {"provider": "openai", "temperature": 0, "cost_input": 0.4, "cost_cached_input": 0.1, "cost_output": 1.6, "input_max": 1047576},
    "o4-mini": {"provider": "openai", "temperature": 0, "cost_input": 1.1, "cost_cached_input": 0.275, "cost_output": 4.4, "input_max": 200000},
    "gpt-4.1": {"provider": "openai", "temperature": 0, "cost_input": 2.0, "cost_cached_input": 0.5, "cost_output": 8.0, "input_max": 1047576},
    # "gpt-4o": {"provider": "openai", "temperature": 0, "cost_input": 2.5, "cost_cached_input": 1.25, "cost_output": 10, "input_max": 128000},
    # ... (ä»–ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å®šç¾©) ...
    "c4ai-aya-vision-32b": {"provider": "cohere", "temperature": 0, "cost_input": 0, "cost_cached_input": 0, "cost_output": 0, "input_max": 16000}, # ãƒ„ãƒ¼ãƒ«éå¯¾å¿œ
    # "meta/llama-4-maverick-17b-128e-instruct": {"provider": "nvidia", "temperature": 0, "cost_input": 0, "cost_cached_input": 0, "cost_output": 0, "input_max": 1000000}, # ãƒ„ãƒ¼ãƒ«éå¯¾å¿œ (NVIDIAç‰ˆ)
}

def initialize_models(selected_model_name: str):
    """é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã«åŸºã¥ã„ã¦ã€ãã®ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’åˆæœŸåŒ–ã™ã‚‹"""
    print(f"Initializing model: {selected_model_name}") # ã©ã®ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã‚‹ã‹ãƒ­ã‚°å‡ºåŠ›

    if selected_model_name not in model_definitions:
        raise ValueError(f"æœªå®šç¾©ã®ãƒ¢ãƒ‡ãƒ«åã§ã™: {selected_model_name}")

    model_info = model_definitions[selected_model_name]

    # init_chat_model ã«æ¸¡ã™å¼•æ•°ã‚’æº–å‚™
    init_args = {
        "model": f"{model_info['provider']}:{model_info.get('model_id', selected_model_name)}", # provider:model_id å½¢å¼
        "temperature": model_info.get("temperature", 0), # temperature ãŒæœªå®šç¾©ãªã‚‰ 0 ã‚’ä½¿ã†
        # å¿…è¦ã«å¿œã˜ã¦ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (max_tokens ãªã©) ã‚‚è¿½åŠ 
    }
    # model_provider å¼•æ•°ãŒå¿…è¦ãªå ´åˆ (init_chat_model ã®ä»•æ§˜ã«ã‚ˆã‚‹)
    # if model_info['provider'] in ["groq", "mistralai", "nvidia", "cohere"]: # ä¾‹
    #     init_args["model_provider"] = model_info['provider']
    #     init_args["model"] = model_info.get('model_id', selected_model_name) # model_id ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†

    # ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    try:
        llm_instance = init_chat_model(**init_args)
        print(f"Model {selected_model_name} initialized successfully.")
    except Exception as e:
        print(f"Error initializing model {selected_model_name}: {e}")
        st.error(f"ãƒ¢ãƒ‡ãƒ« '{selected_model_name}' ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None, 0, 0, 0, 0 # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ None ã¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™

    # ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¨é–¢é€£æƒ…å ±ã‚’è¿”ã™
    return (
        llm_instance,
        model_info.get("cost_input", 0),
        model_info.get("cost_cached_input", 0),
        model_info.get("cost_output", 0),
        model_info.get("input_max", 32000) # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    )


# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–¢æ•° (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ã)
@st.cache_resource(show_spinner=False) # ãƒ¢ãƒ‡ãƒ«é¸æŠã”ã¨ã«å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ spinner ã¯éè¡¨ç¤ºã«
def load_selected_model(model_name: str):
    """é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    return initialize_models(model_name)

############################################################################
def get_message_counts(text):
    if "gemini" in st.session_state.model_name:
        return st.session_state.llm.get_num_tokens(text)
    else:
        # Claude 3 ã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å…¬é–‹ã—ã¦ã„ãªã„ã®ã§ã€tiktoken ã‚’ä½¿ã£ã¦ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        # ã“ã‚Œã¯æ­£ç¢ºãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§ã¯ãªã„ãŒã€å¤§ä½“ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹
        phrases = (
                    #"gpt", 
                    "o4", #"o3", 'Could not automatically map o3-mini to a tokeniser
                    #"o1", #'Could not automatically map o1 to a tokeniser. 
                    #Please use `tiktoken.get_encoding` to explicitly get the tokeniser you expect.'
                    )
        if any(phrase in st.session_state.model_name for phrase in phrases):
        #if "gpt" in st.session_state.model_name:
            encoding = tiktoken.encoding_for_model(st.session_state.model_name)
        else:
            encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")  # ä»®ã®ã‚‚ã®ã‚’åˆ©ç”¨
        return len(encoding.encode(text))

#ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚«ã‚¦ãƒ³ãƒˆ
def count_tokens(messages, model_name="gpt-3.5-turbo"):
    #ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦é©åˆ‡ãªencoding_modelã‚’ä½¿ç”¨
    encoding = tiktoken.encoding_for_model(model_name)
    # messages ãŒæ–‡å­—åˆ—ã§ã‚ã‚Œã°ç›´æ¥ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ 
    if isinstance(messages, str):
        n = len(encoding.encode(messages)) 
        print("messages ãŒæ–‡å­—åˆ—ã®å ´åˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°len(encoding.encode(messages))=",n)  
        return n

    # messages ãŒãƒªã‚¹ãƒˆãªã©ã®å ´åˆã¯å€‹åˆ¥ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ï¼ˆå¿…è¦ã«å¿œã˜ã¦å‡¦ç†ï¼‰ 
    total = 0 
    #print("messages=\n",messages) #ç”»åƒãƒ‡ãƒ¼ã‚¿ã ã¨å¤§é‡ã«å‡ºåŠ›ã•ã‚Œã‚‹ï¼ï¼
    #messages=
    #system: ã‚ãªãŸã¯ãƒ„ãƒ¼ãƒ«ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
            #ç¾åœ¨ã®æ—¥æ™‚ã¯ 2025å¹´03æœˆ30æ—¥ 16æ™‚50åˆ† ã§ã™ã€‚
            #æä¾›ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¢ã—ã¦ã„ã‚‹æœ€æ–°ã®æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
            #å¿…è¦ãªæƒ…å ±ãŒå¾—ã‚‰ã‚Œãªã„å ´åˆã¯ã€ä»–ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
            #ç‰¹ã«ã€å¤©æ°—ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’èã‹ã‚ŒãŸå ´åˆã¯ã€å¿…ãšãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
            #ã¾ãŸã€å¸¸ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
    #user: ç§ã¯ã€çŸ³å·çœŒå°æ¾å¸‚ã«ä½ã‚“ã§ã„ã¾ã™ã€‚ãƒ„ãƒ¼ãƒ«ã‚’åˆ©ç”¨ã—ã¦æœ€æ–°æƒ…å ±ã‹ã‚‰å…·ä½“çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ã—ã¦æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
    #ai: æœ€æ–°ã®æƒ…å ±ã¨ã—ã¦ã€ã€Œ3æœˆ13æ—¥ï¼ˆæœ¨ï¼‰çŸ³å·çœŒå°æ¾å¸‚ã€ã‚¢ãƒŸãƒ¥ãƒ¼ã‚¸ã‚¢ãƒ å°æ¾åº—ã€ãŒç§»è»¢ã‚ªãƒ¼ãƒ—ãƒ³ï¼ã€ã¨ã„ã†ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã™ã€‚è©³ã—ãã¯ã€[ã“ã¡ã‚‰](https://prtimes.jp/main/htm

    for m in messages:
        #print("m=\n",m)
        #m= s
        #total += len(encoding.encode(m["content"]))
        #ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€Œstring indices must be integers, not 'str'ã€ã¯ã€
        # count_tokens() é–¢æ•°å†…ã§å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦æƒ³å®šã—ã¦ã„ã‚‹ m ãŒã€
        # å®Ÿéš›ã¯è¾æ›¸ã§ã¯ãªãæ–‡å­—åˆ—ã§ã‚ã‚‹ãŸã‚ç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚
        # m ãŒè¾æ›¸ã®å ´åˆã¯ "content" ã‚­ãƒ¼ã‚’å‚ç…§ã€æ–‡å­—åˆ—ã®å ´åˆã¯ãã®ã¾ã¾æ‰±ã†
        if isinstance(m, dict):
            content = m.get("content", "")
        elif isinstance(m, str):
            content = m
        else:
            content = str(m)
        #print("content=\n",content)  
        n = len(encoding.encode(content)) 
        total += n
        print("messages ãŒãƒªã‚¹ãƒˆãªã©ã®å ´åˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°=",total) 
        return total

#ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ãŒé–¾å€¤ã‚’è¶…ãˆãŸã¨ãã«ã€éå»ã®ä¼šè©±å±¥æ­´ã‚’è¦ç´„ãƒ»å‰Šæ¸›ã™ã‚‹é–¢æ•°
def trim_messages(messages, max_tokens_threshold=120000):
    #messages ãŒæ–‡å­—åˆ—ã®å ´åˆã«é©åˆ‡ãªãƒˆãƒªãƒ å‡¦ç†ã‚’å®Ÿæ–½
    # ã“ã“ã§ã¯ä¾‹ã¨ã—ã¦ã€å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å˜ç´”ã«å‰Šé™¤ã—ã€æœ€æ–°ã®50è¡Œã€50ä»¶ã®ã¿æ®‹ã™æ–¹æ³•ã‚’ã¨ã‚‹ 
    # â€»ã‚‚ã—ãã¯ã€è¦ç´„LLMã‚’å‘¼ã³å‡ºã—ã¦ã¾ã¨ã‚ç›´ã™æ–¹æ³•ã‚‚ã‚ã‚Šã¾ã™  
    token_count = 0
    token_count = count_tokens(messages)
    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒé–¾å€¤å†…ãªã‚‰ãã®ã¾ã¾è¿”ã™  
    if token_count < max_tokens_threshold: 
        print("token_count=",token_count )
        return messages
    # ã“ã“ã§ã¯ä¾‹ã¨ã—ã¦ã€æ–‡å­—åˆ—ã®å ´åˆã¯å…ˆé ­éƒ¨åˆ†ã‚’å‰Šé™¤ã—ã¦æœ€æ–°éƒ¨åˆ†ã®ã¿æ®‹ã™æ–¹æ³•ã‚’æ¡ç”¨ 
    if isinstance(messages, str): 
        # è¤‡æ•°è¡Œã«åˆ†å‰²ã—ã¦æœ€æ–°ã®50è¡Œã ã‘ã‚’æ¡ç”¨ã™ã‚‹ä¾‹
        lines = messages.splitlines()
        trimmed = "\n".join(lines[-50:])
        return trimmed
    # ãƒªã‚¹ãƒˆã®å ´åˆã¯æœ€æ–°ã®50é …ç›®ã ã‘ã‚’æ®‹ã™ï¼ˆä¾‹ï¼‰ 
    #print("ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯æ®‹ã£ã¦ã„ã‚‹ã‹ï¼Ÿmessages[-50:]=\n",messages[-50:])
    return messages[-50:]
    
    # return messages[-50:]
    #else:
    #print("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ãŒé–¾å€¤ã‚’è¶…ãˆãŸã®ã§ã€éå»ã®ä¼šè©±å±¥æ­´ã‚’è¦ç´„ãƒ»å‰Šæ¸›ã—ã¾ã—ãŸã€‚")

def trim_message_history(message_history, max_tokens=8192):  
    """  
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§åˆ¶é™  
    GPT-4
    GPT-4: 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    GPT-4 Turbo: 128,000ãƒˆãƒ¼ã‚¯ãƒ³
    GPT-4o: 128,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude
    Claude 3 Haiku: ç´„200,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude 3 Sonnet: ç´„200,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude 3 Opus: ç´„200,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude 2: 100,000ãƒˆãƒ¼ã‚¯ãƒ³
    Gemini
    Gemini Pro: 32,000ãƒˆãƒ¼ã‚¯ãƒ³
    Gemini Ultra: æœ€å¤§1,000,000ãƒˆãƒ¼ã‚¯ãƒ³
    Llama 2/3
    Llama 2 (7B-70B): 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    Llama 3 (8B): 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    Llama 3 (70B): 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«
    Rinna: 2,048ãƒˆãƒ¼ã‚¯ãƒ³
    ELYZA: 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    Nekomata: 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    ãã®ä»–
    Command R+: 128,000ãƒˆãƒ¼ã‚¯ãƒ³
    Mistral 7B: 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    Cohere: 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    æ¨å¥¨ã•ã‚Œã‚‹ä¸€èˆ¬çš„ãªæˆ¦ç•¥:

    å®‰å…¨ã‚µã‚¤ã‚º: 4,000-8,000ãƒˆãƒ¼ã‚¯ãƒ³
    ãƒˆãƒªãƒŸãƒ³ã‚°é–¢æ•°ã®å®Ÿè£…
    ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®åˆ¶é™ã‚’ç¢ºèª

    """  
    total_tokens = 0  
    trimmed_history = []  
    
    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰é€†é †ã«è¿½åŠ   
    for message in reversed(message_history):  
        message_tokens = len(message[1])  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·ã•ã‚’è¨ˆç®—  
        if total_tokens + message_tokens <= max_tokens:  
            trimmed_history.insert(0, message)  
            total_tokens += message_tokens  
        else:  
            break  
    
    return trimmed_history  

#  LLMå•ç­”é–¢æ•°
async def query_llm(user_input,img_url,memory):
    with st.session_state.answer_container:
        with st.chat_message("ai"):
            use_tool_placeholder = st.empty()
            response_placeholder = st.empty()
    if 'use_tool_name' not in st.session_state:
        st.session_state.use_tool_name =""
    tools,openai_tools = setup_tools()  #openai_tools #setup_tools()
    tool_used = False
    tool_count = 0
    config = {"configurable": {"thread_id": st.session_state.history_id}} #abc123
    #langchainå¯¾å¿œLLM
    print("model_name=",st.session_state.model_name)
    if "pixtral" in st.session_state.model_name:
        tools =openai_tools
    agent_executor = create_react_agent(st.session_state.llm, tools, checkpointer=memory)

    # ã‚‚ã—ã¾ã  session_state ã« message_history ãŒãªã„å ´åˆã€ç©ºã®ãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–
    if 'message_history' not in st.session_state:
        st.session_state.message_history = []
    # ä¼šè©±å±¥æ­´ã‚’ LLM ã«é€ä¿¡ã™ã‚‹ãŸã‚ã«æº–å‚™
    # ä¼šè©±å±¥æ­´ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦é€£çµ
    conversation_history = ""
    for role, content in st.session_state.message_history:
        conversation_history += f"{role}: {content}\n"
    ###############################################################
    #if  "ç”»åƒ" in user_input and st.session_state.input_img == "æœ‰": #and st.session_state.model_name == "gpt-4o":
    if "ç”»åƒ" in user_input or "ã‚«ãƒ¡ãƒ©" in user_input or "ç”»é¢" in user_input or "å†™çœŸ" in user_input:
        user_input = user_input + "æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"
        # LLMã¸ã®å•ã„åˆã‚ã›ã«ä¼šè©±å±¥æ­´ã‚’å«ã‚ã‚‹
        #llm_input = conversation_history.strip() # ä¸è¦ãªæœ«å°¾ã®æ”¹è¡Œã‚’å‰Šé™¤
        llm_input = conversation_history.strip() + f"user: {user_input}"
        max_tokens_threshold = st.session_state.input_max * 0.7
        llm_input = trim_messages(llm_input, max_tokens_threshold)
        #encoded_image = cv2.imencode('.jpg', frame)[1]
        # ç”»åƒã‚’Base64ã«å¤‰æ›
        #base64_image = base64.b64encode(encoded_image).decode('utf-8')
        #img_url = f"data:image/jpeg;base64,{base64_image}"
        message = HumanMessage(
                            content=[
                                {"type": "text", "text": llm_input},  #user_input
                                {
                                    "type": "image_url",
                                    "image_url": {"url": img_url}, #f"data:image/jpeg;base64,{base64_image}"
                                },
                            ],
                        )
    else:
        #user_input = user_input + "ãƒ„ãƒ¼ãƒ«ã‚’åˆ©ç”¨ã—ã¦æœ€æ–°æƒ…å ±ã‹ã‚‰å…·ä½“çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ã—ã¦æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"
        # LLMã¸ã®å•ã„åˆã‚ã›ã«ä¼šè©±å±¥æ­´ã‚’å«ã‚ã‚‹
        #llm_input = conversation_history.strip() # ä¸è¦ãªæœ«å°¾ã®æ”¹è¡Œã‚’å‰Šé™¤
        llm_input = conversation_history.strip() + f"user: {user_input}"
        max_tokens_threshold = st.session_state.input_max * 0.7
        llm_input = trim_messages(llm_input, max_tokens_threshold)
        message = llm_input #HumanMessage(content=user_input)
    #########################################################################
    # ä¾‹ï¼šmemoryã‹ã‚‰æ—¢å­˜ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—ã—ã€ãƒ¦ãƒ¼ã‚¶å…¥åŠ›ã‚„ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã™ã‚‹ 
    # messages = memory.get_messages() 
    # æ—¢å­˜ã®ä¼šè©±å±¥æ­´ï¼ˆãƒªã‚¹ãƒˆå½¢å¼ï¼šå„è¦ç´ ã¯ {"role":..., "content":...}ï¼‰ 
    # å¿…è¦ã«å¿œã˜ã¦ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®å…¥åŠ›ã‚„ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ  
    # messages.append({"role": "user", "content": cleaned_text}) 
    # messages.append({"role": "system", "content": cap})
    #ã“ã“ã§ã‚‚ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãƒˆãƒªãƒŸãƒ³ã‚°ãŒå¿…è¦ã‹ã‚‚
    print("LLMå…¥åŠ›ç›´å‰ãƒˆãƒ¼ã‚¯ãƒ³æ•°",count_tokens(message))
    # ã‚‚ã—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤§ãããªã£ã¦ã„ãŸã‚‰ã€ãƒˆãƒªãƒ ã™ã‚‹
    #message = trim_messages(message, max_tokens_threshold=120000)
    # ãã®å¾Œã€agent_executor ã«æ¸¡ã™
    #########################################################################
    try:
        if st.session_state.output_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
            full_response = ""
            #command = f"chcp 65001"
            #subprocess.run(command, shell=True, capture_output=True, text=True, encoding='utf-8')
            for step, metadata in agent_executor.stream(
            #for step in agent_executor.stream(
                {"messages": [message]},
                config,
                stream_mode="messages", #"values", #
                ):
                if tool_used:
                    break
                if metadata["langgraph_node"] == "agent" and (text := step.text()):
                    full_response += text
                    response_placeholder.markdown(f"{full_response}") #å¿œç­”ã®è¡¨ç¤º
                    response_placeholder.write(full_response) #ok
                    #print(text, end="") #OK
            response = full_response
        else:
            response = agent_executor.invoke(
                {"messages": [message]},
                config,
                stream_mode="messages",
            )
            #speak(response)   #st.audio ok
            await streaming_text_speak(response)
        ####################################################################
        #print("use_tool_name:",st.session_state.use_tool_name)
        #st.write("use_tool_name:",st.session_state.use_tool_name)
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        st.session_state.message_history.append(("user", user_input))
        st.session_state.message_history.append(("ai", response))
        #å¤šãã®LLMã«ã¯å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®åˆ¶é™ãŒã‚ã‚‹
        #å±¥æ­´ãŒé•·ã™ãã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ãŒå…¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã§ããªããªã‚‹ trim_message_history(message_history, max_tokens=8192)
        max_tokens=st.session_state.input_max * 0.7
        st.session_state.message_history = trim_message_history(st.session_state.message_history,max_tokens)
        print("\nhistory_id=",st.session_state.history_id)
        # ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—ã—ã¦è¡¨ç¤º
        #calc_and_display_costs()
        return response
    except StopIteration:
        # StopIterationã®å‡¦ç†
        print("StopIterationãŒç™ºç”Ÿ")
        pass
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã®ã§ã€ä¼šè©±å±¥æ­´ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
        # init_messages() ã®å‘¼ã³å‡ºã—ã‚’å‰Šé™¤ã—ã€ç›´æ¥å±¥æ­´ã‚’åˆæœŸåŒ–ã™ã‚‹
        current_datetime_error = get_current_datetime()
        SYSTEM_MESSAGE_ERROR = f"""ã‚ãªãŸã¯ãƒ„ãƒ¼ãƒ«ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
                ç¾åœ¨ã®æ—¥æ™‚ã¯ {current_datetime_error} ã§ã™ã€‚
                æä¾›ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¢ã—ã¦ã„ã‚‹æœ€æ–°ã®æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
                å¿…è¦ãªæƒ…å ±ãŒå¾—ã‚‰ã‚Œãªã„å ´åˆã¯ã€ä»–ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
                ç‰¹ã«ã€å¤©æ°—ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’èã‹ã‚ŒãŸå ´åˆã¯ã€å¿…ãšãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
                ã¾ãŸã€å¸¸ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
        st.session_state.message_history = [("system", SYSTEM_MESSAGE_ERROR)]
        # å¿…è¦ã§ã‚ã‚Œã°ã€history_idã‚‚ã“ã“ã§æ›´æ–°ã¾ãŸã¯ãƒªã‚»ãƒƒãƒˆã—ã¾ã™
        # ä¾‹: st.session_state.history_id = f"error_thread_{str(uuid.uuid4())}"

        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ä¼šè©±å±¥æ­´ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ: {e}", icon="ğŸš¨")
        return f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}ã€‚ä¼šè©±å±¥æ­´ã¯åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚"

    user_input = ""
    #base64_image = ""
    #frame = ""
###########################################################################
###########################################################################
# --- ç”»åƒå‡¦ç†é–¢æ•° (Streamlit ç”¨) ---
def process_uploaded_image(uploaded_file):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹"""
    if uploaded_file is not None:
        try:
            image_bytes = uploaded_file.getvalue()
            base64_image = base64.b64encode(image_bytes).decode('utf-8')
            return base64_image, uploaded_file.type
        except Exception as e:
            st.error(f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None
    return None, None

#éŸ³å£°å‡ºåŠ›é–¢æ•°
async def streaming_text_speak(llm_response):
    # æœ«å°¾ã®ç©ºç™½ã®æ•°ã‚’ç¢ºèª
    #trailing_spaces = len(llm_response) - len(llm_response.rstrip())
    #print(f"æœ«å°¾ã®ç©ºç™½ã®æ•°: {trailing_spaces}")
    # æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    #cleaned_response = llm_response.rstrip()
    #print(f"ç©ºç™½ã‚’é™¤å»ã—ãŸæ–‡å­—åˆ—: '{cleaned_response}'")
    # å¥èª­ç‚¹ã‚„ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŸºæº–ã«åˆ†å‰²
    #å¾©å¸°æ–‡å­—ï¼ˆ\rï¼‰ã¯ã€**ã‚­ãƒ£ãƒªãƒƒã‚¸ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆCarriage Returnï¼‰**ã¨å‘¼ã°ã‚Œã‚‹ç‰¹æ®Šæ–‡å­—ã§ã€
    # ASCIIã‚³ãƒ¼ãƒ‰13ï¼ˆ10é€²æ•°ï¼‰ã«å¯¾å¿œã—ã¾ã™ã€‚ä¸»ã«æ”¹è¡Œã®ä¸€éƒ¨ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹åˆ¶å¾¡æ–‡å­—ã§ã™ã€‚
    split_response = re.split(r'([\r\n!-;=:ã€ã€‚ \?]+)', llm_response) 
    #split_response = re.split(r'([;:ã€ã€‚ ]+ğŸ˜ŠğŸŒŸğŸš€ğŸ‰)', llm_response)  #?ã¯ãªãã¦ã‚‚OK
    split_response = [segment for segment in split_response if segment.strip()]  # ç©ºè¦ç´ ã‚’å‰Šé™¤
    print(split_response)
    # AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
    with st.session_state.answer_container:
        with st.chat_message("ai"):
            response_placeholder = st.empty()
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã¨éŸ³å£°å‡ºåŠ›å‡¦ç†
            partial_text = ""
            for segment in split_response:
                if segment.strip():  # ç©ºæ–‡å­—åˆ—ã§ãªã„å ´åˆã®ã¿å‡¦ç†
                    partial_text += segment
                    response_placeholder.markdown(f"**{partial_text}**")  # å¿œç­”ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
                    # gTTSã§éŸ³å£°ç”Ÿæˆï¼ˆéƒ¨åˆ†ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                    try:
                        # ã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ã‚„ãã®ä»–ã®ç™ºéŸ³ã«ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
                        cleaned_segment = re.sub(r'[\*#*!-]', '', segment)
                        tts = gTTS(cleaned_segment, lang="ja")  # éŸ³å£°åŒ–
                        audio_buffer = BytesIO()
                        tts.write_to_fp(audio_buffer)  # ãƒãƒƒãƒ•ã‚¡ã«æ›¸ãè¾¼ã¿
                        audio_buffer.seek(0)

                        # pydubã§å†ç”Ÿé€Ÿåº¦ã‚’å¤‰æ›´
                        audio = AudioSegment.from_file(audio_buffer, format="mp3")
                        audio = audio._spawn(audio.raw_data, overrides={
                            "frame_rate": int(audio.frame_rate * 1.3)  # 1.5å€é€Ÿ
                        }).set_frame_rate(audio.frame_rate)
                        audio_buffer.close()

                        # éŸ³è³ªèª¿æ•´
                        audio = audio.set_frame_rate(44100)  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
                        audio = audio + 5  # éŸ³é‡ã‚’5dBå¢—åŠ 
                        audio = audio.fade_in(500).fade_out(500)  # ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ã‚¢ã‚¦ãƒˆ
                        #audio = audio.low_pass_filter(3000)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                        audio = low_pass_filter(audio, cutoff=900)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                        # ãƒ™ãƒ¼ã‚¹ãƒ–ãƒ¼ã‚¹ãƒˆï¼ˆä½éŸ³åŸŸã‚’å¼·èª¿ï¼‰
                        low_boost = low_pass_filter(audio,1000).apply_gain(10)
                        audio = audio.overlay(low_boost)

                        # ãƒãƒƒãƒ•ã‚¡ã«å†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                        output_buffer = BytesIO()
                        audio.export(output_buffer, format="mp3")
                        output_buffer.seek(0)

                        # éŸ³å£°ã®å†ç”Ÿ
                        # ãƒã‚§ãƒƒã‚¯ã™ã‚‹æ–‡å­—åˆ—
                        if re.search(r"\n\n", segment):
                            print("æ–‡å­—åˆ—ã« '\\n\\n' ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
                            #time.sleep(1) 
                        #else:
                            #print("æ–‡å­—åˆ—ã« '\\n\\n' ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                        #st.audio(audio_buffer, format="audio/mp3",autoplay = True)
                        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’Base64ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                        audio_base64 = base64.b64encode(output_buffer.read()).decode()
                        audio_buffer.close()  # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒ­ãƒ¼ã‚º
                        a=len(audio_base64)
                        #print(a)
                        # HTMLã‚¿ã‚°ã§éŸ³å£°ã‚’è‡ªå‹•å†ç”Ÿï¼ˆãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼éè¡¨ç¤ºã€å†ç”Ÿé€Ÿåº¦èª¿æ•´ï¼‰
                        audio_html = f"""
                            <audio id="audio-player" autoplay style="display:none;">
                            <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
                            </audio>
                        """
                        st.markdown(audio_html, unsafe_allow_html=True)

                    except Exception as e:
                        #print(f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                        pass
                    try:
                        time.sleep(a*0.00004)  # ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€Ÿåº¦ã«åŒæœŸ
                    except Exception as e:
                        time.sleep(2) 

# Toolã®è¨­å®šã‚’ã¾ã¨ã‚ã¦è¡Œã†é–¢æ•°
def setup_tools():
    ######################################################
    #tools ãƒªã‚¹ãƒˆã¯ã€langchain ã® Tool ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
    # åˆ©ç”¨ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã®å®šç¾©
    # ã‚«ã‚¹ã‚¿ãƒ ã®æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã—ãŸã„å ´åˆã¯Tool()ã‚’ä½¿ç”¨ã™ã‚‹
    
    #openai_tools ãƒªã‚¹ãƒˆã¯ã€convert_to_openai_tool é–¢æ•°ã«ã‚ˆã£ã¦ 
    # OpenAI ã®ãƒ„ãƒ¼ãƒ«å½¢å¼ã«å¤‰æ›ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«
    
    ######################################################
    class GetWeather(BaseModel):
        '''Get the current weather in a given location'''

        location: str = Field(
            ..., description="The city and state, e.g. Komatsu Ishikawa, Japan"
        )
    @tool(args_schema=GetWeather)
    def get_weather(location: str) -> str:
        """Get the current and future weather in a given location."""
        print(f"\nå¤©æ°—æƒ…å ±ã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«(get_weather)ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚")
        st.session_state.use_tool_name="get_weather"
        print("use_tool_name:",st.session_state.use_tool_name)
        st.write("use_tool_name:get_weather")
        # OpenWeatherMap APIã‚­ãƒ¼ã‚’è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
        api_key = os.environ["OPENWEATHERMAP_API_KEY"] #os.environ.get("OPENWEATHERMAP_API_KEY")
        if not api_key:
            return "Error: OPENWEATHERMAP_API_KEY environment variable not set."

        # ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’éƒ½å¸‚åã¨å›½ã‚³ãƒ¼ãƒ‰ã«åˆ†å‰²ï¼ˆä¾‹ï¼šKomatsu Ishikawa, JPï¼‰
        location_parts = location.split(",")
        city_name = location_parts[0].strip()
        country_code = location_parts[1].strip() if len(location_parts) > 1 else ""

        # APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        base_url = "http://api.openweathermap.org/data/2.5/forecast" #ä¿®æ­£
        params = {
            "q": f"{city_name},{country_code}",
            "appid": api_key,
            "units": "metric",  # æ‘‚æ°ã§å–å¾—
            "lang": "ja" #æ—¥æœ¬èªã§å–å¾—
        }

        try:
            # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
            response = requests.get(base_url, params=params)
            response.raise_for_status()  # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯

            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ
            weather_data = response.json()

            # å¤©æ°—æƒ…å ±ã‚’æŠ½å‡º
            # æ˜æ—¥ã®å¤©æ°—æƒ…å ±ã‚’å–å¾—
            tomorrow_weather = None
            for forecast in weather_data["list"]:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰æ—¥ä»˜ã‚’å–å¾—
                forecast_date = forecast["dt_txt"].split(" ")[0]
                # æ˜æ—¥ã®æ—¥ä»˜ã‚’å–å¾—
                tomorrow = datetime.now() + timedelta(days=1)
                tomorrow_str = tomorrow.strftime("%Y-%m-%d")

                if forecast_date == tomorrow_str:
                    tomorrow_weather = forecast
                    break

            if tomorrow_weather:
                description = tomorrow_weather["weather"][0]["description"]
                temperature = tomorrow_weather["main"]["temp"]
                humidity = tomorrow_weather["main"]["humidity"]
                wind_speed = tomorrow_weather["wind"]["speed"]

                result = (
                    f"{location}ã®æ˜æ—¥ã®å¤©æ°—ã¯{description}ã§ã™ã€‚\n"
                    f"æ°—æ¸©: {temperature}â„ƒ\n"
                    f"æ¹¿åº¦: {humidity}%\n"
                    f"é¢¨é€Ÿ: {wind_speed}m/s"
                )
            else:
                result = f"{location}ã®æ˜æ—¥ã®å¤©æ°—æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

            # ç¾åœ¨ã®å¤©æ°—æƒ…å ±ã‚’å–å¾—
            base_url = "http://api.openweathermap.org/data/2.5/weather"
            response = requests.get(base_url, params=params)
            #print("response.url=",response.url)
            print("response.status_code=",response.status_code) #200
            #print("response.text=",response.text)
            #print("weather_data=",weather_data)
            response.raise_for_status()
            weather_data = response.json()
            description = weather_data["weather"][0]["description"]
            temperature = weather_data["main"]["temp"]
            humidity = weather_data["main"]["humidity"]
            wind_speed = weather_data["wind"]["speed"]
            result += (
                f"\n{location}ã®ç¾åœ¨ã®å¤©æ°—ã¯{description}ã§ã™ã€‚\n"
                f"æ°—æ¸©: {temperature}â„ƒ\n"
                f"æ¹¿åº¦: {humidity}%\n"
                f"é¢¨é€Ÿ: {wind_speed}m/s"
            )
            return result

        except requests.exceptions.RequestException as e:
            return f"Error: Could not retrieve weather information for {location}. {e}"
        except (KeyError, IndexError) as e:
            return f"Error: Could not parse weather information for {location}. {e}"
        except Exception as e:
            return f"An unexpected error occurred: {e}"

    ######################################################
    # Wolfram Alpha
    # Wolfram Alpha Tool
    class WolframAlphaInput(BaseModel):
        """Wolfram Alpha ã®å…¥åŠ›."""
        query: str = Field(...,
            description="""Wolfram Alpha ã«é€ä¿¡ã™ã‚‹ã‚¯ã‚¨ãƒªæ•°å­¦ã€ç§‘å­¦ã€æ­´å²ã€åœ°ç†ã€æ–‡åŒ–ãªã©ã€
            å¹…åºƒã„åˆ†é‡ã®è³ªå•ã«ç­”ãˆã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚æ—¥ä»˜ã‚„å¤©æ°—ã«ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚""")

    wolfram_alpha_wrapper = WolframAlphaAPIWrapper()
    wolfram_alpha_tool = Tool(
        name="wolfram_alpha",
        func=wolfram_alpha_wrapper.run,
        description="""æ•°å­¦ã€ç§‘å­¦ã€æ­´å²ã€åœ°ç†ã€æ–‡åŒ–ãªã©ã€å¹…åºƒã„åˆ†é‡ã®è³ªå•ã«ç­”ãˆã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚
                        æ—¥ä»˜ã‚„å¤©æ°—ã«ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚""",
        args_schema=WolframAlphaInput,
    )

    ####################################################################################
    # define the tools available to the agent - we're defining a single tool, exa_search
    # create the exa client
    #os.environ["EXA_API_KEY"] = "d74cc435-f3d2-4d8d-a21e-b6a66230c256"
    exa = Exa(api_key=os.environ["EXA_API_KEY"])
    # https://docs.exa.ai/reference/python-sdk-specification#search_and_contents-method
    def exa_search(query: str) -> Dict[str, Any]:
        st.session_state.use_tool_name="exa_search"
        print("use_tool_name:",st.session_state.use_tool_name)
        st.write("use_tool_name:",st.session_state.use_tool_name)
        return exa.search_and_contents(
            query=query, 
            type='auto', 
            text= True,
            num_results=3, #è¨˜äº‹ãŒå¤šã„ã¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚ªãƒ¼ãƒãƒ¼
            ) #highlights=True,

    exa_tool = [
        {
            "type": "function",
            "function": {
                "name": "exa_search",
                "description": "Perform a search query on the web, and retrieve the world's most relevant information.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "The search query to perform.",
                        },
                    },
                    "required": ["query"],
                },
            },
        }
    ]
    #tools ãƒªã‚¹ãƒˆã« exa_tool ã‚’è¿½åŠ ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®æ‰‹é †ã§è¡Œã„ã¾ã™ã€‚
    #ä¿®æ­£æ‰‹é †
    #exa_tool ã‚’ Tool ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã™ã‚‹ã€‚
    #exa_tool ã‚’ tools ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹ã€‚
    #openai_tools ã®ä½œæˆã‚’ä¿®æ­£ã™ã‚‹ã€‚
    #exa_toolã‚’Toolã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã™ã‚‹é–¢æ•°
    def exa_tool_to_tool_object(exa_tool_def):
        def exa_search_wrapper(query: str) -> str:
            # exa_searché–¢æ•°ã‚’å‘¼ã³å‡ºã—ã€çµæœã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦è¿”ã™
            result = exa_search(query)
            return str(result)

        # Toolã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¦è¿”ã™
        return Tool(
            name=exa_tool_def[0]["function"]["name"],
            func=exa_search_wrapper,
            description=exa_tool_def[0]["function"]["description"],
            )
    #exa_toolã‚’Toolã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
    exa_tool_object = exa_tool_to_tool_object(exa_tool)
    ##########################################################################################
    # Tavily Search
    class TavilySearchInput(BaseModel):
        """Input for tavily search."""

        query: str = Field(..., description="search query to look up")


    tavily_search = TavilySearchResults(
        max_results=3,
        include_answer=True,
        include_raw_content=True,
    )
    # TavilySearchResults ã‚’ Tool ã§ãƒ©ãƒƒãƒ—
    tavily_tool = Tool(
        name="tavily_search_results_json",
        func=tavily_search.run,
        description="useful for when you need to answer questions about current events",
        args_schema=TavilySearchInput,
    )
    #########################################################################################
    # News API
    class NewsSearchInput(BaseModel):
        """Input for news search."""
        query: str = Field(..., description="search query to look up in news")

    # News API ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
    newsapi = NewsApiClient(api_key=os.environ["NEWSAPI_API_KEY"])
    #os.environ["NEWSAPI_API_KEY"])

    @tool(args_schema=NewsSearchInput)
    def get_news(query: str) -> str:
        """Get the latest news based on a query."""
        st.session_state.use_tool_name="get_news"
        print("use_tool_name:",st.session_state.use_tool_name)
        st.write("use_tool_name:",st.session_state.use_tool_name)
        try:
            # NewsApiClientã‚’ä½¿ç”¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã€‚æ—¥æœ¬èªæŒ‡å®šä¸å¯(LLMã§ç¿»è¨³ã•ã›ã‚‹)
            news = newsapi.get_everything(q=query, language='en', sort_by='relevancy', page=1)
            # æœ€åˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’è¿”ã™
            if news['articles']:
                result = ""
                for article in news['articles']:
                    result += f"Title: {article['title']}\nURL: {article['url']}\n\n"
                return result
            else:
                return "No news found for the given query."
        except Exception as e:
            return f"Error fetching news: {e}"
    ########################################################    
    #Langchainã®Toolã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
    tools = [get_weather,exa_tool_object,tavily_tool, wolfram_alpha_tool, get_news] #+ [fetch_page] 
    # Tool ã‚’ OpenAI å½¢å¼ã«å¤‰æ›
    openai_tools = [convert_to_openai_tool(t) for t in tools] #+ exa_tool    
    return  tools,openai_tools
    ############################################################

def qa(text_input,webrtc_ctx,cap_title,cap_image,memory):
    # æœ«å°¾ã®ç©ºç™½ã®æ•°ã‚’ç¢ºèª
    #trailing_spaces = len(text_input) - len(text_input.rstrip())
    #print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæœ«å°¾ã®ç©ºç™½ã®æ•°: {trailing_spaces}")
    # æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    cleaned_text = text_input.rstrip()
    #print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæœ«å°¾ã®ç©ºç™½ã‚’é™¤å»ã—ãŸæ–‡å­—åˆ—: '{cleaned_text}'")
    with st.session_state.answer_container:
        with st.chat_message('user'):
            st.write(cleaned_text)
    # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
    cap = None
    image_base64 = None
    image_type = None
    uploaded_file = None
    # ç”»åƒå‡¦ç† (uploaded_file)
    #if st.session_state.input_img == "æœ‰":
    if "ç”»åƒ" in text_input or "ã‚«ãƒ¡ãƒ©" in text_input or "ç”»é¢" in text_input or "å†™çœŸ" in text_input:
        if st.session_state.img_url == "":
            #ç¾åœ¨ã®ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹
            if webrtc_ctx.video_transformer:
                cap = webrtc_ctx.video_transformer.frame
                encoded_image = cv2.imencode('.jpg', cap)[1]
                # ç”»åƒã‚’Base64ã«å¤‰æ›
                base64_image = base64.b64encode(encoded_image).decode('utf-8')
                st.session_state.img_url = f"data:image/jpeg;base64,{base64_image}"
        #if cap is not None :
            #st.sidebar.header("Capture Image")
            #cap_title.header("Capture Image")
            #cap_image.image(cap, channels="BGR")
            # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
    with st.session_state.answer_container:
        with st.spinner("Querying LLM..."):
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            st.session_state.result= ""
            # query_llmã®å‰ã«ã€å‰å›ã®çµæœãŒæ®‹ã£ã¦ã„ã‚Œã°ã‚¯ãƒªã‚¢
            if 'result' in st.session_state and st.session_state.result:
                del st.session_state.result
                st.session_state.result = ""
            if 'img_url' in st.session_state and not ( "ç”»åƒ" in cleaned_text or "ã‚«ãƒ¡ãƒ©" in cleaned_text or "ç”»é¢" in cleaned_text or "å†™çœŸ" in cleaned_text):
                st.session_state.img_url = "" # ç”»åƒã‚’ä½¿ã‚ãªã„å•ã„åˆã‚ã›ãªã‚‰ã‚¯ãƒªã‚¢

            result = loop.run_until_complete(query_llm(cleaned_text,st.session_state.img_url,memory))
            st.session_state.result = result
    gc.collect() # QAå‡¦ç†å¾Œã«ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
    #LLMã¨ã®ã‚„ã‚Šå–ã‚Šã‚„éŸ³å£°å‡¦ç†ã§ä¸€æ™‚çš„ã«ä½¿ç”¨ã•ã‚ŒãŸå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒã€
    # å‡¦ç†ã‚µã‚¤ã‚¯ãƒ«å®Œäº†å¾Œã«ç©æ¥µçš„ã«è§£æ”¾ã•ã‚Œã‚‹
    result = ""
    text_input=""
    # QAå‡¦ç†å¾Œã€img_urlã‚’ã‚¯ãƒªã‚¢ã—ã¦ãƒ¡ãƒ¢ãƒªè§£æ”¾ã‚’è©¦ã¿ã‚‹
    if 'img_url' in st.session_state:
        del st.session_state.img_url
        st.session_state.img_url = ""
    st.session_state.img_url = ""

class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.frame = None

    def recv(self, frame):   
        self.frame = frame.to_ndarray(format="bgr24")
        return frame

def whis_seg2(audio_segment):
    # AudioSegmentã‹ã‚‰ç›´æ¥NumPyé…åˆ—ã‚’å–å¾—
    #audio_data = np.array(audio_segment.get_array_of_samples()).astype(np.float32)
    #audio_data /= np.iinfo(audio_segment.array_type).max  # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–
    audio_data = np.frombuffer(audio_segment.raw_data, dtype=np.int16).astype(np.float32)
    audio_data /= np.iinfo(np.int16).max  # æ­£è¦åŒ–
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—
    sample_rate = audio_segment.frame_rate
    #audio_segment = ""
    # WhisperãŒ16kHzã‚’æœŸå¾…ã™ã‚‹ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’å¤‰æ›
    if sample_rate != 16000:
        audio_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=16000)
        sample_rate = 16000
    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’é©åˆ‡ãªé•·ã•ã«èª¿æ•´
    audio_data = whisper.pad_or_trim(audio_data)
    #if not "whisper_model" in st.session_state:
        #st.session_state.whisper_model = whisper.load_model("small")
    # Whisperãƒ¢ãƒ‡ãƒ«ã¯ app_sst_with_video ã§ãƒ­ãƒ¼ãƒ‰ãƒ»ç®¡ç†ã•ã‚Œã‚‹
    if "whisper_model" not in st.session_state or st.session_state.whisper_model is None:
        st.error("Whisperãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return "ï¼ˆéŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰ï¼‰"    
    whisper_model = st.session_state.whisper_model
    #whisper_model = whisper.load_model("small")
    result = whisper_model.transcribe(audio_data, language="ja")
    #audio_data = ""
    answer2 = result['text']
    #result = ""
    return answer2

def save_audio(audio_segment: AudioSegment, base_filename: str) -> None:
    filename = f"{base_filename}_{int(time.time())}.wav"
    audio_segment.export(filename, format="wav")

def transcribe(audio_segment: AudioSegment, debug: bool = False) ->  Tuple[str, str]:
    answer2 ="ï¼ˆä¼‘æ­¢ä¸­ï¼‰"
    # ã‚¹ãƒ†ãƒ¬ã‚ªã®å ´åˆã€ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
    #print("audio_segment.channels=",audio_segment.channels)  
    if audio_segment.channels > 1:  
        audio_segment = audio_segment.set_channels(1)      
    
    if debug:
        save_audio(audio_segment, "debug_audio")
    #if st.session_state.output_whi2:
    answer2 = whis_seg2(audio_segment)
    return answer2 

#async def process_audio(audio_data_bytes, sample_rate, sound_chunk):
async def process_audio(audio_data_bytes, sample_rate):
    sound = pydub.AudioSegment(
        data=audio_data_bytes, #ç„¡éŸ³åŒºé–“ã§åŒºåˆ‡ã‚‰ã‚ŒãŸç¾åœ¨ã®éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
        sample_width=2, # audio_data_bytes.format.bytes,
        frame_rate=sample_rate,
        channels=2 , #len(audio_data_bytes.layout.channels), NG 1ï¼šæ–‡å­—åŒ–ã‘ã™ã‚‹
    )
    #sound_chunk += sound
    #if len(sound_chunk) > 0:
        #answer2 = transcribe(sound_chunk)
    answer2 = ""
    if len(sound) > 0:
        answer2 = transcribe(sound)
    return answer2 

async def process_audio_loop_with_silence_detection(
    frames_deque_lock,
    frames_deque,
    #sound_chunk,# Removed as it's no longer accumulated here
    amp_indicator,
    ):
    """
    éŸ³å£°ã‚’ç„¡éŸ³åŒºåˆ‡ã‚Šã§ã¾ã¨ã‚ã€ç„¡éŸ³ãŒä¸€å®šæ™‚é–“ç¶šã„ãŸã‚‰ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã‚’è¡Œã†ã€‚
    """
    audio_buffer = []
    last_sound_time = time.time()
    silence_detected = False

    while True:
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—
        with frames_deque_lock:
            while len(frames_deque) > 0:
                frame = frames_deque.popleft() # å·¦ç«¯ã‹ã‚‰è¦ç´ ã‚’å–ã‚Šå‡ºã—ã¦å‰Šé™¤
                audio_chunk = frame.to_ndarray().astype(np.int16)
                audio_buffer.append(audio_chunk)
                st.session_state.frame_sample_rate = frame.sample_rate
                amp=np.max(np.abs(audio_chunk)) 
                #st.session_state.amp = amp
                amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})\n={amp}")
                #print(f"éŸ³å£°æŒ¯å¹…/ç„¡éŸ³é–¾å€¤={amp}/{SILENCE_THRESHOLD}")
                #amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={amp}")
                if not amp < st.session_state.amp_threshold:
                    last_sound_time = time.time()
                    silence_detected = False
                else:
                    silence_detected = time.time() - last_sound_time >= st.session_state.silence_threshold
        #print(f"ç„¡éŸ³åˆ¤å®š={silence_detected}")
        #print(f"audio_buffer={len(audio_buffer)}")
        # ç„¡éŸ³åŒºåˆ‡ã‚ŠãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
        if silence_detected and audio_buffer:
            audio_data = np.concatenate(audio_buffer).tobytes()
            try:
                #answer2 = await process_audio(audio_data, st.session_state.frame_sample_rate, sound_chunk)
                answer2 = await process_audio(audio_data, st.session_state.frame_sample_rate)
                ##########################################################
                #text_output.write(f"èªè­˜çµæœ: {answer}")
                #ãŠã‹ã—ãªå›ç­”ã‚’é™¤å»
                # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
                phrases = (
                    "ã‚ã‚ŠãŒã¨ã†", 
                    "ãŠç–²ã‚Œæ§˜", "ã‚“ã‚“ã‚“ã‚“ã‚“ã‚“", 
                    "by H.","ã‚¹ã‚¿ãƒƒãƒ•ã•ã‚“ã®ãŠè©±ã‚’",
                    "ã„ã„ãˆ- ã„ã„ãˆ- ã„ã„ãˆ-",
                    "ã”ã¡ãã†ã•ã¾ã§ã—ãŸ"
                    )
                if len(answer2) < 5:
                    pass
                elif any(phrase in answer2 for phrase in phrases):
                    pass
                else:
                    #with text_output.chat_message('user'):
                        #st.write(answer2)
                    print("[Whis_seg]",answer2) 
                    return answer2 
                audio_buffer = []  # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
                silence_detected = False
            except Exception as e:
                st.error(f"éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        #amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={st.session_state.amp}")
        #amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={amp}")
        # å‡¦ç†è² è·ã‚’æŠ‘ãˆã‚‹ãŸã‚ã«çŸ­ã„é…å»¶ã‚’æŒ¿å…¥
        time.sleep(0.1)

def app_sst_with_video():
    memory = MemorySaver()
    frames_deque_lock = threading.Lock()
    frames_deque: deque = deque([])
    async def queued_audio_frames_callback(
        frames: List[av.AudioFrame],
    ) -> av.AudioFrame:
        with frames_deque_lock:
            frames_deque.extend(frames)
        # Return empty frames to be silent.
        new_frames = []
        for frame in frames:
            input_array = frame.to_ndarray()
            new_frame = av.AudioFrame.from_ndarray(
                np.zeros(input_array.shape, dtype=input_array.dtype),
                layout=frame.layout.name,
            )
            new_frame.sample_rate = frame.sample_rate
            new_frames.append(new_frame)
        return new_frames
    
    memory_use = st.sidebar.empty()
    memory_alt = st.sidebar.empty()
    memory_ok = st.sidebar.empty()
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦– (ãƒ«ãƒ¼ãƒ—ã®å¤–ã«ç§»å‹•ã—ãŸãŒã€ã‚«ã‚¦ãƒ³ã‚¿iã®æ‰±ã„ã¯è¦æ¤œè¨)
        # ã“ã®iã¯éŸ³å£°å…¥åŠ›ãƒ«ãƒ¼ãƒ—ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ã ã£ãŸãŸã‚ã€ã“ã“ã§ã¯å›ºå®šå€¤ã¾ãŸã¯åˆ¥ã®æ–¹æ³•ã§ç®¡ç†
    current_memory_use(0, memory_use, memory_alt, memory_ok) # iã‚’0ã«å›ºå®šã€ã¾ãŸã¯åˆ¥ã®ã‚«ã‚¦ãƒ³ã‚¿ã‚’ä½¿ç”¨
    amp_indicator = st.sidebar.empty() #éŸ³å£°æŒ¯å¹…è¡¨ç¤ºç”¨
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çŠ¶æ…‹ã‚’ç®¡ç†ã™ã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–
    if "streaming" not in st.session_state:
        st.session_state["streaming"] = True  # åˆæœŸçŠ¶æ…‹ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å†ç”Ÿä¸­
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¡¨ç¤º
    with st.sidebar:
        st.header("Webcam Stream")
        webrtc_ctx = webrtc_streamer(
            key="speech-to-text-w-video",
            desired_playing_state=st.session_state["streaming"], 
            mode=WebRtcMode.SENDRECV, #.SENDONLY,  #
            #audio_receiver_size=2048,  #1024ã€€#512 #ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4
            #å°ã•ã„ã¨Queue overflow. Consider to set receiver size bigger. Current size is 1024.
            queued_audio_frames_callback=queued_audio_frames_callback,
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            media_stream_constraints={"video": True, "audio": True},
            video_processor_factory=VideoTransformer,  
        )
    if not webrtc_ctx.state.playing:
        return
    #status_indicator.write("Loading...")
    cap_title = st.sidebar.empty()
    cap_image = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
    # --- â–²â–²â–² Whisper ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºé¸æŠã‚’è¿½åŠ  â–²â–²â–² ---
    uploaded_file = st.sidebar.file_uploader(
        "ã“ã“ã«ç”»åƒã‚’ã‚¢ãƒƒãƒ—ã—ã¦å•åˆã›å¯ (æ‹¡å¼µå­ã¯å°æ–‡å­—ã§)",
        type=["jpg", "jpeg", "png"],
        key="file_uploader"
    )
    if uploaded_file is not None:
        base64_image, image_type = process_uploaded_image(uploaded_file)
        st.session_state.img_url = f"data:{image_type};base64,{base64_image}"
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒã‚’è¡¨ç¤º (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
        if cap_title and cap_image: # cap_title, cap_image ã®å­˜åœ¨ç¢ºèª
            cap_title.header("Uploaded Image")
            cap_image.image(uploaded_file)

    #amp_indicator = st.sidebar.empty() #éŸ³å£°æŒ¯å¹…è¡¨ç¤ºç”¨
    #status_indicator = st.empty() # çŠ¶æ…‹è¡¨ç¤ºç”¨

    st.sidebar.title("Options")
    col1, col2 = st.sidebar.columns(2)
    # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
    with col1:
        # å…¥åŠ›æ–¹æ³•ã®é¸æŠ
        input_method = st.sidebar.radio("å…¥åŠ›æ–¹æ³•", ("éŸ³å£°ã¨ãƒ†ã‚­ã‚¹ãƒˆ","ãƒ†ã‚­ã‚¹ãƒˆã®ã¿" ))
        st.session_state.input_method = input_method
    with col2:
        # å‡ºåŠ›æ–¹æ³•ã®é¸æŠ
        output_method = st.sidebar.radio("å‡ºåŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.output_method = output_method

    # --- è¡¨ç¤ºãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã¨ã‚³ãƒ³ãƒ†ãƒŠå®šç¾© ---
    history_container = st.container(height=400)
    st.write("ğŸ‘‡ï¼šå›ç­”ä¸­ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¿”ç­”ï¼‰ã€ğŸ‘†ï¼šå›ç­”æ¸ˆï¼ˆãƒãƒ£ãƒƒãƒˆå±¥æ­´ï¼‰")
    answer_container = st.container(height=300)
    #status_indicator = st.empty() # "ä½•ã‹è©±ã—ã¦ã­"ã®è¡¨ç¤ºã€çŠ¶æ…‹è¡¨ç¤ºç”¨
    button_container = st.container()
    status_indicator = st.empty() # "ä½•ã‹è©±ã—ã¦ã­"ã®è¡¨ç¤ºã€çŠ¶æ…‹è¡¨ç¤ºç”¨
    #answer_container = st.container(height=350)
    #with answer_container:
        #status_indicator = st.empty() # "ä½•ã‹è©±ã—ã¦ã­"ã®è¡¨ç¤ºã€çŠ¶æ…‹è¡¨ç¤ºç”¨
    st.session_state.answer_container = answer_container
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤ºï¼ˆå±¥æ­´ã‚³ãƒ³ãƒ†ãƒŠå†…ã«è¡¨ç¤ºï¼‰
    with history_container:
        for role, message in st.session_state.get("message_history", []):
            #st.chat_message(role).markdown(message)
            with st.chat_message(role):
                st.markdown(message)

    ###################################################################
    # --- ãƒœã‚¿ãƒ³å®šç¾©ã¨ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚¨ãƒªã‚¢ã‚’ãƒ«ãƒ¼ãƒ—ã®å¤–ã«é…ç½® ---
    button_input_query = None
    #ãƒœã‚¿ãƒ³ã‚³ãƒ³ãƒ†ãƒŠé ˜åŸŸã«è¡¨ç¤º
    with button_container:
        st.write("ã‚¯ã‚¤ãƒƒã‚¯å•åˆã›ãƒœã‚¿ãƒ³:")
        cols = st.columns(7)
        button_definitions = [
            ("ç”»åƒèª¬æ˜", "ç”»åƒã®å†…å®¹ã‚’è©³ã—ãèª¬æ˜ã—ã¦"),
            ("å‰ç”»åƒã¨å·®", "å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"),
            ("ç”»åƒæ–‡ã®ç¿»è¨³", "ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"),
            ("ä½•æ—¥", "ä»Šæ—¥ã¯ä½•æ—¥ã§ã™ã‹ï¼Ÿ"),
            ("ã¯ã˜ã‚ã®è³ªå•", "ã¯ã˜ã‚ã®è³ªå•ã‚’æ†¶ãˆã¦ã„ã¾ã™ã‹?"),
            ("å¤©æ°—", "å°æ¾å¸‚ã®æ˜æ—¥ã¨æ˜å¾Œæ—¥ã®å¤©æ°—ã¯ã€‚èŠ±ç²‰æƒ…å ±ã‚‚æ•™ãˆã¦ã€‚"),
            ("ãƒ‹ãƒ¥ãƒ¼ã‚¹", "ä»Šæ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯?"),
            ("AIæƒ…å ±", "ä»Šæ—¥ã®AIã«é–¢ã™ã‚‹æƒ…å ±ã¯ï¼Ÿ"),
            ("äººæ°—æ¼«ç”»", "ç¾åœ¨ã€äººæ°—ã®æ¼«ç”»ã‚’5ä»¶æ•™ãˆã¦ã€‚"),
            ("ä¸Šæ˜ æ˜ ç”»", "ç¾åœ¨ã€ä¸Šæ˜ ä¸­ã¾ãŸã¯ä¸Šæ˜ äºˆå®šã®æ˜ ç”»ã‚’5ä»¶æ•™ãˆã¦ã€‚"),
            ("å°æ¾ã®æ–™ç†åº—", "å°æ¾å¸‚ã®æœ€è¿‘è©±é¡Œã®æ–™ç†åº—ã¯ï¼Ÿ"),
            ("ç™¾åå±±","æ—¥æœ¬ã®ç™¾åå±±ã‚’æ¨™é«˜ã®é«˜ã„é †ä½ã‹ã‚‰10å±±æ•™ãˆã¦"),
            ("äººç”Ÿã®æ„ç¾©", "äººç”Ÿã®æ„ç¾©ã¯ï¼Ÿ"),
            ("ã‚ªã‚»ãƒ­ä½œæˆ", "Webç”»é¢ã§ãƒ—ãƒ¬ã‚¤ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼å¯¾AIã®ã‚ªã‚»ãƒ­ã‚²ãƒ¼ãƒ ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦"),
        ]
        for j, (label, query) in enumerate(button_definitions):
            if cols[j % 7].button(label, key=f"button_{j}_main"): # ã‚­ãƒ¼ã‚’ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«ã™ã‚‹
                button_input_query = query

    # chat_input ã‚’ä¸€åº¦ã ã‘å‘¼ã³å‡ºã™
    #st.chat_inputã¯ã€streamlitã®ä»•æ§˜ã§ä¸€ç•ªä¸‹ã«å›ºå®šã•ã‚Œã¦ã„ã‚‹ã€‚
    user_typed_input = st.chat_input("ğŸ¤–ãƒãƒ£ãƒƒãƒˆãƒ†ã‚­ã‚¹ãƒˆã¯ã“ã“ã«å…¥åŠ›ã—ã¦ãã ã•ã„...", key="main_chat_input_widget")
    final_user_input_to_process = None
    if button_input_query:
        final_user_input_to_process = button_input_query
    elif user_typed_input:
        final_user_input_to_process = user_typed_input
    ###################################################################
    #éŸ³å£°ã¨ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ãŸå…¥åŠ›ï¼‰ã®å¯¾è©±ãƒ«ãƒ¼ãƒ—
    if st.session_state.input_method == "éŸ³å£°ã¨ãƒ†ã‚­ã‚¹ãƒˆ":
        gc.collect() # é¸æŠå¾Œã«ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        #status_indicator = st.sidebar.empty()
        #amp_indicator = st.sidebar.empty()
        st.session_state.amp_threshold = st.sidebar.slider(
            "ç„¡éŸ³æŒ¯å¹…é–¾å€¤ (å°ã•ã„ã»ã©æ•æ„Ÿ):",
            min_value=300, max_value=3000, value=1000, step=100
            )
        st.session_state.silence_threshold = st.sidebar.slider(
            "ç„¡éŸ³æœ€å°æ™‚é–“ï¼ˆç§’ï¼‰",
            min_value=0.1, max_value=3.0, value=0.5, step=0.1
            )
        #if not "whisper_model" in st.session_state:
            #st.session_state.whisper_model = whisper.load_model("small") #,device = "cuda")
        # Whisperãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨é¸æŠ
        whisper_model_size_key = "whisper_model_size_select"
        if whisper_model_size_key not in st.session_state:
            st.session_state[whisper_model_size_key] = "tiny" # åˆæœŸå€¤

        selected_whisper_model_size = st.sidebar.selectbox(
            "Whisperãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º (å°ã•ã„ã»ã©è»½é‡):",
            ("tiny", "base", "small", "medium"), # "large" ã¯ãƒ¡ãƒ¢ãƒªçš„ã«å³ã—ã„å¯èƒ½æ€§
            index=("tiny", "base", "small", "medium").index(st.session_state[whisper_model_size_key]),
            key=whisper_model_size_key
        )
        if "whisper_model" not in st.session_state or st.session_state.get("current_whisper_size") != selected_whisper_model_size:
            with st.spinner(f"Whisperãƒ¢ãƒ‡ãƒ« ({selected_whisper_model_size}) ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                if "whisper_model" in st.session_state and st.session_state.whisper_model is not None:
                    del st.session_state.whisper_model # å¤ã„ãƒ¢ãƒ‡ãƒ«ã‚’è§£æ”¾è©¦è¡Œ
                    gc.collect()
                st.session_state.whisper_model = whisper.load_model(selected_whisper_model_size) #,device = "cuda")
                st.session_state.current_whisper_size = selected_whisper_model_size
                gc.collect()
        #base:74M,small:244M,medium:769M,large:1550M

        i = 0
        #current_memory_use(i,memory_use,memory_alt,memory_ok)
        frames_deque_lock = threading.Lock()
        # frames_deque_lockã‚’ä½¿ç”¨ã—ã¦ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã—ã¦ã„ã¾ã™ãŒã€
        # dequeã®ã‚¯ãƒªã‚¢æ“ä½œãªã©ã§ãƒªã‚½ãƒ¼ã‚¹ç«¶åˆãŒèµ·ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
        # dequeã®æœ€å¤§é•·ã‚’è¨­å®šï¼ˆä¾‹: deque([], maxlen=100)) ã—ã€ãƒãƒƒãƒ•ã‚¡æº¢ã‚Œã‚’é˜²æ­¢ã™ã‚‹æ–¹ãŒå®‰å…¨ã§ã™ã€‚
        frames_deque: deque = deque([], maxlen=100) #NG 1
        #print(text_input)
        # éŸ³å£°å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã§ã€ã‹ã¤ãƒ†ã‚­ã‚¹ãƒˆã‚„ãƒœã‚¿ãƒ³ã‹ã‚‰ã®å…¥åŠ›ãŒã¾ã ãªã„å ´åˆã®ã¿éŸ³å£°èªè­˜ã‚’å®Ÿè¡Œ
        if final_user_input_to_process is None:
            status_indicator.write("ğŸ¤–ä½•ã‹è©±ã—ã¦!ğŸ¦œ...ä¸Šã®ã‚¯ã‚¤ãƒƒã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã‹ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãƒãƒ£ãƒƒãƒˆã—ã¦ã‚‚ã„ã„ã§ã™ã€‚")
            # éŸ³å£°å‡¦ç†ã®éåŒæœŸã‚¿ã‚¹ã‚¯ã‚’èµ·å‹•
            recognized_audio_input = asyncio.run(process_audio_loop_with_silence_detection(
                frames_deque_lock,
                frames_deque,
                amp_indicator,
            ))
            if recognized_audio_input:
                final_user_input_to_process = recognized_audio_input
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦– (ãƒ«ãƒ¼ãƒ—ã®å¤–ã«ç§»å‹•ã—ãŸãŒã€ã‚«ã‚¦ãƒ³ã‚¿iã®æ‰±ã„ã¯è¦æ¤œè¨)
        # ã“ã®iã¯éŸ³å£°å…¥åŠ›ãƒ«ãƒ¼ãƒ—ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ã ã£ãŸãŸã‚ã€ã“ã“ã§ã¯å›ºå®šå€¤ã¾ãŸã¯åˆ¥ã®æ–¹æ³•ã§ç®¡ç†
        current_memory_use(0, memory_use, memory_alt, memory_ok) # iã‚’0ã«å›ºå®šã€ã¾ãŸã¯åˆ¥ã®ã‚«ã‚¦ãƒ³ã‚¿ã‚’ä½¿ç”¨
    ###################################################################
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®ã¿ã®å ´åˆ
    elif st.session_state.input_method == "ãƒ†ã‚­ã‚¹ãƒˆã®ã¿":
        gc.collect() # é¸æŠå¾Œã«ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        st.session_state["streaming"] = True  # Webã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å†ç”Ÿ
        # final_user_input_to_process ã¯æ—¢ã«ãƒœã‚¿ãƒ³ã¾ãŸã¯ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã§è¨­å®šã•ã‚Œã¦ã„ã‚‹

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
        i = 0 # ã“ã®ã‚«ã‚¦ãƒ³ã‚¿ã‚‚ã€ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒ«ãƒ¼ãƒ—ãŒãªã„ãŸã‚ã€æ„å‘³åˆã„ãŒå¤‰ã‚ã‚‹
        current_memory_use(i,memory_use,memory_alt,memory_ok)
        mem_use = get_memory_usage()
        # i += 1 # ãƒ«ãƒ¼ãƒ—ãŒãªã„ãŸã‚ä¸è¦
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡(ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰):{mem_use:.0f}MB")

    # --- QAå‡¦ç† ---
    if final_user_input_to_process:
        qa(final_user_input_to_process,webrtc_ctx,cap_title,cap_image,memory)
        # å‡¦ç†å¾Œã«å†å®Ÿè¡Œã—ã¦å…¥åŠ›ã‚’ã‚¯ãƒªã‚¢ã—ã€UIã‚’æ›´æ–°
        st.rerun()
        # st.rerun() ã‚’å‰Šé™¤ï¼ˆã¾ãŸã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã¨AIã®å¿œç­”ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºã‚’å«ã‚€ï¼‰ãŒå®Œäº†ã—ãŸå¾Œã‚‚ã€
        # ãã‚Œã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å³åº§ã«å±¥æ­´ã‚³ãƒ³ãƒ†ãƒŠã«ç§»å‹•ã›ãšã€ç”»é¢ã®ç¾åœ¨ã®ä½ç½®ã«æ®‹ã‚Šç¶šã‘ã¾ã™ã€‚
        # ãã—ã¦ã€æ¬¡ã®æ–°ã—ã„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã™ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã€æ–°ã—ã„éŸ³å£°å…¥åŠ›ãŒèªè­˜ã•ã‚Œã‚‹ãªã©ï¼‰
        # ãŒç™ºç”Ÿã—ã¦Streamlitã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå†å®Ÿè¡Œã•ã‚Œã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã€
        # ãã‚Œã¾ã§ã®ã‚„ã‚Šå–ã‚ŠãŒå±¥æ­´ã‚³ãƒ³ãƒ†ãƒŠã«æ ¼ç´ã•ã‚Œã€æ–°ã—ã„ã‚„ã‚Šå–ã‚ŠãŒé–‹å§‹ã•ã‚Œã¾ã™ã€‚
    else: # å…¥åŠ›ãŒãªã„å ´åˆã¯ã€ãƒ¡ãƒ¢ãƒªç›£è¦–ã®ã¿è¡Œã†ï¼ˆéŸ³å£°ãƒ¢ãƒ¼ãƒ‰ã§éŸ³å£°å…¥åŠ›å¾…ã¡ã®å ´åˆãªã©ï¼‰
        #if st.session_state.input_method == "éŸ³å£°ã¨ãƒ†ã‚­ã‚¹ãƒˆ":
        current_memory_use(0, memory_use, memory_alt, memory_ok) # iã‚’0ã«å›ºå®š
        #st.rerun ä¸è¦

def init_page():
    st.set_page_config(
        page_title="Yas Chatbot",
        page_icon="ğŸ¤–"
    )
    #st.header("Yas Chatbot(ç”»åƒã€éŸ³å£°å¯¾å¿œ) ğŸ¤–")
    #st.write("""Webã‚«ãƒ¡ãƒ©ç”»åƒã«ã¤ã„ã¦ã®å•åˆã›ã€éŸ³å£°ã§ã®å…¥å‡ºåŠ›ãŒã§ãã¾ã™ã€‚\n
            #ãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚«ãƒ¡ãƒ©,ãƒã‚¤ã‚¯ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã—ã¦ä½¿ç”¨ã€‚""")
    st.sidebar.title("ğŸ¤– Yas Chatbot")
    st.sidebar.caption("ã‚«ãƒ¡ãƒ©ç”»åƒã€ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã€Webæœ€æ–°æƒ…å ±ã€éŸ³å£°ã§ã®å•åˆã›ãŒã§ãã¾ã™")

def init_messages():
    history_id = 123
    clear_button = st.sidebar.button("ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢", key="clear")
    
    #if clear_button or "message_history" not in st.session_state:
        #st.session_state.message_history = [
            #("system", "You are a helpful assistant.")
        #]   
    #å•é¡Œã¯ã€åˆå›ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã§ã™ã€‚
    # ã“ã®ã¨ãã€clear_button ã¯ã¾ã æŠ¼ã•ã‚Œã¦ã„ãªã„ã®ã§ False ã§ã™ã€‚
    # ãã—ã¦ã€message_history ã‚‚ã¾ã å­˜åœ¨ã—ãªã„ã®ã§ 
    # "message_history" not in st.session_state ã¯ True ã¨ãªã‚Šã¾ã™ã€‚
    # False or True ã¯ True ãªã®ã§ã€åˆæœŸåŒ–å‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
    # ä¸€è¦‹ã€å•é¡Œãªã„ã‚ˆã†ã«è¦‹ãˆã¾ã™ã€‚
    # clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    # clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã«åˆæœŸåŒ–
    # ç‹¬ç«‹ã—ãŸifã§è¨˜è¿°ã™ã‚‹ã“ã¨ã§ã€ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã€ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã€ã™ã¹ã¦ã®çŠ¶æ³ã«å¯¾å¿œã§ãã¾ã—ãŸã€‚
    # ç¾åœ¨ã®æ—¥ä»˜ã¨æ™‚åˆ»ã‚’å–å¾—
    current_datetime = get_current_datetime()
    # define the system message (primer) of your agent
    SYSTEM_MESSAGE = f"""ã‚ãªãŸã¯ãƒ„ãƒ¼ãƒ«ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
            ç¾åœ¨ã®æ—¥æ™‚ã¯ {current_datetime} ã§ã™ã€‚
            æä¾›ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¢ã—ã¦ã„ã‚‹æœ€æ–°ã®æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
            å¿…è¦ãªæƒ…å ±ãŒå¾—ã‚‰ã‚Œãªã„å ´åˆã¯ã€ä»–ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
            ç‰¹ã«ã€å¤©æ°—ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’èã‹ã‚ŒãŸå ´åˆã¯ã€å¿…ãšãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
            ã¾ãŸã€å¸¸ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
            #"""You are the world's most advanced search engine.
            #Please provide the user with the new information they are looking for by using the tools provided."""
            #You are a helpful assistant.
            #You are a competent assistant that effectively utilizes tools.ã€€åŸæ–‡
            #è²´æ–¹ã¯ãƒ„ãƒ¼ãƒ«ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚åŸæ–‡ã®è¨³
            #ã‚ãªãŸã¯ä¸–ç•Œã§æœ€ã‚‚é«˜åº¦ãªæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚
    if clear_button:
        st.session_state.message_history = [("system",SYSTEM_MESSAGE)]
        history_id += 1
        st.session_state.history_id = f"abc{history_id}"
    #clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    if "message_history" not in st.session_state:
        st.session_state.message_history = [("system", SYSTEM_MESSAGE)]

def main():
    #ç’°å¢ƒå¤‰æ•°
    setup_environment()
    #st.header("Real Time Speech-to-Text with_video")
    #ç”»é¢è¡¨ç¤º
    init_page()
    #ä¼šè©±å±¥æ­´åˆæœŸåŒ–ã¨ã‚¯ãƒªã‚¢
    init_messages()
    #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
    #st.session_state.llm = select_model()
    # st.session_state.llm = None
    # st.session_state.selected_model_name = ""
    # st.session_state.model_name = ""
    # st.session_state.input_max = ""
    # st.session_state.input_method = ""
    # st.session_state.user_input = ""
    # st.session_state.result = ""
    # st.session_state.frame = ""
    if 'llm' not in st.session_state: st.session_state.llm = None
    if 'selected_model_name' not in st.session_state: st.session_state.selected_model_name = ""
    if 'model_name' not in st.session_state: st.session_state.model_name = ""
    if 'input_max' not in st.session_state: st.session_state.input_max = 0
    if 'input_method' not in st.session_state: st.session_state.input_method = "éŸ³å£°ã¨ãƒ†ã‚­ã‚¹ãƒˆ" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    if 'user_input' not in st.session_state: st.session_state.user_input = ""
    if 'result' not in st.session_state: st.session_state.result = ""
    #if 'frame' not in st.session_state: st.session_state.frame = "" # frameã¯VideoTransformerå†…ã§ç®¡ç†
    st.session_state.img_url = ""
    st.session_state.history_id = "abc123"
    #
    model_names = list(model_definitions.keys())
    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒ¢ãƒ‡ãƒ«é¸æŠ ---
    selected_model_name = st.sidebar.selectbox(
        "è¨€èªãƒ¢ãƒ‡ãƒ«é¸æŠ(å„ªåŠ£æœ‰ã‚Š):",
        model_names,
        #index=model_names.index(st.session_state.selected_model_name) if st.session_state.selected_model_name in model_names else 0,
        index=model_names.index(st.session_state.selected_model_name)
            #if hasattr(st.session_state, "selected_model_name") and st.session_state.selected_model_name in model_names
            if st.session_state.selected_model_name in model_names
            else 0,
        key="model_select"
    )
    # --- ãƒ¢ãƒ‡ãƒ«/ã‚°ãƒ©ãƒ•/ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å†åˆæœŸåŒ– ---
    if st.session_state.selected_model_name is None or selected_model_name != st.session_state.selected_model_name:
        print("#"*100)
        #print("st.session_state.selected_model_name=",st.session_state.selected_model_name)
        #print("selected_model_name=",selected_model_name)
        print(f"ãƒ¢ãƒ‡ãƒ«å¤‰æ›´ã¾ãŸã¯åˆå›ãƒ­ãƒ¼ãƒ‰: æ—§={st.session_state.selected_model_name}, æ–°={selected_model_name}")

        # å¤ã„ãƒ¢ãƒ‡ãƒ«é–¢é€£ãƒªã‚½ãƒ¼ã‚¹ã®è§£æ”¾
        if hasattr(st.session_state, 'llm') and st.session_state.llm is not None:
            del st.session_state.llm
            st.session_state.llm = None
        # if hasattr(st.session_state, 'graph_or_agent') and st.session_state.graph_or_agent is not None:
        #     del st.session_state.graph_or_agent
        #     st.session_state.graph_or_agent = None
        gc.collect()

        st.session_state.selected_model_name = selected_model_name
        st.session_state.model_name = selected_model_name
        #st.session_state.messages = []# LangGraphã®å…¥åŠ›ç”¨messageãƒªã‚¹ãƒˆã€‚é€šå¸¸ã¯ä¼šè©±å±¥æ­´ã‹ã‚‰æ§‹ç¯‰ã€‚
                                        # ã“ã“ã§ã‚¯ãƒªã‚¢ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«å¤‰æ›´æ™‚ã«ä¼šè©±ãŒãƒªã‚»ãƒƒãƒˆã•ã‚Œã‚‹æŒ™å‹•ã«ãªã‚‹ã€‚
                                        # message_history ã¯ init_messages ã§ç®¡ç†ã€‚
        #st.session_state.graph_or_agent = None
        # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã€æ–°ã—ã„ä¼šè©±ã‚¹ãƒ¬ãƒƒãƒ‰IDã§MemorySaverã‚’æº–å‚™
        st.session_state.memory = MemorySaver()
        st.session_state.history_id = f"streamlit_thread_{selected_model_name}_{str(uuid.uuid4())}"

        # ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾— (ãƒ„ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒ‰å‰)
        #llm_instance_base, cost_input, cost_cached_input, cost_output, input_max = models_dict[selected_model_name]
        # --- â–¼â–¼â–¼ é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ â–¼â–¼â–¼ ---
        with st.spinner(f"ãƒ¢ãƒ‡ãƒ« '{selected_model_name}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­..."): # â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€ â˜…â˜…â˜…
            llm_instance_base, cost_input, cost_cached_input, cost_output, input_max = load_selected_model(selected_model_name)
            st.session_state.llm = llm_instance_base #select_model()
            st.session_state.input_max =  input_max
            gc.collect() # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¾Œã«ã‚‚å¿µã®ãŸã‚GC
        if llm_instance_base is None:
            st.error(f"ãƒ¢ãƒ‡ãƒ« '{selected_model_name}' ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            st.stop() # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰ã‚¢ãƒ—ãƒªã‚’åœæ­¢
        #else:
        #     # --- â–¼â–¼â–¼ ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆå¾Œã«ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ â–¼â–¼â–¼ ---
            #gc.collect() # ä¸è¦ã«ãªã£ãŸå¤ã„ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾ã‚’æœŸå¾…  NG
            #st.rerun()  NG

    app_sst_with_video()

###################################################################      
if __name__ == "__main__":
    main()
