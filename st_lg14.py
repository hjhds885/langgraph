# chatbot_langgraph28-1.py ã‚’ Streamlit ã‚¢ãƒ—ãƒªåŒ–
# LangGraph/ReActã® stream ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã†é™ã‚Šã€æ–‡ç¯€ã‚„å¥èª­ç‚¹ã”ã¨ã® æ»‘ã‚‰ã‹ãª ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºã¯å›°é›£ã§ã™ã€‚
# ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ã¯ã€æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã‚„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ›´æ–°ã”ã¨ã«è¡¨ç¤ºã‚’æ›´æ–°ã™ã‚‹ã€ç¾å®Ÿçš„ãªå®Ÿè£…ã¨ãªã£ã¦ã„ã¾ã™ã€‚
# ã‚ˆã‚Šç´°ã‹ã„ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒå¿…è¦ãªå ´åˆã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ/ã‚°ãƒ©ãƒ•ã®æ©Ÿèƒ½ï¼ˆãƒ„ãƒ¼ãƒ«é€£æºã€è¤‡é›‘ãªçŠ¶æ…‹ç®¡ç†ãªã©ï¼‰ã‚’è«¦ã‚ã¦ã€
# LLMã® stream() ã‚’ç›´æ¥ä½¿ã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
# ã”æç¤ºã®ä¿®æ­£ã‚³ãƒ¼ãƒ‰ (st_lg13-1.py) ã«å«ã¾ã‚Œã‚‹ if new_content != full_response: ã®ãƒã‚§ãƒƒã‚¯ã¯ã€
# ä¸è¦ãªUIæ›´æ–°ã‚’æ¸›ã‚‰ã™ãŸã‚ã«æœ‰åŠ¹ã§ã™ã€‚
# ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ã¯ã€LangGraph/ReActã®ä»•çµ„ã¿ã®ä¸­ã§ã®ãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆãªã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºã¨è¨€ãˆã¾ã™ã€‚
# æœŸå¾…ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ãªç´°ã‹ã„ç²’åº¦ã§ã®è¡¨ç¤ºã«ã¯ãªã£ã¦ã„ãªã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€
# ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®åˆ¶ç´„ä¸Šã€ã‚ã‚‹ç¨‹åº¦ã¯ä»•æ–¹ãŒãªã„éƒ¨åˆ†ã¨ãªã‚Šã¾ã™ã€‚

import streamlit as st
from streamlit_webrtc import (
    WebRtcMode,
    webrtc_streamer,
    VideoHTMLAttributes,
    VideoTransformerBase,
    ClientSettings
)
from typing import Annotated, Sequence, Optional, Literal # Literal ã‚’è¿½åŠ 
from typing_extensions import TypedDict
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.graph.message import add_messages
from langgraph.prebuilt import create_react_agent
from langgraph.prebuilt import ToolNode, tools_condition

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    ToolMessage,
    BaseMessage,
    SystemMessage,
    trim_messages,
)
# from langchain.chains import RetrievalQA # æœªä½¿ç”¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
# from langchain_community.vectorstores import FAISS # æœªä½¿ç”¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
# from langchain_openai import OpenAIEmbeddings # æœªä½¿ç”¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
# from langchain.schema import Document # æœªä½¿ç”¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
from serpapi import GoogleSearch
from exa_py import Exa
import base64
from io import BytesIO # ç”»åƒå‡¦ç†ç”¨
import cv2
import av
import time
# import json # æœªä½¿ç”¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
import tiktoken
from datetime import datetime, timedelta
import uuid
# ä»–ã®pythonã‚³ãƒ¼ãƒ‰ã‚’å‘¼ã³è¾¼ã‚€
import my_llms, my_querys, my_environments, my_tools
import os
import requests
import traceback # ã‚¨ãƒ©ãƒ¼è©³ç´°è¡¨ç¤ºã®ãŸã‚
from collections import deque
import numpy as np
from typing import Any, Dict, List,Tuple # å‹ãƒ’ãƒ³ãƒˆ
import threading
import psutil
# --- éŸ³å£°å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’è¿½åŠ  ---
from gtts import gTTS
import whisper
import pydub
from pydub import AudioSegment
from pydub.effects import low_pass_filter
import librosa
import asyncio # è¿½åŠ 
import re # è¿½åŠ 
import torch
import torchaudio
import torchvision

# é–¢æ•°ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
def get_memory_usage():
    process = psutil.Process()
    mem_info = process.memory_info()
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’MBå˜ä½ã§è¿”ã™
    return mem_info.rss / (1024 * 1024)

def current_memory_use(memory_use,memory_alt,memory_ok):
    # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
    current_memory_usage = get_memory_usage()
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º

    #memory_use.metric("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)", f"{current_memory_usage:.2f}")
    memory_use.write(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:{current_memory_usage:.0f}MB")
    #print("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)", f"{current_memory_usage:.2f}")
    # ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã‚’å®šç¾©
    MEMORY_LIMIT_MB = 2700  # 1GB
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ã‚’è¶…ãˆãŸå ´åˆã®è­¦å‘Š
    if current_memory_usage > MEMORY_LIMIT_MB:
        memory_alt.error(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ ({MEMORY_LIMIT_MB} MB) ã‚’è¶…ãˆã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ãã ã•ã„ã€‚")
        memory_ok.empty()
        #st.stop()
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ ({MEMORY_LIMIT_MB} MB) ã‚’è¶…ãˆã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ãã ã•ã„ã€‚")
    else:
        memory_ok.success("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚")
        memory_alt.empty()
        #print("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚")

# --- ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆé–¢æ•° (å¤‰æ›´ãªã—) ---
def get_token_count_for_messages(messages: Sequence[BaseMessage], model_name: str = "cl100k_base") -> int:
    """tiktoken ã‚’ä½¿ã£ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆè¿‘ä¼¼å€¤ï¼‰"""
    try:
        encoding = tiktoken.encoding_for_model(model_name)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")

    num_tokens = 0
    for message in messages:
        content_str = ""
        if isinstance(message.content, str):
            content_str = message.content
        elif isinstance(message.content, list): # ç”»åƒã‚’å«ã‚€å ´åˆãªã©
            for part in message.content:
                if isinstance(part, dict) and part.get("type") == "text":
                    content_str += part.get("text", "")
        else:
            content_str = str(message.content)
        num_tokens += len(encoding.encode(content_str))
        num_tokens += 4
    num_tokens += 3
    return num_tokens

# --- æ—¥ä»˜å–å¾—é–¢æ•° (å¤‰æ›´ãªã—) ---
def get_current_datetime():
    now = datetime.now()
    return now.strftime("%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†")

# --- ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (å¤‰æ›´ãªã—) ---
current_datetime_str = get_current_datetime() # ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«å–å¾—
SYSTEM_MESSAGE = f"""ã‚ãªãŸã¯ãƒ„ãƒ¼ãƒ«ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ç¾åœ¨ã®æ—¥ä»˜ã¯{current_datetime_str}ã§ã™ã€‚æ—¥ä»˜ã«é–¢ã™ã‚‹æƒ…å ±ã¯ã€ã“ã®æ—¥ä»˜ã®ã¿ã‚’å…ƒã«ã—ã¦ãã ã•ã„ã€‚
        æœ€æ–°ã®æƒ…å ±ãŒå¿…è¦ãªå ´åˆã‚„ã‚ãªãŸãŒç­”ãˆã‚‰ã‚Œãªã„å ´åˆã€æä¾›ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
        å¿…è¦ãªæƒ…å ±ãŒå¾—ã‚‰ã‚Œãªã„å ´åˆã¯ã€ä»–ã®ãƒ„ãƒ¼ãƒ«ã‚‚åˆ©ç”¨ã—ã¦ãã ã•ã„ã€‚
        Webã‚µã‚¤ãƒˆã‚’ç´¹ä»‹ã™ã‚‹ã ã‘ã®å›ç­”ã‚’ã—ãªã„ã§ã€ã‚µã‚¤ãƒˆã‹ã‚‰å…·ä½“çš„ãªæƒ…å ±ã‚’å¾—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
        ã¾ãŸã€å¸¸ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_MESSAGE), # language ã¯ chatbot_node å†…ã§å‡¦ç†
        MessagesPlaceholder(variable_name="messages"),
    ]
)

# --- ãƒ„ãƒ¼ãƒ«å®šç¾© (get_weather, search_q) (å¤‰æ›´ãªã—) ---
# @tool ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã‚’ä½¿ã†å ´åˆ
@tool
def get_weather(location: str) -> str:
    """Get the current and future weather in a given location."""
    st.info(f"å¤©æ°—æƒ…å ±ã‚’å–å¾—ä¸­: {location}") # Streamlit UI ã«è¡¨ç¤º
    api_key = os.environ.get("OPENWEATHERMAP_API_KEY")
    if not api_key:
        return "Error: OPENWEATHERMAP_API_KEY environment variable not set."
    location_parts = location.split(",")
    city_name = location_parts[0].strip()
    country_code = location_parts[1].strip() if len(location_parts) > 1 else ""
    base_url_forecast = "http://api.openweathermap.org/data/2.5/forecast"
    params = {
        "q": f"{city_name},{country_code}", "appid": api_key, "units": "metric", "lang": "ja"
    }
    result = ""
    try:
        response_forecast = requests.get(base_url_forecast, params=params)
        response_forecast.raise_for_status()
        weather_data_forecast = response_forecast.json()
        tomorrow_weather = None
        tomorrow = datetime.now() + timedelta(days=1)
        tomorrow_str = tomorrow.strftime("%Y-%m-%d")
        for forecast in weather_data_forecast.get("list", []):
            if forecast.get("dt_txt", "").startswith(tomorrow_str):
                tomorrow_weather = forecast
                break
        if tomorrow_weather:
            description = tomorrow_weather.get("weather", [{}])[0].get("description", "ä¸æ˜")
            temperature = tomorrow_weather.get("main", {}).get("temp", "ä¸æ˜")
            humidity = tomorrow_weather.get("main", {}).get("humidity", "ä¸æ˜")
            wind_speed = tomorrow_weather.get("wind", {}).get("speed", "ä¸æ˜")
            result += f"{location}ã®æ˜æ—¥ã®å¤©æ°—: {description}, æ°—æ¸©: {temperature}â„ƒ, æ¹¿åº¦: {humidity}%, é¢¨é€Ÿ: {wind_speed}m/s\n"
        else:
            result += f"{location}ã®æ˜æ—¥ã®å¤©æ°—æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"

        base_url_current = "http://api.openweathermap.org/data/2.5/weather"
        response_current = requests.get(base_url_current, params=params)
        response_current.raise_for_status()
        weather_data_current = response_current.json()
        description = weather_data_current.get("weather", [{}])[0].get("description", "ä¸æ˜")
        temperature = weather_data_current.get("main", {}).get("temp", "ä¸æ˜")
        humidity = weather_data_current.get("main", {}).get("humidity", "ä¸æ˜")
        wind_speed = weather_data_current.get("wind", {}).get("speed", "ä¸æ˜")
        result += f"{location}ã®ç¾åœ¨ã®å¤©æ°—: {description}, æ°—æ¸©: {temperature}â„ƒ, æ¹¿åº¦: {humidity}%, é¢¨é€Ÿ: {wind_speed}m/s"
        return result
    except requests.exceptions.RequestException as e:
        st.error(f"å¤©æ°—æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ (Request): {e}")
        return f"Error: Could not retrieve weather information for {location}. {e}"
    except (KeyError, IndexError, TypeError) as e:
        st.error(f"å¤©æ°—æƒ…å ±è§£æã‚¨ãƒ©ãƒ¼: {e}")
        return f"Error: Could not parse weather information for {location}. {e}"
    except Exception as e:
        st.error(f"äºˆæœŸã›ã¬å¤©æ°—æƒ…å ±ã‚¨ãƒ©ãƒ¼: {e}")
        return f"An unexpected error occurred: {e}"

@tool
def search_q(query: str) -> str:
    #"""Googleæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€æ¤œç´¢çµæœã®ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã®ãƒªã‚¹ãƒˆã‚’è¿”ã™é–¢æ•°"""
    """Webæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€é–¢é€£æ€§ã®é«˜ã„ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’è¿”ã™é–¢æ•° (Google Search ã¾ãŸã¯ Exa)"""
    google_search_failed = False
    search_result_text = ""
    # --- Google Search ã‚’è©¦è¡Œ ---
    st.info(f"Webæ¤œç´¢ã‚’å®Ÿè¡Œä¸­: {query}") # Streamlit UI ã«è¡¨ç¤º
    serpapi_key = os.environ.get("SERPAPI_API_KEY")
    if not serpapi_key:
        st.warning("SERPAPI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Exaæ¤œç´¢ã‚’è©¦ã¿ã¾ã™ã€‚")
        google_search_failed = True
    else:
        params = {
        "api_key": os.environ.get("SERPAPI_API_KEY"),
        "engine": "google", "q": query, "location": "Komatsu, Ishikawa, Japan",
        "google_domain": "google.com", "gl": "jp", "hl": "ja"
        }
        try:
            search = GoogleSearch(params)
            results = search.get_dict()
            # organic_results ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            organic_results = results.get('organic_results', [])

            #snippets = [item.get('snippet', '') for item in results.get('organic_results', []) if item.get('snippet')]
            snippets = [item.get('snippet', '') for item in organic_results if item.get('snippet')]
            if snippets:
                search_result_text = "\n".join(snippets)
            else:
                # ã‚¹ãƒ‹ãƒšãƒƒãƒˆãŒãªã„å ´åˆã€ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒªãƒ³ã‚¯ã‚’è¿”ã™
                titles_links = [f"{item.get('title', '')}: {item.get('link', '')}" for item in organic_results]
                if titles_links:
                    search_result_text = "\n".join(titles_links)
                else:
                    st.warning("Googleæ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                    google_search_failed = True # çµæœãªã—ã‚‚å¤±æ•—ã¨ã¿ãªã™
        except requests.exceptions.HTTPError as http_err: # requestsç”±æ¥ã®HTTPErrorã‚’æ•æ‰
            st.error(f"Googleæ¤œç´¢HTTPã‚¨ãƒ©ãƒ¼: {http_err}")
            if http_err.response.status_code == 403:
                st.warning("Googleæ¤œç´¢ã§403ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚Exaæ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                google_search_failed = True
            else:
                # 403ä»¥å¤–ã®HTTPã‚¨ãƒ©ãƒ¼ã‚‚Exaã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                st.warning(f"Googleæ¤œç´¢ã§HTTPã‚¨ãƒ©ãƒ¼({http_err.response.status_code})ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚Exaæ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                google_search_failed = True
        except Exception as e: # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ (serpapiãƒ©ã‚¤ãƒ–ãƒ©ãƒªå†…ã®ã‚¨ãƒ©ãƒ¼ãªã©)
            st.error(f"Googleæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            google_search_failed = True

        # --- Google Search ãŒå¤±æ•—ã—ãŸå ´åˆã€Exa Search ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ---
        if google_search_failed:
            st.info("Exaæ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™...")
            exa_results_response = exa_search(query) # exa_searché–¢æ•°ã‚’å‘¼ã³å‡ºã™

            # Exa ã®çµæœ (SearchResponse ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ) ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æŠ½å‡º
            exa_contents = []
            # exa_results_response.results ãŒãƒªã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
            if hasattr(exa_results_response, 'results') and isinstance(exa_results_response.results, list):
                for result in exa_results_response.results:
                    # result ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¿…è¦ãªå±æ€§ãŒã‚ã‚‹ã‹ç¢ºèª
                    content_part = ""
                    if hasattr(result, 'title') and result.title:
                        content_part += f"Title: {result.title}\n"
                    if hasattr(result, 'url') and result.url:
                        content_part += f"URL: {result.url}\n"
                    if hasattr(result, 'text') and result.text:
                        # æœ¬æ–‡ã¯é•·ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§500æ–‡å­—ã«åˆ‡ã‚Šè©°ã‚ã‚‹
                        content_part += f"Content: {result.text[:500]}..."
                    if content_part: # ä½•ã‹ã—ã‚‰ã®æƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ 
                        exa_contents.append(content_part.strip())

            if exa_contents:
                search_result_text = "\n\n".join(exa_contents)
            else:
                st.warning("Exaæ¤œç´¢ã§ã‚‚çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                search_result_text = "Webæ¤œç´¢ã§é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚" # æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

        return search_result_text

# --- ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ ---
# Streamlitç”¨ã« human_assistance ã‚’é™¤å¤–
tools_std,openai_tools,serp_tool = my_tools.setup_tools() # å…ƒã®ãƒ„ãƒ¼ãƒ«è¨­å®šé–¢æ•°ã‚’å‘¼ã³å‡ºã™
tools_list = [t for t in tools_std if t.name != "human_assistance"]
#tools_list = [get_weather, search_q] # ç›´æ¥å®šç¾©ã™ã‚‹å ´åˆ

# --- ç”»åƒå‡¦ç†é–¢æ•° (Streamlit ç”¨) ---
def process_uploaded_image(uploaded_file):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹"""
    if uploaded_file is not None:
        try:
            image_bytes = uploaded_file.getvalue()
            base64_image = base64.b64encode(image_bytes).decode('utf-8')
            return base64_image, uploaded_file.type
        except Exception as e:
            st.error(f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None
    return None, None

# --- éŸ³å£°å‡ºåŠ›é–¢æ•° (st_webrtc_37-1.py ã‹ã‚‰ã‚³ãƒ”ãƒ¼) ---
async def streaming_text_speak(llm_response):
    split_response = re.split(r'([\r\n!-;=:ã€ã€‚ \?]+)', llm_response)
    split_response = [segment for segment in split_response if segment.strip()]
    print(split_response)
    # AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’å–å¾—ã¾ãŸã¯ä½œæˆ
    # ã“ã®é–¢æ•°ãŒ st.chat_message("assistant") ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã§å‘¼ã³å‡ºã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®š
    response_placeholder = st.empty()
    partial_text = ""
    for segment in split_response:
        if segment.strip():
            partial_text += segment
            response_placeholder.markdown(f"{partial_text}") # å¤ªå­—è§£é™¤
            try:
                # --- â–¼â–¼â–¼ ä¿®æ­£ç®‡æ‰€ â–¼â–¼â–¼ ---
                # è‹±æ•°å­—ã€æ¼¢å­—ã€ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠï¼ˆå…¨è§’å«ã‚€ï¼‰ä»¥å¤–ã‚’å‰Šé™¤
                # \u4e00-\u9fff: CJKçµ±åˆæ¼¢å­—
                # \u3040-\u309f: ã²ã‚‰ãŒãª
                # \u30a0-\u30ff: ã‚«ã‚¿ã‚«ãƒŠ (å…¨è§’ã‚«ã‚¿ã‚«ãƒŠã€é•·éŸ³è¨˜å·å«ã‚€)
                # \uff10-\uff19: å…¨è§’æ•°å­—
                # \uff21-\uff3a: å…¨è§’è‹±å¤§æ–‡å­—
                # \uff41-\uff5a: å…¨è§’è‹±å°æ–‡å­—
                cleaned_segment = re.sub(
                    #r'[\*#*!-]',
                    r'[^a-zA-Z0-9\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff\uff10-\uff19\uff21-\uff3a\uff41-\uff5a]',
                    '',
                    segment
                    )
                # å‰Šé™¤ã«ã‚ˆã£ã¦ã§ããŸå¯èƒ½æ€§ã®ã‚ã‚‹é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹ã‚„å˜ç‹¬ã‚¹ãƒšãƒ¼ã‚¹ã‚‚é™¤å»ï¼ˆèª­ã¿ä¸Šã’æ™‚ã®ä¸è‡ªç„¶ãªé–“ã‚’é˜²ãï¼‰
                cleaned_segment = cleaned_segment.replace(' ', '')
                # cleaned_segmentãŒç©ºæ–‡å­—åˆ—ã«ãªã£ãŸå ´åˆã¯TTSå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if not cleaned_segment:
                    continue
                # --- â–²â–²â–² ä¿®æ­£ç®‡æ‰€ â–²â–²â–² ---
                tts = gTTS(cleaned_segment, lang="ja")
                audio_buffer = BytesIO()
                tts.write_to_fp(audio_buffer)
                audio_buffer.seek(0)
                audio = AudioSegment.from_file(audio_buffer, format="mp3")
                audio = audio._spawn(audio.raw_data, overrides={
                    "frame_rate": int(audio.frame_rate * 1.4) #1.3
                }).set_frame_rate(audio.frame_rate)
                audio_buffer.close()
                audio = audio.set_frame_rate(44100)
                audio = audio + 5
                audio = audio.fade_in(500).fade_out(500)
                audio = low_pass_filter(audio, cutoff=900)
                low_boost = low_pass_filter(audio,1000).apply_gain(10)
                audio = audio.overlay(low_boost)
                output_buffer = BytesIO()
                audio.export(output_buffer, format="mp3")
                output_buffer.seek(0)
                audio_base64 = base64.b64encode(output_buffer.read()).decode()
                output_buffer.close()
                a=len(audio_base64)
                audio_html = f"""
                    <audio id="audio-player" autoplay style="display:none;">
                    <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
                    </audio>
                """
                st.markdown(audio_html, unsafe_allow_html=True)
            except Exception as e:
                print(f"éŸ³å£°ç”Ÿæˆ/å†ç”Ÿã‚¨ãƒ©ãƒ¼: {e}") # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°æ”¹å–„
                pass
            try:
                # å†ç”Ÿæ™‚é–“ã‚’è€ƒæ…®ã—ãŸå¾…æ©Ÿï¼ˆã‚ˆã‚Šæ­£ç¢ºã«ï¼‰
                playback_duration = len(audio) / 1000.0 # milliseconds to seconds
                await asyncio.sleep(playback_duration * 0.7) # å†ç”Ÿæ™‚é–“ã®ä¸€éƒ¨ã‚’å¾…æ©Ÿ (èª¿æ•´å¯èƒ½)
            except Exception as e:
                await asyncio.sleep(1) # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

# --- éŸ³å£°å…¥åŠ›é–¢æ•° (st_webrtc_37-1.py ã‹ã‚‰ã‚³ãƒ”ãƒ¼) ---
def whis_seg2(audio_segment):
    audio_data = np.frombuffer(audio_segment.raw_data, dtype=np.int16).astype(np.float32)
    audio_data /= np.iinfo(np.int16).max
    sample_rate = audio_segment.frame_rate
    if sample_rate != 16000:
        audio_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=16000)
        sample_rate = 16000
    audio_data = whisper.pad_or_trim(audio_data)
    if not "whisper_model" in st.session_state:
        st.session_state.whisper_model = whisper.load_model("small")
    whisper_model = st.session_state.whisper_model
    result = whisper_model.transcribe(audio_data, language="ja")
    return result['text']

def transcribe(audio_segment: AudioSegment, debug: bool = False) -> str:
    answer2 ="ï¼ˆä¼‘æ­¢ä¸­ï¼‰"
    # ã‚¹ãƒ†ãƒ¬ã‚ªã®å ´åˆã€ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
    if audio_segment.channels > 1:
        try:
            audio_segment = audio_segment.set_channels(1)
        except Exception as e:
            st.error(f"éŸ³å£°ãƒãƒ£ãƒ³ãƒãƒ«ã®å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            traceback.print_exc() # è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’å‡ºåŠ›
            return "" # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™
    if debug:
        # save_audio(audio_segment, "debug_audio") # å¿…è¦ãªã‚‰æœ‰åŠ¹åŒ–
        pass
    answer2 = whis_seg2(audio_segment)
    return answer2

async def process_audio(audio_data_bytes, sample_rate, sound_chunk):
    sound = pydub.AudioSegment(
        data=audio_data_bytes,
        sample_width=2,
        frame_rate=sample_rate,
        channels=2  #NG1 # ãƒ¢ãƒãƒ©ãƒ«ã ã¨æ–‡å­—åŒ–ã‘ã™ã‚‹
    )
    sound_chunk += sound
    answer2 = ""
    if len(sound_chunk) > 0:
        answer2 = transcribe(sound_chunk)
    return answer2

async def process_audio_loop_with_silence_detection(
    frames_deque_lock,
    frames_deque,
    sound_chunk,
    amp_indicator,
    status_indicator, # è¿½åŠ 
    button_input,
    prompt,
    memory_use,memory_alt,memory_ok
    ):
    """
    éŸ³å£°ã‚’ç„¡éŸ³åŒºåˆ‡ã‚Šã§ã¾ã¨ã‚ã€ç„¡éŸ³ãŒä¸€å®šæ™‚é–“ç¶šã„ãŸã‚‰ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã‚’è¡Œã†ã€‚
    """
    audio_buffer = []
    last_sound_time = time.time()
    silence_detected = False

    while True:
        final_prompt = button_input if button_input is not None else prompt
        if final_prompt:
            return final_prompt
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—
        with frames_deque_lock:
            while len(frames_deque) > 0:
                frame = frames_deque.popleft() # å·¦ç«¯ã‹ã‚‰è¦ç´ ã‚’å–ã‚Šå‡ºã—ã¦å‰Šé™¤
                audio_chunk = frame.to_ndarray().astype(np.int16)
                audio_buffer.append(audio_chunk)
                st.session_state.frame_sample_rate = frame.sample_rate
                amp=np.max(np.abs(audio_chunk))
                #st.session_state.amp = amp
                #amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})=\n\n{amp}")
                amp_indicator.write(f"éŸ³å£°æŒ¯å¹…={amp}")
                #print(f"éŸ³å£°æŒ¯å¹…/ç„¡éŸ³é–¾å€¤={amp}/{SILENCE_THRESHOLD}")
                #amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={amp}")
                if not amp < st.session_state.amp_threshold:
                    last_sound_time = time.time()
                    silence_detected = False
                else:
                    silence_detected = time.time() - last_sound_time >= st.session_state.silence_threshold
        #print(f"ç„¡éŸ³åˆ¤å®š={silence_detected}")
        #print(f"audio_buffer={len(audio_buffer)}")
        # ç„¡éŸ³åŒºåˆ‡ã‚ŠãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
        if silence_detected and audio_buffer:
            audio_data = np.concatenate(audio_buffer).tobytes()
            try:
                answer2 = await process_audio(audio_data, st.session_state.frame_sample_rate, sound_chunk)
                ##########################################################
                #text_output.write(f"èªè­˜çµæœ: {answer}")
                #ãŠã‹ã—ãªå›ç­”ã‚’é™¤å»
                # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
                phrases = (
                    "ã‚ã‚ŠãŒã¨ã†", 
                    "ãŠç–²ã‚Œæ§˜", "ã‚“ã‚“ã‚“ã‚“ã‚“ã‚“", 
                    "by H.","ã‚¹ã‚¿ãƒƒãƒ•ã•ã‚“ã®ãŠè©±ã‚’",
                    "ã„ã„ãˆ- ã„ã„ãˆ- ã„ã„ãˆ-",
                    "ã”ã¡ãã†ã•ã¾ã§ã—ãŸ","ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²ã‚’",
                    )
                if len(answer2) < 5:
                    pass
                elif any(phrase in answer2 for phrase in phrases):
                    pass
                else:
                    #with text_output.chat_message('user'):
                        #st.write(answer2)
                    print("[Whis_seg]",answer2) 
                    return answer2 
                audio_buffer = []  # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
                silence_detected = False
            except Exception as e:
                st.error(f"éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        #amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={st.session_state.amp}")
        #amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={amp}")
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
        current_memory_use(memory_use,memory_alt,memory_ok)
        mem_use = get_memory_usage()
            
        # å‡¦ç†è² è·ã‚’æŠ‘ãˆã‚‹ãŸã‚ã«çŸ­ã„é…å»¶ã‚’æŒ¿å…¥
        time.sleep(0.1)


# --- â–¼â–¼â–¼ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1 ç”¨ã® State ã¨ãƒãƒ¼ãƒ‰å®šç¾© â–¼â–¼â–¼ ---
class EnhancedState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    needs_weather: bool
    weather_info: Optional[str]
    needs_search: bool
    search_results: Optional[str]
    is_image_query: bool # ç”»åƒãŒå«ã¾ã‚Œã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°

def classify_input(state: EnhancedState):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’åˆ†é¡ã—ã€äº‹å‰å‡¦ç†ãŒå¿…è¦ã‹åˆ¤æ–­ã™ã‚‹ãƒãƒ¼ãƒ‰"""
    last_message = state["messages"][-1]
    content = last_message.content
    text_content = ""
    has_image = False

    if isinstance(content, list):
        for part in content:
            if part.get("type") == "text":
                text_content = part.get("text", "")
            elif part.get("type") == "image_url":
                has_image = True
    elif isinstance(content, str):
        text_content = content

    needs_weather = False
    needs_search = False

    # ç”»åƒãŒãªã„å ´åˆã®ã¿ã€ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã«åŸºã¥ã„ã¦äº‹å‰å‡¦ç†ã‚’åˆ¤æ–­
    if not has_image:
        if "å¤©æ°—" in text_content:
            needs_weather = True
        elif any(phrase in text_content for phrase in ("æ¤œç´¢", "æœ€æ–°", "è©±é¡Œ", "æœ€è¿‘", "ç¾åœ¨", "ã«ã¤ã„ã¦")):
            needs_search = True

    return {
        "needs_weather": needs_weather,
        "needs_search": needs_search,
        "is_image_query": has_image,
        "weather_info": None, # åˆæœŸåŒ–
        "search_results": None # åˆæœŸåŒ–
    }

def get_weather_node(state: EnhancedState):
    """å¤©æ°—æƒ…å ±ã‚’å–å¾—ã™ã‚‹ãƒãƒ¼ãƒ‰"""
    last_message = state["messages"][-1]
    text_content = ""
    if isinstance(last_message.content, list):
         for part in last_message.content:
            if part.get("type") == "text":
                text_content = part.get("text", "")
                break
    elif isinstance(last_message.content, str):
        text_content = last_message.content

    # ç°¡å˜ãªåœ°åæŠ½å‡º (ã‚ˆã‚Šé«˜åº¦ãªæŠ½å‡ºãŒå¿…è¦ãªå ´åˆã‚ã‚Š)
    if "çŸ³å·" in text_content:city = "çŸ³å·"
    if "å°æ¾" in text_content:city = "å°æ¾"
    city_parts = text_content.split("ã®å¤©æ°—")[0].split()[-1] # "ã€‡ã€‡ã®å¤©æ°—" ã®å‰ã®å˜èªã‚’å–å¾—
    city = city_parts.replace("å¸‚","").replace("çœŒ","") # å¸‚ã‚„çœŒã‚’é™¤å»
    if "çŸ³å·" in city_parts:city = "çŸ³å·"
    if "å°æ¾" in city_parts:city = "å°æ¾"
    if not city: city = "å°æ¾" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    st.info(f"å¤©æ°—æƒ…å ±ã‚’å–å¾—ä¸­ (ãƒãƒ¼ãƒ‰): {city}")
    weather_info = get_weather.invoke({"location": f"{city}, JP"}) # @tool ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ä»˜ãé–¢æ•°ã‚’å‘¼ã³å‡ºã™
    return {"weather_info": weather_info}

# Exaæ¤œç´¢é–¢æ•° (search_q ã¨åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½ãªå ´æ‰€ã«å®šç¾©)
# @tool ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã¯ä»˜ã‘ãªã„ï¼ˆsearch_qå†…ã§å‘¼ã³å‡ºã™ãŸã‚ï¼‰
def exa_search(query: str) -> Dict[str, Any]:
    """Exaæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€æ¤œç´¢çµæœã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å«ã‚€è¾æ›¸ã‚’è¿”ã™é–¢æ•°"""
    st.info(f"Exaæ¤œç´¢ã‚’å®Ÿè¡Œä¸­: {query}") # Streamlit UI ã«è¡¨ç¤º
    #st.session_state.use_tool_name="exa_search"
    print("use_tool_name:",exa_search)
    try:
        exa_api_key = os.environ.get("EXA_API_KEY")
        if not exa_api_key:
            st.error("Exa APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return {"results": []}
        exa = Exa(api_key=exa_api_key)
        # text=True ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æœ¬æ–‡ã‚’å–å¾—ã€num_results ã§ä»¶æ•°åˆ¶é™
        # use_autoprompt=True ã¯ã‚¯ã‚¨ãƒªã‚’è‡ªå‹•ã§æœ€é©åŒ–ã—ã¾ã™ãŒã€æ„å›³ã—ãªã„æ¤œç´¢ã«ãªã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚‹ãŸã‚æ³¨æ„
        results = exa.search_and_contents(
            query=query,
            type='neural', # ã¾ãŸã¯ 'keyword'
            # use_autoprompt=True,
            num_results=3, # çµæœä»¶æ•°ã‚’çµã‚‹ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³é‡ã¨é–¢é€£æ€§ã®ãŸã‚ï¼‰
            text=True # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æœ¬æ–‡ã‚’å–å¾—
        )
        # results ã¯ SearchResponse ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ (pydanticãƒ¢ãƒ‡ãƒ«)
        return results # ãã®ã¾ã¾è¿”ã™
    except Exception as e:
        st.error(f"Exaæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return {"results": []} # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®çµæœã‚’ç¤ºã™è¾æ›¸ã‚’è¿”ã™

def search_node(state: EnhancedState):
    """Webæ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ãƒãƒ¼ãƒ‰"""
    last_message = state["messages"][-1]
    text_content = ""
    if isinstance(last_message.content, list):
         for part in last_message.content:
            if part.get("type") == "text":
                text_content = part.get("text", "")
                break
    elif isinstance(last_message.content, str):
        text_content = last_message.content

    st.info(f"Webæ¤œç´¢ã‚’å®Ÿè¡Œä¸­ (ãƒãƒ¼ãƒ‰): {text_content}")
    search_results = search_q.invoke({"query": text_content}) # @tool ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ä»˜ãé–¢æ•°ã‚’å‘¼ã³å‡ºã™
    return {"search_results": search_results}

def route_after_classification(state: EnhancedState) -> Literal["get_weather", "search", "chatbot", "__end__"]:
    """classify_input ã®çµæœã«åŸºã¥ã„ã¦ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
    if state.get("needs_weather"):
        return "get_weather"
    elif state.get("needs_search"):
        return "search"
    else:
        # ç”»åƒã‚¯ã‚¨ãƒªã®å ´åˆã€ã¾ãŸã¯äº‹å‰å‡¦ç†ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã®å ´åˆ
        return "chatbot"

# --- ã‚°ãƒ©ãƒ•å®šç¾©é–¢æ•° (ä¿®æ­£: ãƒ„ãƒ¼ãƒ«éå¯¾å¿œãƒ¢ãƒ‡ãƒ«ç”¨ã®ã‚°ãƒ©ãƒ•ã‚‚ä½œæˆ) ---
def create_graph(
    llm_instance, 
    tools_list_for_graph, 
    memory_instance, 
    trimmer_instance, 
    prompt_template_instance, 
    use_enhanced_graph=False
    ):
    """LangGraphã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆãƒ»ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹é–¢æ•°"""
    if use_enhanced_graph:
        # --- ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: äº‹å‰å‡¦ç†ãƒãƒ¼ãƒ‰ã‚’å«ã‚€ã‚°ãƒ©ãƒ• ---
        graph_builder = StateGraph(EnhancedState) # æ‹¡å¼µ State ã‚’ä½¿ç”¨
        graph_builder.add_node("classify_input", classify_input)
        graph_builder.add_node("get_weather", get_weather_node)
        graph_builder.add_node("search", search_node)

        def enhanced_chatbot_node(state: EnhancedState, config: Optional[dict] = None):
            language = config.get("configurable", {}).get("language", "Japanese") if config else "Japanese"
            trimmed_messages = trimmer_instance.invoke(state["messages"])
            # äº‹å‰å‡¦ç†ã®çµæœã‚’å–å¾—
            weather_info = state.get("weather_info")
            search_results = state.get("search_results")
            # æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
            last_human_message = None
            for msg in reversed(trimmed_messages):
                if isinstance(msg, HumanMessage):
                    last_human_message = msg
                    break
            if last_human_message is None:
                return {"messages": [AIMessage(content="ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")]}

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«äº‹å‰å‡¦ç†æƒ…å ±ã‚’è¿½åŠ 
            prompt_content_parts = []
            original_text = ""
            image_part = None

            if isinstance(last_human_message.content, list):
                for part in last_human_message.content:
                    if part.get("type") == "text":
                        original_text = part.get("text", "")
                    elif part.get("type") == "image_url":
                        image_part = part
            elif isinstance(last_human_message.content, str):
                original_text = last_human_message.content

            prompt_content_parts.append({"type": "text", "text": original_text})
            if image_part:
                prompt_content_parts.append(image_part)

            additional_context = ""
            # --- â–¼â–¼â–¼ æ—¥ä»˜ã«é–¢ã™ã‚‹å‡¦ç†ã‚’è¿½åŠ  â–¼â–¼â–¼ ---
            # original_text ã«æ—¥ä»˜é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            date_keywords = ["ä»Šæ—¥", "æ—¥ä»˜", "ä½•æ—¥", "æ—¥æ™‚"]
            if any(keyword in original_text for keyword in date_keywords):
                current_date_info = get_current_datetime() # ç¾åœ¨ã®æ—¥ä»˜ãƒ»æ™‚åˆ»ã‚’å–å¾—
                additional_context += f"\n\n[ç¾åœ¨ã®æ—¥æ™‚æƒ…å ±]\n{current_date_info}"
            # --- â–²â–²â–² æ—¥ä»˜ã«é–¢ã™ã‚‹å‡¦ç†ã‚’è¿½åŠ  â–²â–²â–² ---
            elif weather_info: # elif ã«å¤‰æ›´ã—ã¦ã€æ—¥ä»˜æƒ…å ±ã¨å¤©æ°—/æ¤œç´¢ãŒé‡è¤‡ã—ãªã„ã‚ˆã†ã«ã™ã‚‹
                additional_context += f"\n\n[å–å¾—æ¸ˆã¿ã®å¤©æ°—æƒ…å ±]\n{weather_info}"
            elif search_results:
                additional_context += f"\n\n[å–å¾—æ¸ˆã¿ã®æ¤œç´¢çµæœ]\n{search_results}"

            if additional_context:
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ãƒ¼ãƒˆã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
                #prompt_content_parts[0]["text"] += additional_context
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ãƒ¼ãƒˆã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ ã™ã‚‹æ–¹æ³•ã‚’èª¿æ•´
                # æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ãƒ¼ãƒˆã«è¿½åŠ ã™ã‚‹ã‹ã€æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ãƒ¼ãƒˆã¨ã—ã¦æŒ¿å…¥ã™ã‚‹ã‹
                found_text_part = False
                for part in prompt_content_parts:
                    if part.get("type") == "text":
                        part["text"] += additional_context
                        found_text_part = True
                        break
                if not found_text_part: # ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ãƒ¼ãƒˆãŒå…ƒã€…ãªã„å ´åˆ (é€šå¸¸ã¯ãªã„ã¯ãš)
                    prompt_content_parts.insert(0, {"type": "text", "text": additional_context})

            # æœ€çµ‚çš„ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’ä½œæˆ (System + éå»å±¥æ­´ + åŠ å·¥æ¸ˆã¿æœ€æ–°Human)
            # trimmer_instance.invoke(state["messages"]) ã§å¾—ã‚‰ã‚Œã‚‹ã®ã¯æœ€æ–°ã®HumanMessageã‚’å«ã¾ãªã„å ´åˆãŒã‚ã‚‹ã®ã§æ³¨æ„
            # state["messages"] ã‚’ç›´æ¥ä½¿ã†æ–¹ãŒç¢ºå®Ÿã‹ã‚‚ã—ã‚Œãªã„
            final_messages_for_llm = trimmer_instance.invoke(state["messages"][:-1]) # æœ€æ–°ã‚’é™¤ãå±¥æ­´
            final_messages_for_llm.append(HumanMessage(content=prompt_content_parts)) # åŠ å·¥æ¸ˆã¿æœ€æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨
            prompt_value = prompt_template_instance.invoke({"messages": final_messages_for_llm, "language": language})
            # print("prompt_value=",prompt_value)
            try:
                prompt_messages = prompt_value.to_messages()
                # # --- â–¼â–¼â–¼ ç¢ºèªç”¨ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ  â–¼â–¼â–¼ ---
                # print("-" * 80)
                # print(">>> enhanced_chatbot_node: LLMã«æ¸¡ã•ã‚Œã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’ç¢ºèª <<<")
                # if prompt_messages:
                #     first_message = prompt_messages[0]
                #     print(f"  [ã‚¿ã‚¤ãƒ—]: {type(first_message)}")
                #     if isinstance(first_message, SystemMessage):
                #         print("  [å†…å®¹]:")
                #         # content ãŒé•·ã„å ´åˆãŒã‚ã‚‹ã®ã§ã€æ”¹è¡Œã—ã¦è¦‹ã‚„ã™ãè¡¨ç¤º
                #         content_lines = first_message.content.split('\n')
                #         for line in content_lines:
                #             print(f"    {line.strip()}") # å„è¡Œã®å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºç™½ã‚’é™¤å»
                #         # SYSTEM_MESSAGE å®šæ•°ã¨æ¯”è¼ƒ (ä»»æ„)
                #         if first_message.content == SYSTEM_MESSAGE:
                #             print("  [ç¢ºèª]: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãª SYSTEM_MESSAGE ã¨ä¸€è‡´ã—ã¾ã™ã€‚")
                #         else:
                #             print("  [è­¦å‘Š]: ã‚°ãƒ­ãƒ¼ãƒãƒ«ãª SYSTEM_MESSAGE ã¨å†…å®¹ãŒç•°ãªã‚Šã¾ã™ï¼")
                #     else:
                #         print(f"  [è­¦å‘Š]: æœ€åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒ SystemMessage ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ (ã‚¿ã‚¤ãƒ—: {type(first_message)})")
                #     # å¿…è¦ã§ã‚ã‚Œã°ä»–ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚ç¢ºèª
                #     # print("  --- å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¦‚è¦ ---")
                #     # for i, msg in enumerate(prompt_messages):
                #     #     content_preview = str(msg.content)[:100].replace('\n', ' ') + "..." if len(str(msg.content)) > 100 else str(msg.content).replace('\n', ' ')
                #     #     print(f"    {i}: ({type(msg).__name__}) {content_preview}")
                # else:
                #     print("  [ã‚¨ãƒ©ãƒ¼]: prompt_messages ãƒªã‚¹ãƒˆãŒç©ºã§ã™ï¼")
                # print("-" * 80)
                # # --- â–²â–²â–² ç¢ºèªç”¨ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ  â–²â–²â–² ---
            except Exception as e:
                st.error(f"PromptValue ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
                return {"messages": [AIMessage(content=f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")]}

            # LLMå‘¼ã³å‡ºã— (ãƒ„ãƒ¼ãƒ«ã¯ãƒã‚¤ãƒ³ãƒ‰ã—ãªã„)
            try:
                # llm_instance ã¯ session_state ã‹ã‚‰å–å¾— (ãƒ„ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒ‰ã•ã‚Œã¦ã„ãªã„ã‚‚ã®)
                llm_instance_no_tools = st.session_state.llm_instance_no_tools # äº‹å‰ã«ç”¨æ„ã—ã¦ãŠãå¿…è¦ã‚ã‚Š
                message = llm_instance_no_tools.invoke(prompt_messages)
                return {"messages": [message]}
            except Exception as e:
                st.error(f"Enhanced Chatbot Node Error: {e}")
                error_content = f"ãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n```\n{traceback.format_exc()}\n```"
                return {"messages": [AIMessage(content=error_content)]}

        graph_builder.add_node("chatbot", enhanced_chatbot_node)
        graph_builder.add_edge(START, "classify_input")
        graph_builder.add_conditional_edges(
            "classify_input",
            route_after_classification,
            {
                "get_weather": "get_weather",
                "search": "search",
                "chatbot": "chatbot",
                "__end__": END # ã“ã®ãƒ‘ã‚¹ã¯é€šå¸¸é€šã‚‰ãªã„ã¯ãš
            }
        )
        graph_builder.add_edge("get_weather", "chatbot")
        graph_builder.add_edge("search", "chatbot")
        graph_builder.add_edge("chatbot", END) # chatbot ãƒãƒ¼ãƒ‰ã®å¾Œã¯çµ‚äº†

        return graph_builder.compile(checkpointer=memory_instance)

    else:
        # --- é€šå¸¸ã®ã‚°ãƒ©ãƒ• (ãƒ„ãƒ¼ãƒ«ã‚ã‚Š/ãªã—è‡ªå‹•åˆ¤åˆ¥) ---
        graph_builder = StateGraph(EnhancedState) # State ã¯ EnhancedState ã‚’ä½¿ã†

        def chatbot_node(state: EnhancedState, config: Optional[dict] = None):
            language = config.get("configurable", {}).get("language", "Japanese") if config else "Japanese"
            trimmed_messages = trimmer_instance.invoke(state["messages"])

            prompt_value = prompt_template_instance.invoke({"messages": trimmed_messages, "language": language})
            try:
                prompt_messages = prompt_value.to_messages()

            except Exception as e:
                st.error(f"PromptValue ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
                return {"messages": [AIMessage(content=f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")]}

            # llm_instance ã¯ session_state ã‹ã‚‰å–å¾— (ãƒ„ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒ‰æ¸ˆã¿ or ãªã—)
            llm_to_use = st.session_state.llm_instance
            current_tools = tools_list_for_graph # ã‚°ãƒ©ãƒ•ä½œæˆæ™‚ã«æ¸¡ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ

            try:
                use_tools = bool(current_tools)
                has_image = state.get("is_image_query", False) # classify_input ãŒã‚ã‚Œã°ãã“ã‹ã‚‰å–å¾—

                # ç”»åƒãŒã‚ã‚Šã€ã‹ã¤ç”»åƒã¨ãƒ„ãƒ¼ãƒ«ã®ä½µç”¨ãŒä¸å¯ã®å ´åˆã€ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã‚ãªã„
                if has_image and not st.session_state.can_use_tools_with_image:
                    use_tools = False
                    st.warning("ç”»åƒæ·»ä»˜æ™‚ã¯ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")
                    # ãƒ„ãƒ¼ãƒ«ãªã—LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ã† (å¿…è¦ãªã‚‰)
                    # llm_to_use = st.session_state.llm_instance_no_tools

                # ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã†å ´åˆ (bind_toolsæ¸ˆã¿ã®ã¯ãš) / ä½¿ã‚ãªã„å ´åˆ
                message = llm_to_use.invoke(prompt_messages)
                return {"messages": [message]}

            except Exception as e:
                st.error(f"Chatbot Node Error: {e}")
                error_content = f"ãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n```\n{traceback.format_exc()}\n```"
                return {"messages": [AIMessage(content=error_content)]}

        graph_builder.add_node("chatbot", chatbot_node)
        tool_node_func = ToolNode(tools=tools_list_for_graph)
        graph_builder.add_node("tools", tool_node_func)

        if tools_list_for_graph: # ãƒ„ãƒ¼ãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿æ¡ä»¶åˆ†å²ã¨ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰ã¸ã®ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
            graph_builder.add_conditional_edges(
                "chatbot",
                tools_condition,
                {"tools": "tools", END: END}
            )
            graph_builder.add_edge("tools", "chatbot")
        else:
            graph_builder.add_edge("chatbot", END)

        graph_builder.add_edge(START, "chatbot")

        return graph_builder.compile(checkpointer=memory_instance)


class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.frame = None

    def recv(self, frame):
        self.frame = frame.to_ndarray(format="bgr24")
        return frame

# --- Streamlit ã‚¢ãƒ—ãƒªæœ¬ä½“ ---
async def main():
    st.set_page_config(
        page_title="Yas Chatbot",
        page_icon="ğŸ¤–",
        layout="wide"
        )
    st.sidebar.title("ğŸ¤– Yas Chatbot")
    st.sidebar.caption("ã‚«ãƒ¡ãƒ©ç”»åƒã€ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã€Webæœ€æ–°æƒ…å ±ã€éŸ³å£°ã§ã®å•åˆã›ãŒã§ãã¾ã™")

    # --- Webã‚«ãƒ¡ãƒ©è¨­å®š ---
    frames_deque_lock = threading.Lock()
    frames_deque: deque = deque([])
    async def queued_audio_frames_callback(
        frames: List[av.AudioFrame],
    ) -> av.AudioFrame:
        with frames_deque_lock:
            frames_deque.extend(frames)
        # Return empty frames to be silent.
        new_frames = []
        for frame in frames:
            input_array = frame.to_ndarray()
            new_frame = av.AudioFrame.from_ndarray(
                np.zeros(input_array.shape, dtype=input_array.dtype),
                layout=frame.layout.name,
            )
            new_frame.sample_rate = frame.sample_rate
            new_frames.append(new_frame)
        return new_frames

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¡¨ç¤º
    with st.sidebar:
        st.header("Webcam Stream")
        webrtc_ctx = webrtc_streamer(
            key="speech-to-text-w-video",
            desired_playing_state=True,
            mode=WebRtcMode.SENDRECV,
            queued_audio_frames_callback=queued_audio_frames_callback,
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            media_stream_constraints={"video": True, "audio": True},
            video_processor_factory=VideoTransformer,
            async_processing=True, # Streamlitã‚¢ãƒ—ãƒªã®å¿œç­”æ€§ã‚’ä¿ã¤ãŸã‚éåŒæœŸå‡¦ç†ã‚’æ¨å¥¨  
        )
    if not webrtc_ctx.state.playing :
        st.sidebar.warning("Webã‚«ãƒ¡ãƒ©ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")
        return
    # --- åˆæœŸåŒ– ---
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "selected_model_name" not in st.session_state:
        st.session_state.selected_model_name = None
    if "graph_or_agent" not in st.session_state:
        st.session_state.graph_or_agent = None
    if "memory" not in st.session_state:
        st.session_state.memory = None
    if "thread_id" not in st.session_state:
        st.session_state.thread_id = None
    if "is_react" not in st.session_state:
        st.session_state.is_react = False
    if "llm_instance" not in st.session_state: # ãƒã‚¤ãƒ³ãƒ‰æ¸ˆã¿ or ReActç”¨
        st.session_state.llm_instance = None
    if "llm_instance_no_tools" not in st.session_state: # ãƒ„ãƒ¼ãƒ«ãªã—ç”¨ (enhanced_graphç”¨)
        st.session_state.llm_instance_no_tools = None
    if "trimmer" not in st.session_state:
        st.session_state.trimmer = None
    if "can_use_tools_with_image" not in st.session_state:
        st.session_state.can_use_tools_with_image = True
    if "use_enhanced_graph" not in st.session_state: # Enhanced Graph ã‚’ä½¿ã†ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°
        st.session_state.use_enhanced_graph = False
    # --- éŸ³å£°å…¥å‡ºåŠ›ç”¨ã® session_state ã‚’è¿½åŠ  ---
    if "input_method" not in st.session_state:
        st.session_state.input_method = "éŸ³å£°&ãƒ†ã‚­ã‚¹ãƒˆ" # å…¥åŠ›ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    if "output_method" not in st.session_state:
        st.session_state.output_method = "ãƒ†ã‚­ã‚¹ãƒˆ"
    if "whisper_model" not in st.session_state:
        st.session_state.whisper_model = None # Whisperãƒ¢ãƒ‡ãƒ«ã‚’ä¿æŒ
    if "frame_sample_rate" not in st.session_state:
        st.session_state.frame_sample_rate = None # éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
    if "amp_threshold" not in st.session_state:
        st.session_state.amp_threshold = 800 # ç„¡éŸ³é–¾å€¤ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    if "silence_threshold" not in st.session_state:
        st.session_state.silence_threshold = 0.5 # ç„¡éŸ³æ™‚é–“ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    # ç’°å¢ƒå¤‰æ•°è¨­å®š (åˆå›ã®ã¿)
    with st.spinner("ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šä¸­..."):
        try:
            my_environments.setup_environment()
            #info_disp.info("ç’°å¢ƒå¤‰æ•°ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            st.sidebar.error(f"ç’°å¢ƒå¤‰æ•°ã®è¨­å®šã«å¤±æ•—: {e}")
            st.error("ç¶šè¡Œã§ãã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.stop()

    # ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆå–å¾— (ã‚­ãƒ£ãƒƒã‚·ãƒ¥)
    @st.cache_resource
    def load_models():
        models = my_llms.initialize_models()
        return models

    #info_disp.info("LLMãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")
    models_dict = load_models()
    #info_disp.info("LLMãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–å®Œäº†ã€‚")
    model_names = list(models_dict.keys())

    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒ¢ãƒ‡ãƒ«é¸æŠ ---
    selected_model_name = st.sidebar.selectbox(
        "è¨€èªãƒ¢ãƒ‡ãƒ«é¸æŠ(å„ªåŠ£æœ‰ã‚Š):",
        model_names,
        index=model_names.index(st.session_state.selected_model_name) if st.session_state.selected_model_name in model_names else 0,
        key="model_select"
    )

    uploaded_file = st.sidebar.file_uploader(
        "ã“ã“ã«ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦å•åˆã› (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)",
        type=["jpg", "jpeg", "png"],
        key="file_uploader"
    )
    # --- å…¥å‡ºåŠ›æ–¹æ³•é¸æŠã‚’è¿½åŠ  ---
    #st.sidebar.title("å…¥å‡ºåŠ›è¨­å®š")
    #st.sidebar.title("å‡ºåŠ›æ–¹æ³•è¨­å®š")
    #st.session_state.input_method = st.sidebar.radio("å…¥åŠ›æ–¹æ³•:", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°&ãƒ†ã‚­ã‚¹ãƒˆ"), index=1 if st.session_state.input_method == "ãƒ†ã‚­ã‚¹ãƒˆ" else 1)
    st.session_state.output_method = st.sidebar.radio("å‡ºåŠ›æ–¹æ³•:", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"), index=0 if st.session_state.output_method == "ãƒ†ã‚­ã‚¹ãƒˆ" else 1)
    # --- éŸ³å£°å…¥åŠ›è¨­å®šã‚’è¿½åŠ  ---
    #if st.session_state.input_method == "éŸ³å£°&ãƒ†ã‚­ã‚¹ãƒˆ":
    amp_indicator = st.sidebar.empty() # éŸ³å£°æŒ¯å¹…è¡¨ç¤ºç”¨
    st.sidebar.subheader("éŸ³å£°å…¥åŠ›è¨­å®š")
    st.session_state.amp_threshold = st.sidebar.slider(
        "ç„¡éŸ³æŒ¯å¹…é–¾å€¤ (å°ã•ã„ã»ã©æ•æ„Ÿ):",
        min_value=100, max_value=3000, value=st.session_state.amp_threshold, step=100
        )
    st.session_state.silence_threshold = st.sidebar.slider(
        "ç„¡éŸ³æœ€å°æ™‚é–“ (ç§’):",
        min_value=0.1, max_value=3.0, value=st.session_state.silence_threshold, step=0.1
        )
    # Whisperãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ (åˆå›ã®ã¿)
    if st.session_state.whisper_model is None:
        with st.spinner("Whisperãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­..."):
            st.session_state.whisper_model = whisper.load_model("small")

    # --- æƒ…å ±è¡¨ç¤º ---
    info_disp = st.sidebar.empty() 
    memory_use = st.sidebar.empty()
    memory_alt = st.sidebar.empty()
    memory_ok = st.sidebar.empty()
    
    # --- ãƒ¢ãƒ‡ãƒ«/ã‚°ãƒ©ãƒ•/ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å†åˆæœŸåŒ– ---
    if st.session_state.selected_model_name is None or selected_model_name != st.session_state.selected_model_name:
        print("#"*100)
        print("st.session_state.selected_model_name=",st.session_state.selected_model_name)
        print("selected_model_name=",selected_model_name)
        st.session_state.selected_model_name = selected_model_name
        st.session_state.messages = []
        st.session_state.graph_or_agent = None
        st.session_state.memory = MemorySaver()
        st.session_state.thread_id = f"streamlit_thread_{selected_model_name}_{str(uuid.uuid4())}"

        # ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾— (ãƒ„ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒ‰å‰)
        llm_instance_base, cost_input, cost_cached_input, cost_output, input_max = models_dict[selected_model_name]
        st.session_state.llm_instance_no_tools = llm_instance_base # ãƒ„ãƒ¼ãƒ«ãªã—ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä¿å­˜

        st.session_state.is_react = False
        st.session_state.can_use_tools_with_image = True
        st.session_state.use_enhanced_graph = False # Enhanced Graph ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ

        #st.sidebar.info(f"ãƒ¢ãƒ‡ãƒ« '{selected_model_name}' ã‚’æº–å‚™ä¸­...")
        info_disp.info(f"ãƒ¢ãƒ‡ãƒ« '{selected_model_name}' ã‚’æº–å‚™ä¸­...")

        # Trimmer ã®åˆæœŸåŒ–
        st.session_state.trimmer = trim_messages(
            max_tokens=int(input_max * 0.8),
            strategy="last",
            token_counter=lambda messages: get_token_count_for_messages(messages, model_name="gpt-4"),
            include_system=False,
            allow_partial=False,
        )

        # --- ã‚°ãƒ©ãƒ•/ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæº–å‚™ãƒ­ã‚¸ãƒƒã‚¯ (ä¿®æ­£) ---
        #@st.cache_resource(show_spinner=f"'{selected_model_name}' ã®ã‚°ãƒ©ãƒ•/ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æº–å‚™ä¸­...")
        def prepare_graph_or_agent(_model_name, _llm_base, _memory, _trimmer, _tools):
            """
            ã‚°ãƒ©ãƒ•ã¾ãŸã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æº–å‚™ã—ã€ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™ã€‚
            æˆ»ã‚Šå€¤: (graph_or_agent, is_react, can_use_tools_img, use_enhanced, logs)
            logs: List of (level, message) tuples. level can be 'info', 'success', 'warning', 'error'.
            """
            _graph_or_agent = None
            _is_react = False
            _can_use_tools_img = True
            _current_tools = list(_tools)
            _use_enhanced = False
            logs = [] # ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
            # ç‰¹å®šãƒ¢ãƒ‡ãƒ«ã®åˆ¤å®š (ãƒ„ãƒ¼ãƒ«éå¯¾å¿œ or ç”»åƒä½µç”¨ä¸å¯)
            no_tool_models = ("c4ai-aya-vision-32b", "nvidia/llama-3.1", "neva", "Phi", "phi", "kosmos", "fuyu", "gemma") # aya ã‚’å«ã‚€
            is_no_tool_model = any(phrase in _model_name for phrase in no_tool_models)

            if is_no_tool_model:
                # ãƒ„ãƒ¼ãƒ«éå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®å ´åˆ -> Enhanced Graph ã‚’ä½¿ç”¨
                #info_disp.info(f"{_model_name}: ãƒ„ãƒ¼ãƒ«éå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€‚äº‹å‰å‡¦ç†ãƒãƒ¼ãƒ‰ä»˜ãã‚°ãƒ©ãƒ•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                #print(f"{_model_name}: ãƒ„ãƒ¼ãƒ«éå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€‚äº‹å‰å‡¦ç†ãƒãƒ¼ãƒ‰ä»˜ãã‚°ãƒ©ãƒ•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                log_msg = f"{_model_name}: ãƒ„ãƒ¼ãƒ«éå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€‚äº‹å‰å‡¦ç†ãƒãƒ¼ãƒ‰ä»˜ãã‚°ãƒ©ãƒ•ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
                logs.append(("info", log_msg))
                print("enhanced_info", log_msg) # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚å‡ºåŠ›ï¼ˆä»»æ„ï¼‰
                print("enhanced_prompt_template=",prompt_template)
                _graph_or_agent = create_graph(_llm_base, [], _memory, _trimmer, prompt_template, use_enhanced_graph=True)
                _is_react = False
                _can_use_tools_img = False # ãƒ„ãƒ¼ãƒ«è‡ªä½“ä½¿ã‚ãªã„
                _use_enhanced = True

            else:
                # ãƒ„ãƒ¼ãƒ«å¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®å ´åˆ: bind_tools ã‚’è©¦ã™
                try:
                    _llm_bound = _llm_base.bind_tools(_current_tools)
                    # ãƒ„ãƒ¼ãƒ«ã‚ã‚Šã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
                    _graph_or_agent = create_graph(_llm_bound, _current_tools, _memory, _trimmer, prompt_template, use_enhanced_graph=False)
                    _is_react = False
                    _can_use_tools_img = True # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ä½µç”¨å¯èƒ½ã¨ã™ã‚‹ (chatbot_nodeå†…ã§æœ€çµ‚åˆ¤æ–­)
                    #info_disp.success(f"{_model_name}: ãƒ„ãƒ¼ãƒ«å¯¾å¿œã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
                    #print(f"{_model_name}: ãƒ„ãƒ¼ãƒ«å¯¾å¿œã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
                    log_msg = f"{_model_name}: ãƒ„ãƒ¼ãƒ«å¯¾å¿œã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸã€‚"
                    logs.append(("success", log_msg))
                    print(log_msg)
                except (ValueError, AttributeError, NotImplementedError, Exception) as e:
                    # bind_tools å¤±æ•— -> ReAct ã‚’è©¦ã™
                    #st.sidebar.warning(f"{_model_name}: ã‚°ãƒ©ãƒ•ä½œæˆ/ãƒ„ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒ‰å¤±æ•—: {e}. ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è©¦ã¿ã¾ã™ã€‚")
                    #print(f"{_model_name}: ã‚°ãƒ©ãƒ•ä½œæˆ/ãƒ„ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒ‰å¤±æ•—: {e}. ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è©¦ã¿ã¾ã™ã€‚")
                    log_msg = f"{_model_name}: ã‚°ãƒ©ãƒ•ä½œæˆ/ãƒ„ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒ‰å¤±æ•—: {e}. ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è©¦ã¿ã¾ã™ã€‚"
                    logs.append(("warning", log_msg)) # st.sidebar.warning ã®ä»£ã‚ã‚Šã«ãƒ­ã‚°è¿½åŠ 
                    print(log_msg)
                    _is_react = True
                    try:
                        _graph_or_agent = create_react_agent(_llm_base, _tools, checkpointer=_memory)
                        _can_use_tools_img = True # ReActã§ã‚‚ç”»åƒä½µç”¨å¯å¦ã¯åˆ¥é€”åˆ¶å¾¡ãŒå¿…è¦ãªå ´åˆã‚ã‚Š
                        #info_disp.info(f"{_model_name}: ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ(ãƒ„ãƒ¼ãƒ«ã‚ã‚Š)ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
                        #print(f"{_model_name}: ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ(ãƒ„ãƒ¼ãƒ«ã‚ã‚Š)ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
                        log_msg = f"{_model_name}: ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ(ãƒ„ãƒ¼ãƒ«ã‚ã‚Š)ã‚’ä½œæˆã—ã¾ã—ãŸã€‚"
                        logs.append(("info", log_msg))
                        print(log_msg)
                    except Exception as react_e:
                        #st.sidebar.error(f"ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ(ãƒ„ãƒ¼ãƒ«ã‚ã‚Š)ä½œæˆå¤±æ•—: {react_e}")
                        #print(f"ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ(ãƒ„ãƒ¼ãƒ«ã‚ã‚Š)ä½œæˆå¤±æ•—: {react_e}")
                        log_msg = f"ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ(ãƒ„ãƒ¼ãƒ«ã‚ã‚Š)ä½œæˆå¤±æ•—: {react_e}"
                        logs.append(("error", log_msg)) # st.sidebar.error ã®ä»£ã‚ã‚Šã«ãƒ­ã‚°è¿½åŠ 
                        print(log_msg)
                        try:
                            _graph_or_agent = create_react_agent(_llm_base, [], checkpointer=_memory)
                            _current_tools = []
                            _can_use_tools_img = False
                            #info_disp.warning(f"{_model_name}: ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ(ãƒ„ãƒ¼ãƒ«ãªã—)ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
                            #print(f"{_model_name}: ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ(ãƒ„ãƒ¼ãƒ«ãªã—)ã‚’ä½œæˆã—ã¾ã—ãŸã€‚")
                            log_msg = f"{_model_name}: ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ(ãƒ„ãƒ¼ãƒ«ãªã—)ã‚’ä½œæˆã—ã¾ã—ãŸã€‚"
                            logs.append(("warning", log_msg)) # st.sidebar.warning ã®ä»£ã‚ã‚Šã«ãƒ­ã‚°è¿½åŠ 
                            print(log_msg)
                        except Exception as react_no_tool_e:
                            #st.sidebar.error(f"ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ(ãƒ„ãƒ¼ãƒ«ãªã—)ä½œæˆå¤±æ•—: {react_no_tool_e}")
                            log_msg = f"ReActã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ(ãƒ„ãƒ¼ãƒ«ãªã—)ä½œæˆå¤±æ•—: {react_no_tool_e}"
                            logs.append(("error", log_msg)) # st.sidebar.error ã®ä»£ã‚ã‚Šã«ãƒ­ã‚°è¿½åŠ 
                            print(log_msg)
                            _graph_or_agent = None
                            _is_react = False
                            _can_use_tools_img = False

            return _graph_or_agent, _is_react, _can_use_tools_img, _use_enhanced,logs

        # ã‚°ãƒ©ãƒ•/ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æº–å‚™ã—ã¦ session_state ã«ä¿å­˜
        graph_or_agent, is_react, can_use_tools_with_image, use_enhanced, prep_logs = prepare_graph_or_agent(
            st.session_state.selected_model_name,
            llm_instance_base, # ãƒã‚¤ãƒ³ãƒ‰å‰ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æ¸¡ã™
            st.session_state.memory,
            st.session_state.trimmer,
            tools_list
        )
        st.session_state.graph_or_agent = graph_or_agent
        st.session_state.is_react = is_react
        st.session_state.can_use_tools_with_image = can_use_tools_with_image
        st.session_state.use_enhanced_graph = use_enhanced
        print("graph_or_agent=",graph_or_agent)
        print("is_react=",is_react)
        print("can_use_tools_with_image=",can_use_tools_with_image)
        print("use_enhanced=",use_enhanced)
        
        # è¿”ã•ã‚ŒãŸãƒ­ã‚°ã‚’è¡¨ç¤º
        final_log_level = "info" # æœ€çµ‚çš„ãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºç”¨
        for level, message in prep_logs:
            if level == "info":
                info_disp.info(message) # info_disp ã‚’ä½¿ã†
                final_log_level = "info"
            elif level == "success":
                info_disp.success(message) # info_disp ã‚’ä½¿ã†
                final_log_level = "success"
            elif level == "warning":
                st.sidebar.warning(message) # sidebar ã«è¡¨ç¤º
                final_log_level = "warning"
            elif level == "error":
                st.sidebar.error(message) # sidebar ã«è¡¨ç¤º
                final_log_level = "error"

        # æœ€å¾Œã®ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦ info_disp ã®æœ€çµ‚çŠ¶æ…‹ã‚’è¨­å®š (ä»»æ„)
        if final_log_level == "info":
            info_disp.info(f"'{selected_model_name}' ã®æº–å‚™å®Œäº†ã€‚")
        elif final_log_level == "success":
            info_disp.success(f"'{selected_model_name}' ã®æº–å‚™å®Œäº†ã€‚")
        elif final_log_level == "warning":
            info_disp.warning(f"'{selected_model_name}' ã®æº–å‚™å®Œäº† (è­¦å‘Šã‚ã‚Š)ã€‚")
        elif final_log_level == "error":
            info_disp.error(f"'{selected_model_name}' ã®æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")


        # llm_instance ã«ã¯ã€ã‚°ãƒ©ãƒ•/ReActã§å®Ÿéš›ã«ä½¿ã†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å…¥ã‚Œã‚‹
        # Enhanced Graph ã®å ´åˆã¯ãƒ„ãƒ¼ãƒ«ãªã—ã€ãã‚Œä»¥å¤–ã¯ bind_tools è©¦è¡Œå¾Œã®ã‚‚ã® or ReActç”¨
        if use_enhanced:
            st.session_state.llm_instance = llm_instance_base # Enhanced Graph ã¯ãƒ„ãƒ¼ãƒ«ãªã—
        elif is_react:
            st.session_state.llm_instance = llm_instance_base # ReAct ã‚‚ bind_tools ã—ãªã„
        else:
            # ãƒ„ãƒ¼ãƒ«ã‚ã‚Šã‚°ãƒ©ãƒ•ã®å ´åˆã€bind_tools æ¸ˆã¿ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ã†
            try:
                st.session_state.llm_instance = llm_instance_base.bind_tools(tools_list)
            except: # bind å¤±æ•—æ™‚ã¯ãƒ„ãƒ¼ãƒ«ãªã—ã‚°ãƒ©ãƒ•ã®ã¯ãšãªã®ã§å…ƒã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
                st.session_state.llm_instance = llm_instance_base


        if st.session_state.graph_or_agent is None:
            st.error(f"{st.session_state.selected_model_name} ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‹•ä½œã—ã¾ã›ã‚“ã€‚")
            st.stop()
        else:
            st.rerun()

    # --- ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚³ãƒ³ãƒ†ãƒŠå®šç¾© ---
    history_container = st.container(height=400)
    button_container = st.container()

    # --- ãƒœã‚¿ãƒ³å®šç¾© ---
    button_input = None
    with button_container:
        st.write("å•åˆã›ã‚¯ã‚¤ãƒƒã‚¯ãƒœã‚¿ãƒ³:")
        cols = st.columns(7)
        button_definitions = [
            ("ç”»åƒèª¬æ˜", "ç”»åƒã®å†…å®¹ã‚’è©³ã—ãèª¬æ˜ã—ã¦"),
            ("å‰ç”»åƒã¨å·®", "å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"),
            ("ç”»åƒæ–‡ã®ç¿»è¨³", "ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"),
            ("ä½•æ—¥", "ä»Šæ—¥ã¯ä½•æ—¥ã§ã™ã‹ï¼Ÿ"),
            ("ã¯ã˜ã‚ã®è³ªå•", "ã¯ã˜ã‚ã®è³ªå•ã‚’æ†¶ãˆã¦ã„ã¾ã™ã‹?"),
            ("å¤©æ°—", "å°æ¾å¸‚ã®æ˜æ—¥ã¨æ˜å¾Œæ—¥ã®å¤©æ°—ã¯ã€‚èŠ±ç²‰æƒ…å ±ã‚‚æ•™ãˆã¦ã€‚"),
            ("ãƒ‹ãƒ¥ãƒ¼ã‚¹", "ä»Šæ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯?"),
            ("AIæƒ…å ±", "ä»Šæ—¥ã®AIã«é–¢ã™ã‚‹æƒ…å ±ã¯ï¼Ÿ"),
            ("äººæ°—æ¼«ç”»", "ç¾åœ¨ã€äººæ°—ã®æ¼«ç”»ã‚’5ä»¶æ•™ãˆã¦ã€‚"),
            ("ä¸Šæ˜ æ˜ ç”»", "ç¾åœ¨ã€ä¸Šæ˜ ä¸­ã¾ãŸã¯ä¸Šæ˜ äºˆå®šã®æ˜ ç”»ã‚’5ä»¶æ•™ãˆã¦ã€‚"),
            ("ã‚ªã‚»ãƒ­ä½œæˆ", "Webç”»é¢ã§ãƒ—ãƒ¬ã‚¤ã™ã‚‹ã‚ªã‚»ãƒ­ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦"),
            ("ç™¾åå±±","æ—¥æœ¬ã®ç™¾åå±±ã‚’æ¨™é«˜ã®é«˜ã„é †ä½ã‹ã‚‰10å±±æ•™ãˆã¦"),
            ("å°æ¾ã®æ–™ç†åº—", "å°æ¾å¸‚ã®æœ€è¿‘è©±é¡Œã®æ–™ç†åº—ã¯ï¼Ÿ"),
            ("äººç”Ÿã®æ„ç¾©", "äººç”Ÿã®æ„ç¾©ã¯ï¼Ÿ"),
        ]
        for i, (label, query) in enumerate(button_definitions):
            if cols[i % 7].button(label, key=f"button_{i}"):
                button_input = query

    # --- ãƒãƒ£ãƒƒãƒˆå…¥åŠ› ---
    prompt = st.chat_input("ğŸ¤–ã‚¯ã‚¤ãƒƒã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã‹ã€ã“ã“ã«å•åˆã›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...", key="chat_input")

    # --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º ---
    with history_container:
        for msg in st.session_state.messages:
            role = "user" if isinstance(msg, HumanMessage) else "assistant"
            with st.chat_message(role):
                if isinstance(msg.content, list):
                    for part in msg.content:
                        if part["type"] == "text":
                            st.markdown(part["text"])
                        elif part["type"] == "image_url":
                            st.image(part["image_url"]["url"])
                else:
                    st.markdown(msg.content)

    # --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç† & LLM å‘¼ã³å‡ºã— ---
    final_prompt = None
    #if st.session_state.input_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
        #final_prompt = button_input if button_input is not None else prompt
    #elif st.session_state.input_method == "éŸ³å£°&ãƒ†ã‚­ã‚¹ãƒˆ":
    if st.session_state.input_method == "éŸ³å£°&ãƒ†ã‚­ã‚¹ãƒˆ":
        # éŸ³å£°å…¥åŠ›ãƒ«ãƒ¼ãƒ—
        status_indicator = st.empty()
        #amp_indicator = st.sidebar.empty() # éŸ³å£°æŒ¯å¹…è¡¨ç¤ºç”¨
        status_indicator.write("ğŸ¤ è©±ã—ã¦ãã ã•ã„...åˆã¯ä»¥ä¸‹ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        sound_chunk = pydub.AudioSegment.empty()
        #recognized_text = asyncio.run(process_audio_loop_with_silence_detection(
        recognized_text = await process_audio_loop_with_silence_detection( # â˜… asyncio.run ã‚’ await ã«å¤‰æ›´
            frames_deque_lock,
            frames_deque,
            sound_chunk,
            amp_indicator,
            status_indicator, # status_indicator ã‚’æ¸¡ã™
            button_input,
            prompt,
            memory_use,memory_alt,memory_ok,
        )
        if recognized_text:
            final_prompt = recognized_text
            status_indicator.write("âœ… éŸ³å£°ãƒ»ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚’èªè­˜ã—ã¾ã—ãŸã€‚å›ç­”ã¾ã§ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚") #éŸ³å£°èªè­˜å®Œäº†


    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
    current_memory_use(memory_use,memory_alt,memory_ok)
    mem_use = get_memory_usage()
    #print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡={mem_use:.0f}MB") #f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:{current_memory_usage:.0f}MB"

    if final_prompt:

        user_message_content = [{"type": "text", "text": final_prompt}]
        image_base64 = None
        image_type = None

        # ç”»åƒå‡¦ç† (uploaded_file)
        if uploaded_file is not None:
            image_base64, image_type = process_uploaded_image(uploaded_file)

        # ã‚«ãƒ¡ãƒ©ç”»åƒå‡¦ç† (webrtc_ctx)
        if uploaded_file is None and ("ç”»åƒ" in final_prompt or "ã‚«ãƒ¡ãƒ©" in final_prompt or "ç”»é¢" in final_prompt):
            cap = None
            #if webrtc_ctx and webrtc_ctx.video_transformer: # webrtc_ctx ãŒå­˜åœ¨ã—ã€transformer ãŒã‚ã‚‹ã‹ç¢ºèª
            if webrtc_ctx.video_transformer: 
                cap = webrtc_ctx.video_transformer.frame
            if cap is not None :
                is_success, buffer = cv2.imencode(".jpg", cap)
                if is_success:
                    image_base64 = base64.b64encode(buffer).decode('utf-8')
                    image_type = "image/jpeg"
                else:
                    st.warning("ã‚«ãƒ¡ãƒ©ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            else:
                st.warning("ã‚«ãƒ¡ãƒ©ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚WebRTCæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚") # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ”¹å–„

        if image_base64:
            image_data_url = f"data:{image_type};base64,{image_base64}"
            user_message_content.append(
                {"type": "image_url", "image_url": {"url": image_data_url}}
            )

        current_human_message = HumanMessage(content=user_message_content if image_base64 else final_prompt)
        st.session_state.messages.append(current_human_message)

        # --- Display user message immediately ---
        with history_container:
            with st.chat_message("user"):
                if isinstance(current_human_message.content, list):
                    for part in current_human_message.content:
                        if part["type"] == "text":
                            st.markdown(part["text"])
                        elif part["type"] == "image_url":
                            st.image(part["image_url"]["url"],width=100)
                else:
                    st.markdown(current_human_message.content)

        # --- LLM Call Preparation ---
        graph_instance = st.session_state.graph_or_agent
        is_react_agent = st.session_state.is_react
        use_enhanced_graph = st.session_state.use_enhanced_graph
        thread_id = st.session_state.thread_id
        config = {"configurable": {"thread_id": thread_id, "language": "Japanese"}}

        # --- Main Execution Flow ---
        if graph_instance:
            with history_container:
                with st.chat_message("assistant",avatar="ğŸ›¸"):
                    # --- éŸ³å£°å‡ºåŠ›ã®å ´åˆã€ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¯ streaming_text_speak å†…ã§å‡¦ç† ---
                    if st.session_state.output_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
                        message_placeholder = st.empty()
                    else:
                        # éŸ³å£°å‡ºåŠ›æ™‚ã¯ streaming_text_speak ãŒè¡¨ç¤ºã‚’æ‹…å½“
                        pass
                    full_response = ""
                    tool_calls_info = []
                    final_ai_message = None
                    exec_mode = "ReAct Agent" if is_react_agent else ("Enhanced Graph" if use_enhanced_graph else "LangGraph")
                    #st.info(f"å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: {exec_mode}") # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’è¡¨ç¤º
                    print(f"å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: {exec_mode}")
                    try:
                        # --- Streaming Execution ---
                        if is_react_agent:
                            with st.spinner("ReAct Agent å®Ÿè¡Œä¸­..."):
                                events = graph_instance.stream(
                                    {"messages": [current_human_message]},
                                    config=config,
                                    stream_mode="messages",
                                )
                                for chunk_list in events:
                                    if isinstance(chunk_list, list) and chunk_list:
                                        last_message = chunk_list[-1]
                                        if isinstance(last_message, AIMessage) and not getattr(last_message, 'tool_calls', None):
                                            # æ–°ã—ã„å®Œå…¨ãªå¿œç­”å†…å®¹ã‚’å–å¾—
                                            new_content = last_message.content
                                            # å¿œç­”å†…å®¹ãŒæ›´æ–°ã•ã‚ŒãŸå ´åˆã®ã¿å‡¦ç†
                                            if new_content != full_response:
                                                full_response = new_content
                                                # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰ãªã‚‰ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’æ›´æ–°
                                                if message_placeholder:
                                                    message_placeholder.markdown(full_response + "â–Œ")
                                            final_ai_message = last_message
                                        elif isinstance(last_message, AIMessage) and getattr(last_message, 'tool_calls', None):
                                            tool_calls_info.append(str(last_message.tool_calls))
                                if st.session_state.output_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
                                    message_placeholder.markdown(full_response)
                        else: # LangGraph or Enhanced Graph
                            spinner_text = "Enhanced Graph å®Ÿè¡Œä¸­..." if use_enhanced_graph else "LangGraph å®Ÿè¡Œä¸­..."
                            with st.spinner(spinner_text):
                                #print("current_human_message=",current_human_message)
                                events = graph_instance.stream(
                                    {"messages": [current_human_message]},
                                    config=config,
                                    stream_mode="values",
                                )
                                for event_data in events:
                                    current_graph_messages = event_data.get("messages", [])
                                    if current_graph_messages:
                                        last_graph_message = current_graph_messages[-1]
                                        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—æƒ…å ± (é€šå¸¸ã®LangGraphã®ã¿)
                                        if not use_enhanced_graph and isinstance(last_graph_message, AIMessage) and getattr(last_graph_message, 'tool_calls', None):
                                            tool_calls_info.append(str(getattr(last_graph_message, 'tool_calls')))
                                        # æœ€çµ‚å¿œç­”
                                        elif isinstance(last_graph_message, AIMessage) and last_graph_message.content and not getattr(last_graph_message, 'tool_calls', None):
                                            # æ–°ã—ã„å®Œå…¨ãªå¿œç­”å†…å®¹ã‚’å–å¾—
                                            new_content = last_graph_message.content
                                            # å¿œç­”å†…å®¹ãŒæ›´æ–°ã•ã‚ŒãŸå ´åˆã®ã¿å‡¦ç†
                                            if new_content != full_response:
                                                full_response = new_content
                                                # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰ãªã‚‰ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’æ›´æ–°
                                                if message_placeholder:
                                                    message_placeholder.markdown(full_response + "â–Œ")
                                            final_ai_message = last_graph_message
                            #if st.session_state.output_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
                        # --- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†å¾Œã®å‡¦ç† ---
                        # æœ€çµ‚çš„ãªå¿œç­”å†…å®¹ã§ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’æ›´æ–°ï¼ˆã‚«ãƒ¼ã‚½ãƒ«ãªã—ï¼‰
                        if message_placeholder:
                            message_placeholder.markdown(full_response)

                    except Exception as e:
                        st.error(f"å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                        error_msg = AIMessage(content=f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                        st.session_state.messages.append(error_msg)
                        if message_placeholder: # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãŒã‚ã‚Œã°æ›´æ–°
                            message_placeholder.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                        # éŸ³å£°å‡ºåŠ›ã®å ´åˆã®ã‚¨ãƒ©ãƒ¼å‡¦ç†ã¯ã“ã“ã«ã¯å«ã¾ã‚Œã¦ã„ãªã„ãŒã€å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 

                    # --- å¿œç­”ã‚’å±¥æ­´ã«è¿½åŠ  & éŸ³å£°å‡ºåŠ› ---
                    if final_ai_message:
                        st.session_state.messages.append(final_ai_message)
                        # --- éŸ³å£°å‡ºåŠ› ---
                        if st.session_state.output_method == "éŸ³å£°":
                            #await streaming_text_speak(full_response)
                            #asyncio.run(streaming_text_speak(full_response))
                            # await ã‚’å‰Šé™¤ã— asyncio.run() ã‚’ä½¿ç”¨
                            # éŸ³å£°å‡ºåŠ›æ™‚ã¯ full_response ã‚’ä½¿ã†
                            await streaming_text_speak(full_response)
                            #asyncio.run(streaming_text_speak(full_response)) # Streamlitã®ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã§éåŒæœŸé–¢æ•°ã‚’å®Ÿè¡Œã™ã‚‹å ´åˆ

                    else:
                        # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰æœ€çµ‚å¿œç­”ã‚’å–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                        st.warning("æœ€çµ‚å¿œç­”ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚çŠ¶æ…‹ã‚’ç¢ºèªã—ã¾ã™ã€‚")
                        try:
                            snapshot = graph_instance.get_state(config)
                            if snapshot and 'messages' in snapshot.values:
                                for msg in reversed(snapshot.values['messages']):
                                    if isinstance(msg, AIMessage) and not getattr(msg, 'tool_calls', None):
                                        final_ai_message = msg
                                        full_response = final_ai_message.content
                                        if message_placeholder: # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã®å ´åˆ
                                        #if st.session_state.output_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
                                            message_placeholder.markdown(full_response)
                                        st.session_state.messages.append(final_ai_message)
                                        st.info("çŠ¶æ…‹ã‹ã‚‰æœ€çµ‚å¿œç­”ã‚’å¾©å…ƒã—ã¾ã—ãŸã€‚")
                                        # --- éŸ³å£°å‡ºåŠ› ---
                                        if st.session_state.output_method == "éŸ³å£°":
                                            await streaming_text_speak(full_response)
                                            #asyncio.run(streaming_text_speak(full_response)) 
                                            # await ã‚’å‰Šé™¤ã— asyncio.run() ã‚’ä½¿ç”¨
                                            #await streaming_text_speak(full_response)
                                        break
                            if not final_ai_message:
                                #if st.session_state.output_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
                                if message_placeholder:
                                    message_placeholder.markdown("ã‚¨ãƒ©ãƒ¼ã¾ãŸã¯å¿œç­”ãªã—ã€‚")
                                st.session_state.messages.append(AIMessage(content="å¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"))
                        except Exception as state_e:
                            st.error(f"çŠ¶æ…‹ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {state_e}")
                            #if st.session_state.output_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
                            if message_placeholder:
                                message_placeholder.markdown("ã‚¨ãƒ©ãƒ¼ã¾ãŸã¯å¿œç­”ãªã—ã€‚")
                            st.session_state.messages.append(AIMessage(content="å¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"))

                    # --- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º ---
                    if final_ai_message:
                        usage_info = ""
                        if hasattr(final_ai_message, 'usage_metadata') and final_ai_message.usage_metadata:
                            meta = final_ai_message.usage_metadata
                            usage_info = f"å…¥åŠ›: {meta.get('input_tokens', 'N/A')} ãƒˆãƒ¼ã‚¯ãƒ³, å‡ºåŠ›: {meta.get('output_tokens', 'N/A')} ãƒˆãƒ¼ã‚¯ãƒ³, åˆè¨ˆ: {meta.get('total_tokens', 'N/A')} ãƒˆãƒ¼ã‚¯ãƒ³"
                        elif hasattr(final_ai_message, 'response_metadata') and final_ai_message.response_metadata:
                            meta = final_ai_message.response_metadata
                            token_count = meta.get('token_usage', meta.get('token_count', {}))
                            if isinstance(token_count, dict):
                                usage_info = f"å…¥åŠ›: {token_count.get('input_tokens', 'N/A')} ãƒˆãƒ¼ã‚¯ãƒ³, å‡ºåŠ›: {token_count.get('output_tokens', 'N/A')} ãƒˆãƒ¼ã‚¯ãƒ³, åˆè¨ˆ: {token_count.get('total_tokens', 'N/A')} ãƒˆãƒ¼ã‚¯ãƒ³"
                            else:
                                usage_info = f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {meta}"
                        if usage_info:
                            st.caption(f"ä½¿ç”¨çŠ¶æ³: {usage_info}")

                    if tool_calls_info:
                        with st.expander("ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—è©³ç´°", expanded=False):
                            for i, call in enumerate(tool_calls_info):
                                st.code(call, language='json')

                
        #st.rerun()
    else:
        st.error("ã‚°ãƒ©ãƒ•ã¾ãŸã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        with history_container:
            st.error("ã‚°ãƒ©ãƒ•ã¾ãŸã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    time.sleep(1) #ok 3ç§’
    st.rerun()
# --- main é–¢æ•°ã®å‘¼ã³å‡ºã— ---
if __name__ == "__main__":
    #main()
    asyncio.run(main()) # â˜… main é–¢æ•°ã‚’ asyncio.run ã§å®Ÿè¡Œ
    #await main()
    #st.rerun()