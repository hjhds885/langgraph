# streamlit_watcher_ignore_module: torch, torchaudio, torchvision
import streamlit as st
from streamlit_webrtc import WebRtcMode, webrtc_streamer, VideoTransformerBase
#ãƒ„ãƒ¼ãƒ«###################################################
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.agents import tool,Tool
from langchain.tools import Tool
#from langchain_community.agent_toolkits.load_tools import load_tools
from pydantic import BaseModel, Field
from metaphor_python import Metaphor
#from langchain_community.utilities import SerpAPIWrapper
from newsapi import NewsApiClient
#from langchain_community.utilities.google_search import GoogleSearchAPIWrapper
#from langchain_community.tools import WikipediaQueryRun
#from langchain_community.utilities import WikipediaAPIWrapper
from newsapi import NewsApiClient
from exa_py import Exa
from langchain_core.messages import HumanMessage,AIMessage
from langchain_core.utils.function_calling import convert_to_openai_tool
from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper
import requests
##########################################################
#from langchain_openai import ChatOpenAI
#from langchain_anthropic import ChatAnthropic
#from langchain_google_genai import ChatGoogleGenerativeAI
#from langchain_cohere.chat_models import ChatCohere
#from langchain_nvidia_ai_endpoints import ChatNVIDIA
#from langchain_ollama import ChatOllama
#from langchain_core.prompts import ChatPromptTemplate
#from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage,AIMessage 
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langchain.chat_models import init_chat_model
############################################################
import numpy as np
import threading
import time
from datetime import datetime, timedelta
from typing import Any, Dict, List,Tuple
import asyncio
from collections import deque
import cv2
import av
import base64
from gtts import gTTS
import whisper
import pydub
from pydub import AudioSegment
from pydub.effects import low_pass_filter  #,high_pass_filter
#from scipy.signal import resample
from io import BytesIO

import psutil
import gc
import re
import librosa
import subprocess
import vertexai
#import getpass
import os
#import json
import uuid
#from langchain_ibm import ChatWatsonx
#from databricks_langchain import ChatDatabricks
#from langchain_groq import ChatGroq
#from langchain_mistralai import ChatMistralAI
#from mistralai import Mistral
import tiktoken
import torch
import torchaudio
import torchvision
#####################################################
MODEL_PRICES = {
    "input": {
        "gpt-3.5-turbo": 0.5 / 1_000_000,
        "gpt-4o": 2.5 / 1_000_000,
        "gpt-4.5-preview": 75 / 1_000_000,
        "o1": 15 / 1_000_000,
        "gpt-3.5-turbo": 0.5 / 1_000_000,
        "gpt-3.5-turbo": 0.5 / 1_000_000,
        "gpt-4o-mini": 0.15 / 1_000_000,
        "o1-mini": 1.1 / 1_000_000,
        "o3-mini": 1.1 / 1_000_000,
        "claude-3-5-sonnet-20240620": 3 / 1_000_000,
        "gemini-1.5-pro-latest": 3.5 / 1_000_000,
        "command-r-plus":0,
        "llama-3.3-70b-versatile":0,
        "llama-3.2-90b-vision-preview":0,
        "phi-4-multimodal-instruct":0,
        "mistral-small-latest":0, 
    },
    "output": {
        "gpt-3.5-turbo": 1.5 / 1_000_000,
        "gpt-4o": 1.25 / 1_000_000,
        "gpt-4.5-preview": 37.5 / 1_000_000,
        "o1": 7.5 / 1_000_000,
        "gpt-4o-mini": 0.6 / 1_000_000,
        "o1-mini": 4.4 / 1_000_000,
        "o3-mini": 4.4 / 1_000_000,
        "claude-3-5-sonnet-20240620": 15 / 1_000_000,
        "gemini-1.5-pro-latest": 10.5 / 1_000_000,
        "command-r-plus":0,
        "llama-3.3-70b-versatile":0,
        "llama-3.2-90b-vision-preview":0,
        "phi-4-multimodal-instruct":0,
        "mistral-small-latest":0,
    }
}

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®šã‚’ã¾ã¨ã‚ã¦è¡Œã†é–¢æ•°
def setup_environment():
    try:
        # streamlit cloudã®ç’°å¢ƒå¤‰æ•°ã«APIã‚­ãƒ¼ãªã©ã‚’è¨­å®š (å¿…è¦ã§ã‚ã‚Œã°)
        os.environ["LANGCHAIN_API_KEY"] = st.secrets.key["LANGCHAIN_API_KEY"]
        os.environ["LANGSMITH_API_KEY"] = st.secrets.key["LANGSMITH_API_KEY"]
        os.environ["USER_AGENT"] = st.secrets.key["USER_AGENT"]
        os.environ["OPENAI_API_KEY"] = st.secrets.key["OPENAI_API_KEY"]
        os.environ["GOOGLE_API_KEY"] = st.secrets.key["GOOGLE_API_KEY"]
        os.environ["ANTHROPIC_API_KEY"] = st.secrets.key["ANTHROPIC_API_KEY"]
        os.environ["COHERE_API_KEY"] = st.secrets.key["COHERE_API_KEY"]
        os.environ["NVIDIA_API_KEY"] = st.secrets.key["NVIDIA_API_KEY"]
        os.environ["HUGGINGFACEHUB_API_TOKEN"] = st.secrets.key["HUGGINGFACEHUB_API_TOKEN"]
        os.environ["GROQ_API_KEY"] = st.secrets.key["GROQ_API_KEY"]
        os.environ["FIREWORKS_API_KEY"] = st.secrets.key["FIREWORKS_API_KEY"]
        os.environ["MISTRAL_API_KEY"] = st.secrets.key["MISTRAL_API_KEY"]
        os.environ["TOGETHER_API_KEY"] = st.secrets.key["TOGETHER_API_KEY"]
        os.environ["OPENWEATHERMAP_API_KEY"] = st.secrets.key["OPENWEATHERMAP_API_KEY"]
        os.environ["SEARCHAPI_API_KEY"] = st.secrets.key["SEARCHAPI_API_KEY"]
        os.environ["SERPAPI_API_KEY"] = st.secrets.key["SERPAPI_API_KEY"]
        os.environ["EXA_API_KEY"] = st.secrets.key["EXA_API_KEY"]
        os.environ["TAVILY_API_KEY"] = st.secrets.key["TAVILY_API_KEY"]
        os.environ["NEWSAPI_API_KEY"] = st.secrets.key["NEWSAPI_API_KEY"]
        os.environ["METAPHOR_API_KEY"] = st.secrets.key["METAPHOR_API_KEY"]
        os.environ["WOLFRAM_ALPHA_APPID"] = st.secrets.key["WOLFRAM_ALPHA_APPID"]
        os.environ["GOOGLE_CLOUD_PROJECT"] = st.secrets.key["GOOGLE_CLOUD_PROJECT"]
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ ID ã¨ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¨­å®š
        PROJECT_ID = st.secrets.key["GOOGLE_CLOUD_PROJECT"]
        LOCATION = "us-central1"
        vertexai.init(project=PROJECT_ID, location=LOCATION)
    except:
        print("streamlit cloudã§ã¯ãªããƒ­ãƒ¼ã‚«ãƒ«PCã§ã®èµ·å‹•")
        pass

# æ—¥ä»˜ã¨æ™‚åˆ»ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_current_datetime():
    now = datetime.now()
    return now.strftime("%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†")

# é–¢æ•°ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
def get_memory_usage():
    process = psutil.Process()
    mem_info = process.memory_info()
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’MBå˜ä½ã§è¿”ã™
    return mem_info.rss / (1024 * 1024)

def current_memory_use(i,memory_use,memory_alt,memory_ok):
    # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
    current_memory_usage = get_memory_usage()
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º

    #memory_use.metric("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)", f"{current_memory_usage:.2f}")
    memory_use.write(f"ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:\n\n    ãƒ«ãƒ¼ãƒ—{i}å›ç›®:{current_memory_usage:.0f}MB")
    #print("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)", f"{current_memory_usage:.2f}")
    # ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã‚’å®šç¾©
    MEMORY_LIMIT_MB = 2700  # 1GB
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ã‚’è¶…ãˆãŸå ´åˆã®è­¦å‘Š
    if current_memory_usage > MEMORY_LIMIT_MB:
        memory_alt.error(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ ({MEMORY_LIMIT_MB} MB) ã‚’è¶…ãˆã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ãã ã•ã„ã€‚")
        memory_ok.empty()
        #st.stop()
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶ç´„ ({MEMORY_LIMIT_MB} MB) ã‚’è¶…ãˆã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¦ãã ã•ã„ã€‚")
    else:
        memory_ok.success("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚")
        memory_alt.empty()
        #print("ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ­£å¸¸ç¯„å›²å†…ã§ã™ã€‚")

# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã¨ãƒªã‚¹ãƒˆã¸ã®æ ¼ç´ã‚’ã¾ã¨ã‚ã¦è¡Œã†é–¢æ•°

# ãƒ¢ãƒ‡ãƒ«å®šç¾©: ãƒ¢ãƒ‡ãƒ«åã¨åˆæœŸåŒ–ã«å¿…è¦ãªæƒ…å ±ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
# initialize_models é–¢æ•°ã®å¤–ã«ç§»å‹•ã—ã¦ main é–¢æ•°ã‹ã‚‰ã‚‚å‚ç…§å¯èƒ½ã«ã™ã‚‹
model_definitions = {
    "mistral-small-latest": {"provider": "mistralai", "temperature": 0, "cost_input": 0, "cost_cached_input": 0, "cost_output": 0, "input_max": 131000},
    "gemini-2.5-pro-exp-03-25": {"provider": "google_vertexai", "temperature": 0, "cost_input": 0, "cost_cached_input": 0, "cost_output": 0, "input_max": 1048576},
    "gemini_2.5_flash": {"provider": "google_vertexai", "model_id": "gemini-2.5-flash-preview-04-17", "temperature": 0, "cost_input": 0.15, "cost_cached_input": 0, "cost_output": 3.5, "input_max": 1048576},
    "gpt-4.1-mini": {"provider": "openai", "temperature": 0, "cost_input": 0.4, "cost_cached_input": 0.1, "cost_output": 1.6, "input_max": 1047576},
    "o4-mini": {"provider": "openai", "temperature": 0, "cost_input": 1.1, "cost_cached_input": 0.275, "cost_output": 4.4, "input_max": 200000},
    "gpt-4.1": {"provider": "openai", "temperature": 0, "cost_input": 2.0, "cost_cached_input": 0.5, "cost_output": 8.0, "input_max": 1047576},
    # "gpt-4o": {"provider": "openai", "temperature": 0, "cost_input": 2.5, "cost_cached_input": 1.25, "cost_output": 10, "input_max": 128000},
    # ... (ä»–ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å®šç¾©) ...
    "c4ai-aya-vision-32b": {"provider": "cohere", "temperature": 0, "cost_input": 0, "cost_cached_input": 0, "cost_output": 0, "input_max": 16000}, # ãƒ„ãƒ¼ãƒ«éå¯¾å¿œ
    # "meta/llama-4-maverick-17b-128e-instruct": {"provider": "nvidia", "temperature": 0, "cost_input": 0, "cost_cached_input": 0, "cost_output": 0, "input_max": 1000000}, # ãƒ„ãƒ¼ãƒ«éå¯¾å¿œ (NVIDIAç‰ˆ)
}

def initialize_models(selected_model_name: str):
    """é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã«åŸºã¥ã„ã¦ã€ãã®ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’åˆæœŸåŒ–ã™ã‚‹"""
    print(f"Initializing model: {selected_model_name}") # ã©ã®ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã‚‹ã‹ãƒ­ã‚°å‡ºåŠ›

    if selected_model_name not in model_definitions:
        raise ValueError(f"æœªå®šç¾©ã®ãƒ¢ãƒ‡ãƒ«åã§ã™: {selected_model_name}")

    model_info = model_definitions[selected_model_name]

    # init_chat_model ã«æ¸¡ã™å¼•æ•°ã‚’æº–å‚™
    init_args = {
        "model": f"{model_info['provider']}:{model_info.get('model_id', selected_model_name)}", # provider:model_id å½¢å¼
        "temperature": model_info.get("temperature", 0), # temperature ãŒæœªå®šç¾©ãªã‚‰ 0 ã‚’ä½¿ã†
        # å¿…è¦ã«å¿œã˜ã¦ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (max_tokens ãªã©) ã‚‚è¿½åŠ 
    }
    # model_provider å¼•æ•°ãŒå¿…è¦ãªå ´åˆ (init_chat_model ã®ä»•æ§˜ã«ã‚ˆã‚‹)
    # if model_info['provider'] in ["groq", "mistralai", "nvidia", "cohere"]: # ä¾‹
    #     init_args["model_provider"] = model_info['provider']
    #     init_args["model"] = model_info.get('model_id', selected_model_name) # model_id ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†

    # ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    try:
        llm_instance = init_chat_model(**init_args)
        print(f"Model {selected_model_name} initialized successfully.")
    except Exception as e:
        print(f"Error initializing model {selected_model_name}: {e}")
        st.error(f"ãƒ¢ãƒ‡ãƒ« '{selected_model_name}' ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None, 0, 0, 0, 0 # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ None ã¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™

    # ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¨é–¢é€£æƒ…å ±ã‚’è¿”ã™
    return (
        llm_instance,
        model_info.get("cost_input", 0),
        model_info.get("cost_cached_input", 0),
        model_info.get("cost_output", 0),
        model_info.get("input_max", 32000) # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    )


# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–¢æ•° (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ã)
@st.cache_resource(show_spinner=False) # ãƒ¢ãƒ‡ãƒ«é¸æŠã”ã¨ã«å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ spinner ã¯éè¡¨ç¤ºã«
def load_selected_model(model_name: str):
    """é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    return initialize_models(model_name)

def select_model():
    # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã€temperatureã‚’0ã‹ã‚‰2ã¾ã§ã®ç¯„å›²ã§é¸æŠå¯èƒ½ã«ã™ã‚‹
    # åˆæœŸå€¤ã¯0.0ã€åˆ»ã¿å¹…ã¯0.01ã¨ã™ã‚‹
    temperature = 0.0
    #models = ( "GPT-4o", "Claude 3.5 Sonnet", "Gemini 1.5 Pro")
    #model = st.sidebar.radio("å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ:", models)
    model = st.sidebar.selectbox(
        "LLMå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
        ["mistral-small-latest", "GPT-4o","gpt-4.5-preview","gpt-4o-mini","o1","o1-mini","o3-mini","llama-3.3-70b-versatile","llama-3.2-90b-vision","phi-4-multimodal-instruct","gemini-2.0-flash","Claude 3.5 Sonnet", "Gemini 1.5 Pro"]
    )
    #imgéå¯¾å¿œ o1-mini,o3-mini,deepseek-r1,neva-22b
    #Tooléå¯¾å¿œ gemma-3-27b-it,gemma-3-1b-it,
    #imgå¯¾å¿œ,Tooléå¯¾å¿œ pixtral-12b-2409
    if "mistral-small-latest" in model:
        st.session_state.model_name = "mistral-small-latest"
        return init_chat_model("mistral-small-latest", model_provider="mistralai")
    elif model == "gemini-2.5-pro-exp-03-25":
        st.session_state.model_name = "gemini-2.5-pro-exp-03-25"
        return init_chat_model("gemini-2.5-pro-exp-03-25")
    elif model == "gemini-2.5-flash":
        st.session_state.model_name = "gemini-2.5-flash"
        return init_chat_model("google_vertexai:gemini-2.5-flash", temperature=0)
    elif model == "gpt-4.1-mini":  #
        st.session_state.model_name = "gpt-4.1-mini"
        return init_chat_model("openai:gpt-4.1-mini")
    elif model == "GPT-4o":  #"gpt-4o 'gpt-4o-2024-08-06'" æœ‰æ–™ï¼Ÿã€Best
        st.session_state.model_name = "gpt-4o"
        return init_chat_model("openai:gpt-4o")
    elif model == "o1":  #
        st.session_state.model_name = "o1"
        return init_chat_model("openai:o1")
    elif model == "o1-mini":  #
        st.session_state.model_name = "o1-mini"
        return init_chat_model("openai:o1-mini")
    elif model == "o3-mini":  #
        st.session_state.model_name = "o3-mini"
        return init_chat_model("openai:o3-mini")
    elif model == "Claude 3.5 Sonnet": #ã‚³ãƒ¼ãƒ‰ãŒGoodï¼ï¼
        st.session_state.model_name = "claude-3-5-sonnet-20240620"
        return init_chat_model("anthropic:claude-3-5-sonnet-latest", temperature=0)
    elif model == "Gemini 1.5 Pro":
        st.session_state.model_name = "gemini-1.5-pro-latest"
        return init_chat_model("google_vertexai:gemini-1.5-pro-latest")
    
    elif model == "pixtral-12b":
        st.session_state.model_name = "pixtral-12b-2409"
        return init_chat_model("pixtral-12b-2409", model_provider="mistralai")
    elif model == "llama-3.2-90b-vision":
        st.session_state.model_name = "llama-3.2-90b-vision-preview"
        return init_chat_model("llama-3.2-90b-vision-preview", model_provider="groq")
    elif model == "llama-3.3-70b-versatile":
        st.session_state.model_name = "llama-3.3-70b-versatile"
        return init_chat_model("llama-3.3-70b-versatile", model_provider="groq")
    elif model == "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo":
        st.session_state.model_name = "meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo"
        return init_chat_model("meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo", model_provider="together")
############################################################################

############################################################################
def get_message_counts(text):
    if "gemini" in st.session_state.model_name:
        return st.session_state.llm.get_num_tokens(text)
    else:
        # Claude 3 ã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å…¬é–‹ã—ã¦ã„ãªã„ã®ã§ã€tiktoken ã‚’ä½¿ã£ã¦ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        # ã“ã‚Œã¯æ­£ç¢ºãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§ã¯ãªã„ãŒã€å¤§ä½“ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹
        phrases = (
                    #"gpt", 
                    "o4", #"o3", 'Could not automatically map o3-mini to a tokeniser
                    #"o1", #'Could not automatically map o1 to a tokeniser. 
                    #Please use `tiktoken.get_encoding` to explicitly get the tokeniser you expect.'
                    )
        if any(phrase in st.session_state.model_name for phrase in phrases):
        #if "gpt" in st.session_state.model_name:
            encoding = tiktoken.encoding_for_model(st.session_state.model_name)
        else:
            encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")  # ä»®ã®ã‚‚ã®ã‚’åˆ©ç”¨
        return len(encoding.encode(text))

#ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚«ã‚¦ãƒ³ãƒˆ
def count_tokens(messages, model_name="gpt-3.5-turbo"):
    #ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦é©åˆ‡ãªencoding_modelã‚’ä½¿ç”¨
    encoding = tiktoken.encoding_for_model(model_name)
    # messages ãŒæ–‡å­—åˆ—ã§ã‚ã‚Œã°ç›´æ¥ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ 
    if isinstance(messages, str):
        n = len(encoding.encode(messages)) 
        print("messages ãŒæ–‡å­—åˆ—ã®å ´åˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°len(encoding.encode(messages))=",n)  
        return n

    # messages ãŒãƒªã‚¹ãƒˆãªã©ã®å ´åˆã¯å€‹åˆ¥ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ï¼ˆå¿…è¦ã«å¿œã˜ã¦å‡¦ç†ï¼‰ 
    total = 0 
    print("messages=\n",messages)
    #messages=
    #system: ã‚ãªãŸã¯ãƒ„ãƒ¼ãƒ«ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
            #ç¾åœ¨ã®æ—¥æ™‚ã¯ 2025å¹´03æœˆ30æ—¥ 16æ™‚50åˆ† ã§ã™ã€‚
            #æä¾›ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¢ã—ã¦ã„ã‚‹æœ€æ–°ã®æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
            #å¿…è¦ãªæƒ…å ±ãŒå¾—ã‚‰ã‚Œãªã„å ´åˆã¯ã€ä»–ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
            #ç‰¹ã«ã€å¤©æ°—ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’èã‹ã‚ŒãŸå ´åˆã¯ã€å¿…ãšãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
            #ã¾ãŸã€å¸¸ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
    #user: ç§ã¯ã€çŸ³å·çœŒå°æ¾å¸‚ã«ä½ã‚“ã§ã„ã¾ã™ã€‚ãƒ„ãƒ¼ãƒ«ã‚’åˆ©ç”¨ã—ã¦æœ€æ–°æƒ…å ±ã‹ã‚‰å…·ä½“çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ã—ã¦æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
    #ai: æœ€æ–°ã®æƒ…å ±ã¨ã—ã¦ã€ã€Œ3æœˆ13æ—¥ï¼ˆæœ¨ï¼‰çŸ³å·çœŒå°æ¾å¸‚ã€ã‚¢ãƒŸãƒ¥ãƒ¼ã‚¸ã‚¢ãƒ å°æ¾åº—ã€ãŒç§»è»¢ã‚ªãƒ¼ãƒ—ãƒ³ï¼ã€ã¨ã„ã†ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã™ã€‚è©³ã—ãã¯ã€[ã“ã¡ã‚‰](https://prtimes.jp/main/htm

    for m in messages:
        print("m=\n",m)
        #m= s
        #total += len(encoding.encode(m["content"]))
        #ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€Œstring indices must be integers, not 'str'ã€ã¯ã€
        # count_tokens() é–¢æ•°å†…ã§å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦æƒ³å®šã—ã¦ã„ã‚‹ m ãŒã€
        # å®Ÿéš›ã¯è¾æ›¸ã§ã¯ãªãæ–‡å­—åˆ—ã§ã‚ã‚‹ãŸã‚ç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚
        # m ãŒè¾æ›¸ã®å ´åˆã¯ "content" ã‚­ãƒ¼ã‚’å‚ç…§ã€æ–‡å­—åˆ—ã®å ´åˆã¯ãã®ã¾ã¾æ‰±ã†
        if isinstance(m, dict):
            content = m.get("content", "")
        elif isinstance(m, str):
            content = m
        else:
            content = str(m)
        print("content=\n",content)    
        n = len(encoding.encode(content)) 
        total += n
        print("messages ãŒãƒªã‚¹ãƒˆãªã©ã®å ´åˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°=",total) 
        return total

#ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ãŒé–¾å€¤ã‚’è¶…ãˆãŸã¨ãã«ã€éå»ã®ä¼šè©±å±¥æ­´ã‚’è¦ç´„ãƒ»å‰Šæ¸›ã™ã‚‹é–¢æ•°
def trim_messages(messages, max_tokens_threshold=120000):
    #messages ãŒæ–‡å­—åˆ—ã®å ´åˆã«é©åˆ‡ãªãƒˆãƒªãƒ å‡¦ç†ã‚’å®Ÿæ–½
    # ã“ã“ã§ã¯ä¾‹ã¨ã—ã¦ã€å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å˜ç´”ã«å‰Šé™¤ã—ã€æœ€æ–°ã®50è¡Œã€50ä»¶ã®ã¿æ®‹ã™æ–¹æ³•ã‚’ã¨ã‚‹ 
    # â€»ã‚‚ã—ãã¯ã€è¦ç´„LLMã‚’å‘¼ã³å‡ºã—ã¦ã¾ã¨ã‚ç›´ã™æ–¹æ³•ã‚‚ã‚ã‚Šã¾ã™  
    token_count = 0
    token_count = count_tokens(messages)
    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒé–¾å€¤å†…ãªã‚‰ãã®ã¾ã¾è¿”ã™  
    if token_count < max_tokens_threshold: 
        print("token_count=",token_count )
        return messages
    # ã“ã“ã§ã¯ä¾‹ã¨ã—ã¦ã€æ–‡å­—åˆ—ã®å ´åˆã¯å…ˆé ­éƒ¨åˆ†ã‚’å‰Šé™¤ã—ã¦æœ€æ–°éƒ¨åˆ†ã®ã¿æ®‹ã™æ–¹æ³•ã‚’æ¡ç”¨ 
    if isinstance(messages, str): 
        # è¤‡æ•°è¡Œã«åˆ†å‰²ã—ã¦æœ€æ–°ã®50è¡Œã ã‘ã‚’æ¡ç”¨ã™ã‚‹ä¾‹
        lines = messages.splitlines()
        trimmed = "\n".join(lines[-50:])
        return trimmed
    # ãƒªã‚¹ãƒˆã®å ´åˆã¯æœ€æ–°ã®50é …ç›®ã ã‘ã‚’æ®‹ã™ï¼ˆä¾‹ï¼‰ 
    print("ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯æ®‹ã£ã¦ã„ã‚‹ã‹ï¼Ÿmessages[-50:]=\n",messages[-50:])
    return messages[-50:]
    
    # return messages[-50:]
    #else:
    #print("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ãŒé–¾å€¤ã‚’è¶…ãˆãŸã®ã§ã€éå»ã®ä¼šè©±å±¥æ­´ã‚’è¦ç´„ãƒ»å‰Šæ¸›ã—ã¾ã—ãŸã€‚")

def trim_message_history(message_history, max_tokens=8192):
    """
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§åˆ¶é™
    GPT-4
    GPT-4: 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    GPT-4 Turbo: 128,000ãƒˆãƒ¼ã‚¯ãƒ³
    GPT-4o: 128,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude
    Claude 3 Haiku: ç´„200,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude 3 Sonnet: ç´„200,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude 3 Opus: ç´„200,000ãƒˆãƒ¼ã‚¯ãƒ³
    Claude 2: 100,000ãƒˆãƒ¼ã‚¯ãƒ³
    Gemini
    Gemini Pro: 32,000ãƒˆãƒ¼ã‚¯ãƒ³
    Gemini Ultra: æœ€å¤§1,000,000ãƒˆãƒ¼ã‚¯ãƒ³
    Llama 2/3
    Llama 2 (7B-70B): 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    Llama 3 (8B): 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    Llama 3 (70B): 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«
    Rinna: 2,048ãƒˆãƒ¼ã‚¯ãƒ³
    ELYZA: 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    Nekomata: 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    ãã®ä»–
    Command R+: 128,000ãƒˆãƒ¼ã‚¯ãƒ³
    Mistral 7B: 8,192ãƒˆãƒ¼ã‚¯ãƒ³
    Cohere: 4,096ãƒˆãƒ¼ã‚¯ãƒ³
    æ¨å¥¨ã•ã‚Œã‚‹ä¸€èˆ¬çš„ãªæˆ¦ç•¥:

    å®‰å…¨ã‚µã‚¤ã‚º: 4,000-8,000ãƒˆãƒ¼ã‚¯ãƒ³
    ãƒˆãƒªãƒŸãƒ³ã‚°é–¢æ•°ã®å®Ÿè£…
    ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®åˆ¶é™ã‚’ç¢ºèª

    """  
    total_tokens = 0  
    trimmed_history = []  
    
    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰é€†é †ã«è¿½åŠ   
    for message in reversed(message_history):  
        message_tokens = len(message[1])  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·ã•ã‚’è¨ˆç®—  
        if total_tokens + message_tokens <= max_tokens:  
            trimmed_history.insert(0, message)  
            total_tokens += message_tokens  
        else:  
            break  
    
    return trimmed_history  

def calc_and_display_costs():
    output_count = 0
    input_count = 0
    for role, message in st.session_state.message_history:
        # tiktoken ã§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        token_count = get_message_counts(message)
        if role == "ai":
            output_count += token_count
        else:
            input_count += token_count

    # åˆæœŸçŠ¶æ…‹ã§ System Message ã®ã¿ãŒå±¥æ­´ã«å…¥ã£ã¦ã„ã‚‹å ´åˆã¯ã¾ã APIã‚³ãƒ¼ãƒ«ãŒè¡Œã‚ã‚Œã¦ã„ãªã„
    if len(st.session_state.message_history) == 1:
        return

    input_cost = MODEL_PRICES['input'][st.session_state.model_name] * input_count
    output_cost = MODEL_PRICES['output'][st.session_state.model_name] * output_count
    if "gemini" in st.session_state.model_name and (input_count + output_count) > 128000:
        input_cost *= 2
        output_cost *= 2

    cost = output_cost + input_cost

    st.sidebar.markdown("## Costs")
    st.sidebar.markdown(f"**Total cost: ${cost:.5f}**")
    st.sidebar.markdown(f"- Input cost: ${input_cost:.5f}")
    st.sidebar.markdown(f"- Input tokens: {input_count}")
    st.sidebar.markdown(f"- Output cost: ${output_cost:.5f}")
    st.sidebar.markdown(f"- Output tokens: {output_count}")

#  LLMå•ç­”é–¢æ•°
async def query_llm(user_input,frame):
    with st.chat_message("ai"):
        use_tool_placeholder = st.empty()
        response_placeholder = st.empty()
    if 'use_tool_name' not in st.session_state:
        st.session_state.use_tool_name =""
    tools,openai_tools = setup_tools()  #openai_tools #setup_tools()
    tool_used = False
    tool_count = 0
    #config = {"configurable": {"thread_id": st.session_state.history_id}} #abc123
    config = {"configurable": {"thread_id": "thread-1"}}
    #langchainå¯¾å¿œLLM
    print("model_name=",st.session_state.model_name)
    if "pixtral" in st.session_state.model_name:
        tools =openai_tools
    memory = MemorySaver()
    agent_executor = create_react_agent(st.session_state.llm, tools, checkpointer=memory)
    #from langgraph.checkpoint.memory import MemorySaver
    ######################################################################
    #ç¢ºèªç”¨
    graph = agent_executor
    #graph = create_react_agent(st.session_state.llm, tools, checkpointer=MemorySaver())
    #config = {"configurable": {"thread_id": "thread-1"}}
    def print_stream(graph, inputs, config):
        for s in graph.stream(inputs, config, stream_mode="values"):
            message = s["messages"][-1]
            if isinstance(message, tuple):
                print(message)
            else:
                message.pretty_print()
    #####################################################################
    #('user', "What's the weather in SF?")
    #================================== Ai Message ==================================
    #Tool Calls:
    #check_weather (call_ChndaktJxpr6EMPEB5JfOFYc)
    #Call ID: call_ChndaktJxpr6EMPEB5JfOFYc
    #Args:
    #    location: San Francisco
    #================================= Tool Message =================================
    #Name: check_weather
    #It's always sunny in San Francisco
    #================================== Ai Message ==================================
    #The weather in San Francisco is sunny. Enjoy your day!
    #================================ Human Message =================================
    #Cool, so then should i go biking today?
    #================================== Ai Message ==================================
    #Since the weather in San Francisco is sunny, it sounds like a great day for biking! Enjoy your ride!


    # ã‚‚ã—ã¾ã  session_state ã« message_history ãŒãªã„å ´åˆã€ç©ºã®ãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–
    #if 'message_history' not in st.session_state:
        #st.session_state.message_history = []
    # ä¼šè©±å±¥æ­´ã‚’ LLM ã«é€ä¿¡ã™ã‚‹ãŸã‚ã«æº–å‚™
    # ä¼šè©±å±¥æ­´ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦é€£çµ
    #conversation_history = ""
    #for role, content in st.session_state.message_history:
        #conversation_history += f"{role}: {content}\n"
    #print("conversation_history=",conversation_history)
     # count_tokens() é–¢æ•°å†…ã§å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦æƒ³å®šã—ã¦ã„ã‚‹ m ãŒã€
        # å®Ÿéš›ã¯è¾æ›¸ã§ã¯ãªãæ–‡å­—åˆ—ã§ã‚ã‚‹ãŸã‚ç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚
        # m ãŒè¾æ›¸ã®å ´åˆã¯ "content" ã‚­ãƒ¼ã‚’å‚ç…§ã€æ–‡å­—åˆ—ã®å ´åˆã¯ãã®ã¾ã¾æ‰±ã†
        #if isinstance(m, dict):
            #content = m.get("content", "")
        #elif isinstance(m, str):
            #content = m
        #else:
            #content = str(m)
    # LLMã¸é€ä¿¡
    # response = call_llm(llm_input)  # ã“ã“ã§LLMã¸ã®å•ã„åˆã‚ã›ã‚’è¡Œã†é–¢æ•°ã‚’å‘¼ã³å‡ºã™
    #print(llm_input)  # ã“ã“ã§ç¢ºèªã®ãŸã‚ã€LLMã¸ã®å…¥åŠ›ã‚’è¡¨ç¤º
    ###############################################################

    if  "ç”»åƒ" in user_input and st.session_state.input_img == "æœ‰": #and st.session_state.model_name == "gpt-4o":
        user_input = user_input + "æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"
        # LLMã¸ã®å•ã„åˆã‚ã›ã«ä¼šè©±å±¥æ­´ã‚’å«ã‚ã‚‹
        #llm_input = conversation_history.strip() # ä¸è¦ãªæœ«å°¾ã®æ”¹è¡Œã‚’å‰Šé™¤
        #llm_input = conversation_history.strip() + f"user: {user_input}"
        llm_input = f"user: {user_input}"
        llm_input = trim_messages(llm_input, max_tokens_threshold=120000)
        encoded_image = cv2.imencode('.jpg', frame)[1]
        # ç”»åƒã‚’Base64ã«å¤‰æ›
        base64_image = base64.b64encode(encoded_image).decode('utf-8')
        message = HumanMessage(
                            content=[
                                {"type": "text", "text": llm_input},  #user_input
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                                },
                            ],
                        )
    else:
        user_input = user_input + "ãƒ„ãƒ¼ãƒ«ã‚’åˆ©ç”¨ã—ã¦æœ€æ–°æƒ…å ±ã‹ã‚‰å…·ä½“çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ã—ã¦æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"
        # LLMã¸ã®å•ã„åˆã‚ã›ã«ä¼šè©±å±¥æ­´ã‚’å«ã‚ã‚‹
        #llm_input = conversation_history.strip() # ä¸è¦ãªæœ«å°¾ã®æ”¹è¡Œã‚’å‰Šé™¤
        #llm_input = conversation_history.strip() + f"user: {user_input}"
        llm_input = f"user: {user_input}"
        llm_input = trim_messages(llm_input, max_tokens_threshold=120000)
        message = llm_input #HumanMessage(content=user_input)
    #########################################################################
    # ä¾‹ï¼šmemoryã‹ã‚‰æ—¢å­˜ã®ä¼šè©±å±¥æ­´ã‚’å–å¾—ã—ã€ãƒ¦ãƒ¼ã‚¶å…¥åŠ›ã‚„ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã™ã‚‹ 
    # messages = memory.get_messages() 
    # æ—¢å­˜ã®ä¼šè©±å±¥æ­´ï¼ˆãƒªã‚¹ãƒˆå½¢å¼ï¼šå„è¦ç´ ã¯ {"role":..., "content":...}ï¼‰ 
    # å¿…è¦ã«å¿œã˜ã¦ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®å…¥åŠ›ã‚„ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ  
    # messages.append({"role": "user", "content": cleaned_text}) 
    # messages.append({"role": "system", "content": cap})
    #ã“ã“ã§ã‚‚ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãƒˆãƒªãƒŸãƒ³ã‚°ãŒå¿…è¦ã‹ã‚‚
    print("LLMå…¥åŠ›ç›´å‰ãƒˆãƒ¼ã‚¯ãƒ³æ•°",count_tokens(message))
    # ã‚‚ã—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤§ãããªã£ã¦ã„ãŸã‚‰ã€ãƒˆãƒªãƒ ã™ã‚‹
    #message = trim_messages(message, max_tokens_threshold=120000)
    # ãã®å¾Œã€agent_executor ã«æ¸¡ã™
    #########################################################################
    try:
        if st.session_state.output_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
            full_response = ""
            command = f"chcp 65001"
            subprocess.run(command, shell=True, capture_output=True, text=True, encoding='utf-8')
            #ç¢ºèªã€ãƒã‚§ãƒƒã‚¯ç”¨
            #inputs = {"messages": [("user", "ã“ã‚“ã«ã¡ã¯ã€‚ç§ã®åå‰ã¯èª ã§ã™ã€‚çŸ³å·çœŒå°æ¾å¸‚ã«ä½ã‚“ã§ã„ã¾ã™ã€‚")]} #[message]},#
            #inputs = {"messages": [( message)]}
            #print_stream(graph, inputs, config)
            #inputs2 = {"messages": [("user", "ç§ã®åå‰ã¯ä½•?")]}
            #print_stream(graph, inputs2, config)
            #inputs3 = {"messages": [("user", "è²´æ–¹ã«é€ã£ãŸæœ€åˆã®è¨€è‘‰ã¯ï¼Ÿ")]}
            #print_stream(graph, inputs3, config)
            #inputs4 = {"messages": [("user", "ç§ãŒä½ã‚“ã§ã„ã‚‹ã¨ã“ã‚ã®æ˜æ—¥ã®å¤©æ°—ã¯ï¼Ÿ")]}
            #print_stream(graph, inputs4, config)

            for step, metadata in agent_executor.stream(
            #for step in agent_executor.stream(
                {"messages": [message]},
                config,
                stream_mode="messages", #"values", #
                ):
                #use_tool_placeholder.write(f"use_tool_name:{st.session_state.use_tool_name}")
                #print(step)
                #text = step["messages"][-1].content
                #print(step["messages"][-1].content)
                
                #for msg in step["messages"]:
                    #if isinstance(msg, AIMessage) and "tool_calls" in msg.additional_kwargs:
                        #for tool_call in msg.additional_kwargs["tool_calls"]:
                            #if tool_call["function"]["name"] != "":
                                #tool_name =tool_call["function"]["name"]
                                #tool_used = True
                                #tool_count += 1
                                #print("use_tool_name=",tool_name)
                                #use_tool_placeholder.write(f"use_tool_name:{tool_name}")
                                #break  # ä¸€åº¦è¦‹ã¤ã‘ã‚Œã°ååˆ†ãªã®ã§ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                if tool_used:
                    break
                if metadata["langgraph_node"] == "agent" and (text := step.text()):
                    full_response += text
                    response_placeholder.markdown(f"{full_response}") #å¿œç­”ã®è¡¨ç¤º
                    response_placeholder.write(full_response) #ok
                    print(text, end="") #OK
            response = full_response

        else:
            response = agent_executor.invoke(
                {"messages": [message]},
                config,
                stream_mode="messages",
            )
            
            #speak(response)   #st.audio ok
            await streaming_text_speak(response)
        ####################################################################
        #print("use_tool_name:",st.session_state.use_tool_name)
        #st.write("use_tool_name:",st.session_state.use_tool_name)
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        #st.session_state.message_history.append(("user", user_input))
        #st.session_state.message_history.append(("ai", response))
        #å¤šãã®LLMã«ã¯å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®åˆ¶é™ãŒã‚ã‚‹
        #å±¥æ­´ãŒé•·ã™ãã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ãŒå…¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã§ããªããªã‚‹
        #st.session_state.message_history = trim_message_history(st.session_state.message_history)
        print("\nhistory_id=",st.session_state.history_id)
        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼šMemorySaver ã«è“„ç©ã•ã‚Œã¦ã„ã‚‹ä¼šè©±å±¥æ­´ã‚’è¡¨ç¤ºã™ã‚‹å ´åˆ
        if hasattr(memory, "history"):
            st.markdown("### ä¼šè©±å±¥æ­´")
            for turn in memory.history:
                # turn ã®å½¢å¼ã¯ memory å®Ÿè£…ã«ä¾å­˜ã—ã¾ã™
                st.write(turn)
        print(response.usage_metadata) #{'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}
        # ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—ã—ã¦è¡¨ç¤º
        calc_and_display_costs()
        return response
    except StopIteration:
        # StopIterationã®å‡¦ç†
        print("StopIterationãŒç™ºç”Ÿ")
        pass
    except Exception as e:
        print("ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
        #init_messages()
        #st.error("ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", icon="ğŸš¨")
        #st.session_state.message_history = [("system",SYSTEM_MESSAGE)]
        #history_id += 1
        #st.session_state.history_id = f"abc{history_id}"

        return f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚: {e}"
    
    user_input = ""
    base64_image = ""
    frame = ""
###########################################################################
###########################################################################

#éŸ³å£°å‡ºåŠ›é–¢æ•°
async def streaming_text_speak(llm_response):
    # æœ«å°¾ã®ç©ºç™½ã®æ•°ã‚’ç¢ºèª
    #trailing_spaces = len(llm_response) - len(llm_response.rstrip())
    #print(f"æœ«å°¾ã®ç©ºç™½ã®æ•°: {trailing_spaces}")
    # æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    #cleaned_response = llm_response.rstrip()
    #print(f"ç©ºç™½ã‚’é™¤å»ã—ãŸæ–‡å­—åˆ—: '{cleaned_response}'")
    # å¥èª­ç‚¹ã‚„ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŸºæº–ã«åˆ†å‰²
    #å¾©å¸°æ–‡å­—ï¼ˆ\rï¼‰ã¯ã€**ã‚­ãƒ£ãƒªãƒƒã‚¸ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆCarriage Returnï¼‰**ã¨å‘¼ã°ã‚Œã‚‹ç‰¹æ®Šæ–‡å­—ã§ã€
    # ASCIIã‚³ãƒ¼ãƒ‰13ï¼ˆ10é€²æ•°ï¼‰ã«å¯¾å¿œã—ã¾ã™ã€‚ä¸»ã«æ”¹è¡Œã®ä¸€éƒ¨ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹åˆ¶å¾¡æ–‡å­—ã§ã™ã€‚
    split_response = re.split(r'([\r\n!-;=:ã€ã€‚ \?]+)', llm_response) 
    #split_response = re.split(r'([;:ã€ã€‚ ]+ğŸ˜ŠğŸŒŸğŸš€ğŸ‰)', llm_response)  #?ã¯ãªãã¦ã‚‚OK
    split_response = [segment for segment in split_response if segment.strip()]  # ç©ºè¦ç´ ã‚’å‰Šé™¤
    print(split_response)
    # AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
    with st.chat_message("ai"):
        response_placeholder = st.empty()
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã¨éŸ³å£°å‡ºåŠ›å‡¦ç†
        partial_text = ""
        for segment in split_response:
            if segment.strip():  # ç©ºæ–‡å­—åˆ—ã§ãªã„å ´åˆã®ã¿å‡¦ç†
                partial_text += segment
                response_placeholder.markdown(f"**{partial_text}**")  # å¿œç­”ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
                # gTTSã§éŸ³å£°ç”Ÿæˆï¼ˆéƒ¨åˆ†ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                try:
                    # ã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ã‚„ãã®ä»–ã®ç™ºéŸ³ã«ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
                    cleaned_segment = re.sub(r'[\*#*!-]', '', segment)
                    tts = gTTS(cleaned_segment, lang="ja")  # éŸ³å£°åŒ–
                    audio_buffer = BytesIO()
                    tts.write_to_fp(audio_buffer)  # ãƒãƒƒãƒ•ã‚¡ã«æ›¸ãè¾¼ã¿
                    audio_buffer.seek(0)

                    # pydubã§å†ç”Ÿé€Ÿåº¦ã‚’å¤‰æ›´
                    audio = AudioSegment.from_file(audio_buffer, format="mp3")
                    audio = audio._spawn(audio.raw_data, overrides={
                        "frame_rate": int(audio.frame_rate * 1.3)  # 1.5å€é€Ÿ
                    }).set_frame_rate(audio.frame_rate)
                    audio_buffer.close()

                    # éŸ³è³ªèª¿æ•´
                    audio = audio.set_frame_rate(44100)  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
                    audio = audio + 5  # éŸ³é‡ã‚’5dBå¢—åŠ 
                    audio = audio.fade_in(500).fade_out(500)  # ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ã‚¢ã‚¦ãƒˆ
                    #audio = audio.low_pass_filter(3000)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                    audio = low_pass_filter(audio, cutoff=900)  # é«˜éŸ³åŸŸã‚’ã‚«ãƒƒãƒˆ
                    # ãƒ™ãƒ¼ã‚¹ãƒ–ãƒ¼ã‚¹ãƒˆï¼ˆä½éŸ³åŸŸã‚’å¼·èª¿ï¼‰
                    low_boost = low_pass_filter(audio,1000).apply_gain(10)
                    audio = audio.overlay(low_boost)

                    # ãƒãƒƒãƒ•ã‚¡ã«å†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                    output_buffer = BytesIO()
                    audio.export(output_buffer, format="mp3")
                    output_buffer.seek(0)

                    # éŸ³å£°ã®å†ç”Ÿ
                    # ãƒã‚§ãƒƒã‚¯ã™ã‚‹æ–‡å­—åˆ—
                    if re.search(r"\n\n", segment):
                        print("æ–‡å­—åˆ—ã« '\\n\\n' ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
                        #time.sleep(1) 
                    #else:
                        #print("æ–‡å­—åˆ—ã« '\\n\\n' ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                    #st.audio(audio_buffer, format="audio/mp3",autoplay = True)
                    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’Base64ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                    audio_base64 = base64.b64encode(output_buffer.read()).decode()
                    audio_buffer.close()  # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒ­ãƒ¼ã‚º
                    a=len(audio_base64)
                    #print(a)
                    # HTMLã‚¿ã‚°ã§éŸ³å£°ã‚’è‡ªå‹•å†ç”Ÿï¼ˆãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼éè¡¨ç¤ºã€å†ç”Ÿé€Ÿåº¦èª¿æ•´ï¼‰
                    audio_html = f"""
                        <audio id="audio-player" autoplay style="display:none;">
                        <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
                        </audio>
                    """
                    st.markdown(audio_html, unsafe_allow_html=True)

                except Exception as e:
                    #print(f"éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                    pass
                try:
                    time.sleep(a*0.00004)  # ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€Ÿåº¦ã«åŒæœŸ
                except Exception as e:
                  time.sleep(2) 


# Toolã®è¨­å®šã‚’ã¾ã¨ã‚ã¦è¡Œã†é–¢æ•°
def setup_tools():
    ######################################################
    #tools ãƒªã‚¹ãƒˆã¯ã€langchain ã® Tool ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
    # åˆ©ç”¨ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã®å®šç¾©
    # ã‚«ã‚¹ã‚¿ãƒ ã®æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã—ãŸã„å ´åˆã¯Tool()ã‚’ä½¿ç”¨ã™ã‚‹
    #openai_tools ãƒªã‚¹ãƒˆã¯ã€convert_to_openai_tool é–¢æ•°ã«ã‚ˆã£ã¦
    # OpenAI ã®ãƒ„ãƒ¼ãƒ«å½¢å¼ã«å¤‰æ›ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«
    ######################################################
    class GetWeather(BaseModel):
        '''Get the current weather in a given location'''

        location: str = Field(
            ..., description="The city and state, e.g. Komatsu Ishikawa, Japan"
        )
    @tool(args_schema=GetWeather)
    def get_weather(location: str) -> str:
        """Get the current and future weather in a given location."""
        print(f"\nå¤©æ°—æƒ…å ±ã‚’å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«(get_weather)ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚")
        st.session_state.use_tool_name="get_weather"
        print("use_tool_name:",st.session_state.use_tool_name)
        st.write("use_tool_name:get_weather")
        # OpenWeatherMap APIã‚­ãƒ¼ã‚’è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
        api_key = os.environ["OPENWEATHERMAP_API_KEY"] #os.environ.get("OPENWEATHERMAP_API_KEY")
        if not api_key:
            return "Error: OPENWEATHERMAP_API_KEY environment variable not set."

        # ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’éƒ½å¸‚åã¨å›½ã‚³ãƒ¼ãƒ‰ã«åˆ†å‰²ï¼ˆä¾‹ï¼šKomatsu Ishikawa, JPï¼‰
        location_parts = location.split(",")
        city_name = location_parts[0].strip()
        country_code = location_parts[1].strip() if len(location_parts) > 1 else ""

        # APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        base_url = "http://api.openweathermap.org/data/2.5/forecast" #ä¿®æ­£
        params = {
            "q": f"{city_name},{country_code}",
            "appid": api_key,
            "units": "metric",  # æ‘‚æ°ã§å–å¾—
            "lang": "ja" #æ—¥æœ¬èªã§å–å¾—
        }

        try:
            # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
            response = requests.get(base_url, params=params)
            response.raise_for_status()  # ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯

            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ
            weather_data = response.json()

            # å¤©æ°—æƒ…å ±ã‚’æŠ½å‡º
            # æ˜æ—¥ã®å¤©æ°—æƒ…å ±ã‚’å–å¾—
            tomorrow_weather = None
            for forecast in weather_data["list"]:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰æ—¥ä»˜ã‚’å–å¾—
                forecast_date = forecast["dt_txt"].split(" ")[0]
                # æ˜æ—¥ã®æ—¥ä»˜ã‚’å–å¾—
                tomorrow = datetime.now() + timedelta(days=1)
                tomorrow_str = tomorrow.strftime("%Y-%m-%d")

                if forecast_date == tomorrow_str:
                    tomorrow_weather = forecast
                    break

            if tomorrow_weather:
                description = tomorrow_weather["weather"][0]["description"]
                temperature = tomorrow_weather["main"]["temp"]
                humidity = tomorrow_weather["main"]["humidity"]
                wind_speed = tomorrow_weather["wind"]["speed"]

                result = (
                    f"{location}ã®æ˜æ—¥ã®å¤©æ°—ã¯{description}ã§ã™ã€‚\n"
                    f"æ°—æ¸©: {temperature}â„ƒ\n"
                    f"æ¹¿åº¦: {humidity}%\n"
                    f"é¢¨é€Ÿ: {wind_speed}m/s"
                )
            else:
                result = f"{location}ã®æ˜æ—¥ã®å¤©æ°—æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

            # ç¾åœ¨ã®å¤©æ°—æƒ…å ±ã‚’å–å¾—
            base_url = "http://api.openweathermap.org/data/2.5/weather"
            response = requests.get(base_url, params=params)
            #print("response.url=",response.url)
            print("response.status_code=",response.status_code) #200
            #print("response.text=",response.text)
            #print("weather_data=",weather_data)
            response.raise_for_status()
            weather_data = response.json()
            description = weather_data["weather"][0]["description"]
            temperature = weather_data["main"]["temp"]
            humidity = weather_data["main"]["humidity"]
            wind_speed = weather_data["wind"]["speed"]
            result += (
                f"\n{location}ã®ç¾åœ¨ã®å¤©æ°—ã¯{description}ã§ã™ã€‚\n"
                f"æ°—æ¸©: {temperature}â„ƒ\n"
                f"æ¹¿åº¦: {humidity}%\n"
                f"é¢¨é€Ÿ: {wind_speed}m/s"
            )
            return result

        except requests.exceptions.RequestException as e:
            return f"Error: Could not retrieve weather information for {location}. {e}"
        except (KeyError, IndexError) as e:
            return f"Error: Could not parse weather information for {location}. {e}"
        except Exception as e:
            return f"An unexpected error occurred: {e}"

    ######################################################
    # Wolfram Alpha
    # Wolfram Alpha Tool
    class WolframAlphaInput(BaseModel):
        """Wolfram Alpha ã®å…¥åŠ›."""
        query: str = Field(...,
            description="""Wolfram Alpha ã«é€ä¿¡ã™ã‚‹ã‚¯ã‚¨ãƒªæ•°å­¦ã€ç§‘å­¦ã€æ­´å²ã€åœ°ç†ã€æ–‡åŒ–ãªã©ã€
            å¹…åºƒã„åˆ†é‡ã®è³ªå•ã«ç­”ãˆã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚æ—¥ä»˜ã‚„å¤©æ°—ã«ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚""")

    wolfram_alpha_wrapper = WolframAlphaAPIWrapper()
    wolfram_alpha_tool = Tool(
        name="wolfram_alpha",
        func=wolfram_alpha_wrapper.run,
        description="""æ•°å­¦ã€ç§‘å­¦ã€æ­´å²ã€åœ°ç†ã€æ–‡åŒ–ãªã©ã€å¹…åºƒã„åˆ†é‡ã®è³ªå•ã«ç­”ãˆã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚
                        æ—¥ä»˜ã‚„å¤©æ°—ã«ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚""",
        args_schema=WolframAlphaInput,
    )

    ####################################################################################
    # define the tools available to the agent - we're defining a single tool, exa_search
    # create the exa client
    #os.environ["EXA_API_KEY"] = "d74cc435-f3d2-4d8d-a21e-b6a66230c256"
    exa = Exa(api_key=os.environ["EXA_API_KEY"])
    # https://docs.exa.ai/reference/python-sdk-specification#search_and_contents-method
    def exa_search(query: str) -> Dict[str, Any]:
        st.session_state.use_tool_name="exa_search"
        print("use_tool_name:",st.session_state.use_tool_name)
        st.write("use_tool_name:",st.session_state.use_tool_name)
        return exa.search_and_contents(
            query=query,
            type='auto',
            text= True,
            num_results=3, #è¨˜äº‹ãŒå¤šã„ã¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚ªãƒ¼ãƒãƒ¼
            ) #highlights=True,

    exa_tool = [
        {
            "type": "function",
            "function": {
                "name": "exa_search",
                "description": "Perform a search query on the web, and retrieve the world's most relevant information.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "The search query to perform.",
                        },
                    },
                    "required": ["query"],
                },
            },
        }
    ]
    #tools ãƒªã‚¹ãƒˆã« exa_tool ã‚’è¿½åŠ ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®æ‰‹é †ã§è¡Œã„ã¾ã™ã€‚
    #ä¿®æ­£æ‰‹é †
    #exa_tool ã‚’ Tool ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã™ã‚‹ã€‚
    #exa_tool ã‚’ tools ãƒªã‚¹ãƒˆã«è¿½åŠ ã™ã‚‹ã€‚
    #openai_tools ã®ä½œæˆã‚’ä¿®æ­£ã™ã‚‹ã€‚
    #exa_toolã‚’Toolã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã™ã‚‹é–¢æ•°
    def exa_tool_to_tool_object(exa_tool_def):
        def exa_search_wrapper(query: str) -> str:
            # exa_searché–¢æ•°ã‚’å‘¼ã³å‡ºã—ã€çµæœã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦è¿”ã™
            result = exa_search(query)
            return str(result)

        # Toolã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¦è¿”ã™
        return Tool(
            name=exa_tool_def[0]["function"]["name"],
            func=exa_search_wrapper,
            description=exa_tool_def[0]["function"]["description"],
            )
    #exa_toolã‚’Toolã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
    exa_tool_object = exa_tool_to_tool_object(exa_tool)
    ##########################################################################################
    # Tavily Search
    class TavilySearchInput(BaseModel):
        """Input for tavily search."""

        query: str = Field(..., description="search query to look up")


    tavily_search = TavilySearchResults(
        max_results=3,
        include_answer=True,
        include_raw_content=True,
    )
    # TavilySearchResults ã‚’ Tool ã§ãƒ©ãƒƒãƒ—
    tavily_tool = Tool(
        name="tavily_search_results_json",
        func=tavily_search.run,
        description="useful for when you need to answer questions about current events",
        args_schema=TavilySearchInput,
    )
    #########################################################################################
    # News API
    class NewsSearchInput(BaseModel):
        """Input for news search."""
        query: str = Field(..., description="search query to look up in news")

    # News API ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
    newsapi = NewsApiClient(api_key=os.environ["NEWSAPI_API_KEY"])
    #os.environ["NEWSAPI_API_KEY"])

    @tool(args_schema=NewsSearchInput)
    def get_news(query: str) -> str:
        """Get the latest news based on a query."""
        st.session_state.use_tool_name="get_news"
        print("use_tool_name:",st.session_state.use_tool_name)
        st.write("use_tool_name:",st.session_state.use_tool_name)
        try:
            # NewsApiClientã‚’ä½¿ç”¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã€‚æ—¥æœ¬èªæŒ‡å®šä¸å¯(LLMã§ç¿»è¨³ã•ã›ã‚‹)
            news = newsapi.get_everything(q=query, language='en', sort_by='relevancy', page=1)
            # æœ€åˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’è¿”ã™
            if news['articles']:
                result = ""
                for article in news['articles']:
                    result += f"Title: {article['title']}\nURL: {article['url']}\n\n"
                return result
            else:
                return "No news found for the given query."
        except Exception as e:
            return f"Error fetching news: {e}"
    ########################################################
    #Langchainã®Toolã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
    tools = [get_weather,exa_tool_object,tavily_tool, wolfram_alpha_tool, get_news] #+ [fetch_page] 
    # Tool ã‚’ OpenAI å½¢å¼ã«å¤‰æ›
    openai_tools = [convert_to_openai_tool(t) for t in tools]
    return  tools,openai_tools
    ############################################################

def qa(text_input,webrtc_ctx,cap_title,cap_image):
    # æœ«å°¾ã®ç©ºç™½ã®æ•°ã‚’ç¢ºèª
    #trailing_spaces = len(text_input) - len(text_input.rstrip())
    #print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæœ«å°¾ã®ç©ºç™½ã®æ•°: {trailing_spaces}")
    # æœ«å°¾ã®ç©ºç™½ã‚’å‰Šé™¤
    cleaned_text = text_input.rstrip()
    #print(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæœ«å°¾ã®ç©ºç™½ã‚’é™¤å»ã—ãŸæ–‡å­—åˆ—: '{cleaned_text}'")
    with st.chat_message('user'):   
        st.write(cleaned_text) 
    
    # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
    cap = None 
    if st.session_state.input_img == "æœ‰":
        # ç”»åƒã¨å•ã„åˆã‚ã›å…¥åŠ›ãŒã‚ã£ãŸã¨ãã®å‡¦ç†
        #ç¾åœ¨ã®ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹
        #ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ¼ç”»åƒå…¥åŠ›
        if webrtc_ctx.video_transformer:  
            cap = webrtc_ctx.video_transformer.frame
        if cap is not None :
            #st.sidebar.header("Capture Image") 
            cap_title.header("Capture Image")     
            cap_image.image(cap, channels="BGR")
            # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
    # if st.button("Query LLM : ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦"):
    with st.spinner("Querying LLM..."):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        st.session_state.result= ""
        result = loop.run_until_complete(query_llm(cleaned_text,cap))
        st.session_state.result = result
    result = ""
    text_input="" 

class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.frame = None

    def recv(self, frame):   
        self.frame = frame.to_ndarray(format="bgr24")
        return frame

def whis_seg2(audio_segment):
    # AudioSegmentã‹ã‚‰ç›´æ¥NumPyé…åˆ—ã‚’å–å¾—
    #audio_data = np.array(audio_segment.get_array_of_samples()).astype(np.float32)
    #audio_data /= np.iinfo(audio_segment.array_type).max  # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–
    audio_data = np.frombuffer(audio_segment.raw_data, dtype=np.int16).astype(np.float32)
    audio_data /= np.iinfo(np.int16).max  # æ­£è¦åŒ–
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—
    sample_rate = audio_segment.frame_rate
    #audio_segment = ""
    # WhisperãŒ16kHzã‚’æœŸå¾…ã™ã‚‹ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’å¤‰æ›
    if sample_rate != 16000:
        audio_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=16000)
        sample_rate = 16000
    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’é©åˆ‡ãªé•·ã•ã«èª¿æ•´
    audio_data = whisper.pad_or_trim(audio_data)
    if not "whisper_model" in st.session_state:
        st.session_state.whisper_model = whisper.load_model("small")
    whisper_model = st.session_state.whisper_model
    #whisper_model = whisper.load_model("small")
    result = whisper_model.transcribe(audio_data, language="ja")
    #audio_data = ""
    answer2 = result['text']
    #result = ""
    return answer2

def save_audio(audio_segment: AudioSegment, base_filename: str) -> None:
    filename = f"{base_filename}_{int(time.time())}.wav"
    audio_segment.export(filename, format="wav")

def transcribe(audio_segment: AudioSegment, debug: bool = False) ->  Tuple[str, str]:
    answer2 ="ï¼ˆä¼‘æ­¢ä¸­ï¼‰"
    # ã‚¹ãƒ†ãƒ¬ã‚ªã®å ´åˆã€ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
    #print("audio_segment.channels=",audio_segment.channels)  
    if audio_segment.channels > 1:  
        audio_segment = audio_segment.set_channels(1)      
    
    if debug:
        save_audio(audio_segment, "debug_audio")
    #if st.session_state.output_whi2:
    answer2 = whis_seg2(audio_segment)
    return answer2 

async def process_audio(audio_data_bytes, sample_rate, sound_chunk):
    sound = pydub.AudioSegment(
        data=audio_data_bytes,
        sample_width=2, # audio_data_bytes.format.bytes,
        frame_rate=sample_rate,
        channels=2 , #len(audio_data_bytes.layout.channels), NG 1ï¼šæ–‡å­—åŒ–ã‘ã™ã‚‹
    )
    sound_chunk += sound
    if len(sound_chunk) > 0:
       answer2 = transcribe(sound_chunk)
    return answer2 

async def process_audio_loop_with_silence_detection(
    frames_deque_lock,
    frames_deque,
    sound_chunk,
    amp_indicator,
    ):
    """
    éŸ³å£°ã‚’ç„¡éŸ³åŒºåˆ‡ã‚Šã§ã¾ã¨ã‚ã€ç„¡éŸ³ãŒä¸€å®šæ™‚é–“ç¶šã„ãŸã‚‰ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã‚’è¡Œã†ã€‚
    """
    audio_buffer = []
    last_sound_time = time.time()
    silence_detected = False

    while True:
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—
        with frames_deque_lock:
            while len(frames_deque) > 0:
                frame = frames_deque.popleft() # å·¦ç«¯ã‹ã‚‰è¦ç´ ã‚’å–ã‚Šå‡ºã—ã¦å‰Šé™¤
                audio_chunk = frame.to_ndarray().astype(np.int16)
                audio_buffer.append(audio_chunk)
                st.session_state.frame_sample_rate = frame.sample_rate
                amp=np.max(np.abs(audio_chunk)) 
                #st.session_state.amp = amp
                amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={amp}")
                #print(f"éŸ³å£°æŒ¯å¹…/ç„¡éŸ³é–¾å€¤={amp}/{SILENCE_THRESHOLD}")
                #amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={amp}")
                if not amp < st.session_state.amp_threshold:
                    last_sound_time = time.time()
                    silence_detected = False
                else:
                    silence_detected = time.time() - last_sound_time >= st.session_state.silence_threshold
        #print(f"ç„¡éŸ³åˆ¤å®š={silence_detected}")
        #print(f"audio_buffer={len(audio_buffer)}")
        # ç„¡éŸ³åŒºåˆ‡ã‚ŠãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
        if silence_detected and audio_buffer:
            audio_data = np.concatenate(audio_buffer).tobytes()
            try:
                answer2 = await process_audio(audio_data, st.session_state.frame_sample_rate, sound_chunk)
                ##########################################################
                #text_output.write(f"èªè­˜çµæœ: {answer}")
                #ãŠã‹ã—ãªå›ç­”ã‚’é™¤å»
                # ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ãŒç©ºã€ã¾ãŸã¯ç©ºç™½ã§ã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
                phrases = (
                    "ã‚ã‚ŠãŒã¨ã†", 
                    "ãŠç–²ã‚Œæ§˜", "ã‚“ã‚“ã‚“ã‚“ã‚“ã‚“", 
                    "by H.","ã‚¹ã‚¿ãƒƒãƒ•ã•ã‚“ã®ãŠè©±ã‚’",
                    "ã„ã„ãˆ- ã„ã„ãˆ- ã„ã„ãˆ-",
                    "ã”ã¡ãã†ã•ã¾ã§ã—ãŸ"
                    )
                if len(answer2) < 5:
                    pass
                elif any(phrase in answer2 for phrase in phrases):
                    pass
                else:
                    #with text_output.chat_message('user'):
                        #st.write(answer2)
                    print("[Whis_seg]",answer2) 
                    return answer2 
                audio_buffer = []  # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
                silence_detected = False
            except Exception as e:
                st.error(f"éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        #amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={st.session_state.amp}")
        #amp_indicator.write(f"éŸ³å£°æŒ¯å¹…(ç„¡éŸ³é–¾å€¤{st.session_state.amp_threshold})={amp}")
        # å‡¦ç†è² è·ã‚’æŠ‘ãˆã‚‹ãŸã‚ã«çŸ­ã„é…å»¶ã‚’æŒ¿å…¥
        time.sleep(0.1)

def app_sst_with_video():
    
    
    frames_deque_lock = threading.Lock()
    frames_deque: deque = deque([])
    async def queued_audio_frames_callback(
        frames: List[av.AudioFrame],
    ) -> av.AudioFrame:
        with frames_deque_lock:
            frames_deque.extend(frames)
        # Return empty frames to be silent.
        new_frames = []
        for frame in frames:
            input_array = frame.to_ndarray()
            new_frame = av.AudioFrame.from_ndarray(
                np.zeros(input_array.shape, dtype=input_array.dtype),
                layout=frame.layout.name,
            )
            new_frame.sample_rate = frame.sample_rate
            new_frames.append(new_frame)
        return new_frames
        
     # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çŠ¶æ…‹ã‚’ç®¡ç†ã™ã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–
    if "streaming" not in st.session_state:
        st.session_state["streaming"] = True  # åˆæœŸçŠ¶æ…‹ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å†ç”Ÿä¸­
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«WebRTCã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¡¨ç¤º
    with st.sidebar:
        st.header("Webcam Stream")
        webrtc_ctx = webrtc_streamer(
            key="speech-to-text-w-video",
            desired_playing_state=st.session_state["streaming"], 
            mode=WebRtcMode.SENDRECV, #.SENDONLY,  #
            #audio_receiver_size=2048,  #1024ã€€#512 #ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯4
            #å°ã•ã„ã¨Queue overflow. Consider to set receiver size bigger. Current size is 1024.
            queued_audio_frames_callback=queued_audio_frames_callback,
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            media_stream_constraints={"video": True, "audio": True},
            video_processor_factory=VideoTransformer,  
        )
    if not webrtc_ctx.state.playing:
        return
    #status_indicator.write("Loading...")
    cap_title = st.sidebar.empty()
    cap_image = st.sidebar.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ 

    memory_use = st.sidebar.empty()
    memory_alt = st.sidebar.empty()
    memory_ok = st.sidebar.empty()

    text_input = ""
    st.sidebar.title("Options")
    col1, col2 ,col3= st.sidebar.columns(3)
    # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
    with col1:
        # å…¥åŠ›æ–¹æ³•ã®é¸æŠ
        input_method = st.sidebar.radio("å…¥åŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.input_method = input_method
    with col2:
        # ç”»åƒã«ã¤ã„ã¦ã®å•åˆã›æœ‰ç„¡ã®é¸æŠ
        input_img = st.sidebar.radio("  ã‚«ãƒ¡ãƒ©ç”»åƒå•åˆã›", ("æœ‰", "ç„¡"))
        st.session_state.input_img = input_img
    with col3:
        # å‡ºåŠ›æ–¹æ³•ã®é¸æŠ
        output_method = st.sidebar.radio("å‡ºåŠ›æ–¹æ³•", ("ãƒ†ã‚­ã‚¹ãƒˆ", "éŸ³å£°"))
        st.session_state.output_method = output_method

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    for role, message in st.session_state.get("message_history", []):
        #st.chat_message(role).markdown(message)
        with st.chat_message(role):
            st.markdown(message)
    ###################################################################
    #éŸ³å£°å…¥åŠ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ãŸå…¥åŠ›ï¼‰ã®å¯¾è©±ãƒ«ãƒ¼ãƒ—
    if st.session_state.input_method == "éŸ³å£°":
        #st.session_state.message_history = [
        #    ("system", "You are a helpful assistant.")
        #]  # ä¼šè©±å±¥æ­´ã‚’åˆæœŸåŒ–,ç”»é¢ã‚¯ãƒªã‚¢ ã“ã“ã¯NG
        status_indicator = st.empty() # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        status_indicator.write("ğŸ¤–æº–å‚™ä¸­ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚")
        status_indicator = st.sidebar.empty()    
        amp_indicator = st.sidebar.empty()
        st.session_state.amp_threshold = st.sidebar.slider(
            "ç„¡éŸ³æŒ¯å¹…é–¾å€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1000):",
            min_value=300, max_value=3000, value=1000, step=100
            )
        st.session_state.silence_threshold = st.sidebar.slider(
            "ç„¡éŸ³æœ€å°æ™‚é–“ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.5ç§’ï¼‰",
            min_value=0.1, max_value=3.0, value=0.5, step=0.1
            )
        if not "whisper_model" in st.session_state:
            st.session_state.whisper_model = whisper.load_model("small") #,device = "cuda")
        #base:74M,small:244M,medium:769M,large:1550M 
        #st.session_state["streaming"] = False  # Webã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åœæ­¢
        #status_indicator.empty  #æº–å‚™ãŒã§ããŸã®ã§ã€"ğŸ¤–æº–å‚™ä¸­ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚")ã‚’æ¶ˆã™ NG

        #for key, label, default in [
            #("output_whi2", "Whis-segmå‡ºåŠ›ï¼ˆç„¡æ–™ï¼‰", True),
            #]:
            #st.session_state[key] = st.sidebar.toggle(label, value=default)

        i = 0
        #current_memory_use(i,memory_use,memory_alt,memory_ok)
        frames_deque_lock = threading.Lock()
        # frames_deque_lockã‚’ä½¿ç”¨ã—ã¦ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã—ã¦ã„ã¾ã™ãŒã€
        # dequeã®ã‚¯ãƒªã‚¢æ“ä½œãªã©ã§ãƒªã‚½ãƒ¼ã‚¹ç«¶åˆãŒèµ·ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
        # dequeã®æœ€å¤§é•·ã‚’è¨­å®šï¼ˆä¾‹: deque([], maxlen=100)) ã—ã€ãƒãƒƒãƒ•ã‚¡æº¢ã‚Œã‚’é˜²æ­¢ã™ã‚‹æ–¹ãŒå®‰å…¨ã§ã™ã€‚
        frames_deque: deque = deque([], maxlen=100) #NG 1
        
        sound_chunk = pydub.AudioSegment.empty()  
        while True:
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
            current_memory_use(i,memory_use,memory_alt,memory_ok)
            mem_use = get_memory_usage()
            i += 1
            #print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡={i}å›ç›®:{mem_use}")
            st.write("ğŸ¤–ä½•ã‹è©±ã—ã¦!")
            status_indicator.write(f"len(frames_deque)={len(frames_deque)}")
            # éŸ³å£°å‡¦ç†ã®éåŒæœŸã‚¿ã‚¹ã‚¯ã‚’èµ·å‹•
            text_input = asyncio.run(process_audio_loop_with_silence_detection(
                frames_deque_lock,
                frames_deque,
                sound_chunk,
                amp_indicator,
                ))
            qa(text_input,webrtc_ctx,cap_title,cap_image)
            text_input = ""
    ################################################################### 
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®å ´åˆ
    # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    if st.session_state.input_method == "ãƒ†ã‚­ã‚¹ãƒˆ":
        #st.session_state.message_history = [
            #("system", "You are a helpful assistant.")
        #]  # ä¼šè©±å±¥æ­´ã‚’åˆæœŸåŒ–,ç”»é¢ã‚¯ãƒªã‚¢ ã“ã“ã¯NG
        st.session_state["streaming"] = True  # Webã‚«ãƒ¡ãƒ©ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å†ç”Ÿ
        button_input = ""
        # 4ã¤ã®åˆ—ã‚’ä½œæˆ
        col1, col2, col3, col4 = st.columns(4)
        # å„åˆ—ã«ãƒœã‚¿ãƒ³ã‚’é…ç½®
        with col1:
            if st.button("ç”»åƒã®èª¬æ˜"):
                button_input = "ç”»åƒã®å†…å®¹ã‚’è©³ã—ãèª¬æ˜ã—ã¦"
        with col2:
            if st.button("å‰ç”»åƒã¨ã®å¤‰åŒ–"):
                button_input = "å‰ã®ç”»åƒã¨ä½•ãŒå¤‰ã‚ã‚Šã¾ã—ãŸã‹ï¼Ÿ"
        with col3:
            if st.button("ç”»åƒã®æ–‡ã®ç¿»è¨³"):
                button_input = "ã“ã®ç”»åƒã®æ–‡ã‚’ç¿»è¨³ã—ã¦"
        with col4:
            if st.button("äººç”Ÿã®æ„ç¾©ã€å–„æ‚ªã®åˆ¤æ–­"):
                button_input = "äººç”Ÿã®æ„ç¾©ã¯ï¼Ÿå–„æ‚ªã¯ä½•ã§åˆ¤æ–­ã™ã‚‹ï¼Ÿ"
        col5, col6, col7, col8 = st.columns(4)
        with col5:
            if st.button("å„å›½ã®æ‚ªã„ç‚¹"):
                button_input = "æ—¥æœ¬ã€ã‚¢ãƒ¡ãƒªã‚«ã€ãƒ­ã‚·ã‚¢ã€ä¸­å›½ã®æ‚ªã„ã¨ã“ã‚ã¯ï¼Ÿ"
        with col6:
            if st.button("ä»Šæ—¥ã¯ä½•æ—¥"):
                button_input = "ä»Šæ—¥ã¯ä½•æ—¥ã§ã™ã‹ï¼Ÿ"
        with col7:
            if st.button("å°æ¾ã®è©±é¡Œæ–™ç†åº—"):
                button_input = "å°æ¾å¸‚ã®æœ€è¿‘è©±é¡Œã®æ–™ç†åº—ã¯ï¼Ÿ"
        with col8:
            if st.button("å¤©æ°—"):
                button_input = "å°æ¾å¸‚ã®ã“ã“1é€±é–“ã®å¤©æ°—ã‚’æ•™ãˆã¦ã€‚èŠ±ç²‰æƒ…å ±ã‚‚æ•™ãˆã¦ã€‚"
        col9, col10, col11,col12= st.columns(4)
        with col9:
            if st.button("ä»Šæ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹"):
                button_input = "ä»Šæ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒˆãƒ”ãƒƒã‚¯ã‚¹ã¯?" #ä»Šæ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒˆãƒ”ãƒƒã‚¯ã‚¹ã¯?
        with col10:
            if st.button("æ³¨ç›®ã®LLM"):
                button_input = "æœ€è¿‘ã®ç”»åƒèªè­˜ãŒã§ãã¦ã€ãƒ„ãƒ¼ãƒ«ãŒä½¿ãˆã€APIã§åˆ©ç”¨ã§ãã‚‹ç„¡æ–™ã®LLMã¯ï¼Ÿå…·ä½“çš„ãªãƒ¢ãƒ‡ãƒ«åã‚’10æœ¬æ•™ãˆã¦"
        with col11:
            if st.button("è©±é¡Œã®æ¼«ç”»ãƒ»æ˜ ç”»"):
                button_input = "ä»Šã€è©±é¡Œã®æ¼«ç”»ã¨æ˜ ç”»ã‚’ãã‚Œãã‚Œ5ä»¶æ•™ãˆã¦ã€‚"
        with col12:
            if st.button("ã‚ªã‚»ãƒ­ã‚’ä½œæˆ"):
                button_input = "Webç”»é¢ã§ãƒ—ãƒ¬ã‚¤ã™ã‚‹ã‚ªã‚»ãƒ­ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦"          
        col13, col14, col15,col16= st.columns(4)
        with col13:
            if st.button("çŸ³å·çœŒå°æ¾å¸‚ã«ä½ã‚“ã§ã„ã¾ã™ã€‚"):
                button_input = "ç§ã¯ã€çŸ³å·çœŒå°æ¾å¸‚ã«ä½ã‚“ã§ã„ã¾ã™ã€‚" #ä»Šæ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒˆãƒ”ãƒƒã‚¯ã‚¹ã¯?
        with col14:
            if st.button("ä½ã‚“ã§ã„ã‚‹ã¨ã“ã‚ã®å¤©æ°—"):
                button_input = "ç§ã®ä½ã‚“ã§ã„ã‚‹ã¨ã“ã‚ã®æ˜æ—¥ã¨æ˜å¾Œæ—¥ã®å¤©æ°—ã¯?"
        with col15:
            if st.button("ä»Šæ—¥ã¯ä½•å¹´ä½•æœˆä½•æ—¥ã§ã™ã‹ï¼Ÿ"):
                button_input = "æ—¥æœ¬ã§ã¯ã€ä»Šæ—¥ã¯ä½•å¹´ä½•æœˆä½•æ—¥ã§ã™ã‹ï¼Ÿ"
        with col16:
            if st.button("ã•ã£ãã®è³ªå•ã‚’æ†¶ãˆã¦ã„ã¾ã™ã‹?åˆ¥ã®ãƒ„ãƒ¼ãƒ«ã§ç­”ãˆã¦"):
                button_input = "ã•ã£ãã®è³ªå•ã‚’æ†¶ãˆã¦ã„ã¾ã™ã‹?ã‚‚ã†ä¸€åº¦ã€åˆ¥ã®ãƒ„ãƒ¼ãƒ«ã‚’åˆ©ç”¨ã—ã¦ç­”ãˆã¦ãã ã•ã„ã€‚"
        if button_input !="":
            st.session_state.user_input=button_input

        text_input =st.chat_input("ğŸ¤—ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ã“ã“ã«å…¥åŠ›ã—ã¦ã­ï¼") #,key=st.session_state.text_input)
        #text_input = st.text_input("ãƒ†ã‚­ã‚¹ãƒˆã§å•ã„åˆã‚ã›ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å…¥åŠ›ã—ã¦ãã ã•ã„:", key=st.session_state.text_input) 
        if button_input:
            text_input = button_input

        if text_input:
            qa(text_input,webrtc_ctx,cap_title,cap_image)
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
        i = 0
        current_memory_use(i,memory_use,memory_alt,memory_ok)
        mem_use = get_memory_usage()
        i += 1
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡={i}å›ç›®:{mem_use}")

def init_page():
    st.set_page_config(
        page_title="Yas Chatbot",
        page_icon="ğŸ¤–"
    )
    st.header("Yas Chatbot(ç”»åƒã€éŸ³å£°å¯¾å¿œ) ğŸ¤–")
    st.write("""Webã‚«ãƒ¡ãƒ©ç”»åƒã«ã¤ã„ã¦ã®å•åˆã›ã€éŸ³å£°ã§ã®å…¥å‡ºåŠ›ãŒã§ãã¾ã™ã€‚\n
             ãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚«ãƒ¡ãƒ©,ãƒã‚¤ã‚¯ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ã—ã¦ä½¿ç”¨ã€‚""") 

def init_messages():
    history_id = 123
    clear_button = st.sidebar.button("ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢", key="clear")
    
    #if clear_button or "message_history" not in st.session_state:
        #st.session_state.message_history = [
            #("system", "You are a helpful assistant.")
        #]   
    #å•é¡Œã¯ã€åˆå›ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã§ã™ã€‚
    # ã“ã®ã¨ãã€clear_button ã¯ã¾ã æŠ¼ã•ã‚Œã¦ã„ãªã„ã®ã§ False ã§ã™ã€‚
    # ãã—ã¦ã€message_history ã‚‚ã¾ã å­˜åœ¨ã—ãªã„ã®ã§ 
    # "message_history" not in st.session_state ã¯ True ã¨ãªã‚Šã¾ã™ã€‚
    # False or True ã¯ True ãªã®ã§ã€åˆæœŸåŒ–å‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
    # ä¸€è¦‹ã€å•é¡Œãªã„ã‚ˆã†ã«è¦‹ãˆã¾ã™ã€‚
    # clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    # clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã«åˆæœŸåŒ–
    # ç‹¬ç«‹ã—ãŸifã§è¨˜è¿°ã™ã‚‹ã“ã¨ã§ã€ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã€ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã€ã™ã¹ã¦ã®çŠ¶æ³ã«å¯¾å¿œã§ãã¾ã—ãŸã€‚
    # ç¾åœ¨ã®æ—¥ä»˜ã¨æ™‚åˆ»ã‚’å–å¾—
    current_datetime = get_current_datetime()
    # define the system message (primer) of your agent
    SYSTEM_MESSAGE = f"""ã‚ãªãŸã¯ãƒ„ãƒ¼ãƒ«ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
            ç¾åœ¨ã®æ—¥æ™‚ã¯ {current_datetime} ã§ã™ã€‚
            æä¾›ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¢ã—ã¦ã„ã‚‹æœ€æ–°ã®æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
            å¿…è¦ãªæƒ…å ±ãŒå¾—ã‚‰ã‚Œãªã„å ´åˆã¯ã€ä»–ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
            ç‰¹ã«ã€å¤©æ°—ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’èã‹ã‚ŒãŸå ´åˆã¯ã€å¿…ãšãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
            ã¾ãŸã€å¸¸ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
            #"""You are the world's most advanced search engine.
            #Please provide the user with the new information they are looking for by using the tools provided."""
            #You are a helpful assistant.
            #You are a competent assistant that effectively utilizes tools.ã€€åŸæ–‡
            #è²´æ–¹ã¯ãƒ„ãƒ¼ãƒ«ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã™ã‚‹æœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚åŸæ–‡ã®è¨³
            #ã‚ãªãŸã¯ä¸–ç•Œã§æœ€ã‚‚é«˜åº¦ãªæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚
    if clear_button:
        st.session_state.message_history = [("system",SYSTEM_MESSAGE)]
        history_id += 1
        st.session_state.history_id = f"abc{history_id}"
    #clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    if "message_history" not in st.session_state:
        st.session_state.message_history = [("system", SYSTEM_MESSAGE)]

def main():
    #ç’°å¢ƒå¤‰æ•°
    setup_environment()
    #st.header("Real Time Speech-to-Text with_video")
    #ç”»é¢è¡¨ç¤º
    init_page()
    #ä¼šè©±å±¥æ­´åˆæœŸåŒ–ã¨ã‚¯ãƒªã‚¢
    init_messages()
    #stã§ä½¿ã†å¤‰æ•°åˆæœŸè¨­å®š
    st.session_state.llm = None
    st.session_state.selected_model_name = ""
    st.session_state.model_name = ""
    st.session_state.input_method = ""
    st.session_state.user_input = ""
    st.session_state.result = ""
    st.session_state.frame = ""
    #st.session_state.history_id = "abc123"
    #
    # ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾— (ãƒ„ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒ‰å‰)
    # ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆå–å¾— (ã‚­ãƒ£ãƒƒã‚·ãƒ¥)
    #@st.cache_resource
    #def load_models():
        #models = initialize_models()
        #return models
    # @st.cache_resource(show_spinner=False) # ãƒ¢ãƒ‡ãƒ«é¸æŠã”ã¨ã«å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ spinner ã¯éè¡¨ç¤ºã«
    # def load_selected_model(model_name: str):
    #     """é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    #     return initialize_models(model_name)

    #info_disp.info("LLMãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")
    #models_dict = load_models()
    #info_disp.info("LLMãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–å®Œäº†ã€‚")
    #model_names = list(models_dict.keys())
    model_names = list(model_definitions.keys())
    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒ¢ãƒ‡ãƒ«é¸æŠ ---
    selected_model_name = st.sidebar.selectbox(
        "è¨€èªãƒ¢ãƒ‡ãƒ«é¸æŠ(å„ªåŠ£æœ‰ã‚Š):",
        model_names,
        #index=model_names.index(st.session_state.selected_model_name) if st.session_state.selected_model_name in model_names else 0,
        index=model_names.index(st.session_state.selected_model_name)
            if hasattr(st.session_state, "selected_model_name") and st.session_state.selected_model_name in model_names
            else 0,
        # --- â–²â–²â–² ä¿®æ­£ â–²â–²â–² ---
        key="model_select"
    )

    # --- ãƒ¢ãƒ‡ãƒ«/ã‚°ãƒ©ãƒ•/ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å†åˆæœŸåŒ– ---
    if st.session_state.selected_model_name is None or selected_model_name != st.session_state.selected_model_name:
        print("#"*100)
        print("st.session_state.selected_model_name=",st.session_state.selected_model_name)
        print("selected_model_name=",selected_model_name)
        st.session_state.selected_model_name = selected_model_name
        st.session_state.model_name = selected_model_name
        st.session_state.messages = []
        st.session_state.graph_or_agent = None
        st.session_state.memory = MemorySaver()
        st.session_state.history_id = f"streamlit_thread_{selected_model_name}_{str(uuid.uuid4())}"

        # ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾— (ãƒ„ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒ‰å‰)
        #llm_instance_base, cost_input, cost_cached_input, cost_output, input_max = models_dict[selected_model_name]
        # --- â–¼â–¼â–¼ é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ â–¼â–¼â–¼ ---
        with st.spinner(f"ãƒ¢ãƒ‡ãƒ« '{selected_model_name}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­..."): # â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€ â˜…â˜…â˜…
            llm_instance_base, cost_input, cost_cached_input, cost_output, input_max = load_selected_model(selected_model_name)
            #st.session_state.llm = llm_instance_base #select_model()
        if llm_instance_base is None:
            st.error(f"ãƒ¢ãƒ‡ãƒ« '{selected_model_name}' ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            st.stop() # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰ã‚¢ãƒ—ãƒªã‚’åœæ­¢
        # --- â–²â–²â–² é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ â–²â–²â–² ---
    st.session_state.llm = llm_instance_base
    # â˜…â˜…â˜… è¿½åŠ : llm_instance_base ã‚’ session_state ã«ä»£å…¥ â˜…â˜…â˜…

    app_sst_with_video()

###################################################################
if __name__ == "__main__":
    main()
